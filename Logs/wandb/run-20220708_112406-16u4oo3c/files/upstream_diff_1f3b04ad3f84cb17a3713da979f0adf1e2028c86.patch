diff --git a/Checkpoints/QML-MNIST/version001/NYU/model.ckpt b/Checkpoints/QML-MNIST/version001/NYU/model.ckpt
new file mode 100644
index 0000000..77a70b0
Binary files /dev/null and b/Checkpoints/QML-MNIST/version001/NYU/model.ckpt differ
diff --git a/Configs/config.yaml b/Configs/config.yaml
index 541dd0b..67602ec 100644
--- a/Configs/config.yaml
+++ b/Configs/config.yaml
@@ -31,19 +31,11 @@ data:
 network:
   model: ${log.model_name}
   num_classes: 2
-  encoder: ${SCCNN_bn}
-  attention: ${Attention}
   LSTM: ${LSTM}
-  decoder: ${Decoder}
-  is_encoder_shared: True
-  encoderType: SCCNN_bn
   Simple_QHN: ${Simple_QHN}
-  hybrid: ${quantum_circuit}
 
 optimizer:
   optimizer: Adam
   lr: 1e-5
 
 
-runner: OneSiteHoldout_Runner
-
diff --git a/Configs/models.yaml b/Configs/models.yaml
index 2c5c2ba..9609ff5 100644
--- a/Configs/models.yaml
+++ b/Configs/models.yaml
@@ -1,22 +1,15 @@
 
 LSTM:
-  input_size: ${SCCNN.conv_block4.out_f}
+  input_size: 116
   hidden_size: 128
   num_layers: 1
   bidirectional: True
 
 Simple_QHN:
-  n_qubits: 2
+  n_qubits: 4
   shift: 0.6
-  is_cnot: True
+  shots: 200
   lstm_hidden: 128
   linear_out: 64
+  backend: aer_simulator
 
-
-quantum_circuit:
-  n_qubits: 3
-  simulator: aer_simulator
-  shift: 0.5
-  shots: 100
-  is_cnot: True
-  dense_type: 2
\ No newline at end of file
diff --git a/Logs/QML-MNIST/version000/version_0/hparams.yaml b/Logs/QML-MNIST/version000/version_0/hparams.yaml
index 3851257..000d975 100644
--- a/Logs/QML-MNIST/version000/version_0/hparams.yaml
+++ b/Logs/QML-MNIST/version000/version_0/hparams.yaml
@@ -15,6 +15,5 @@ net:
     simulator: aer_simulator
     shift: 0.5
     shots: 100
-    is_cnot: true
     dense_type: 2
 inputs: null
diff --git a/Logs/QML-MNIST/version001/NYU/version_0/events.out.tfevents.1656942962.DESKTOP-HFMS9NT.3487.0 b/Logs/QML-MNIST/version001/NYU/version_0/events.out.tfevents.1656942962.DESKTOP-HFMS9NT.3487.0
new file mode 100644
index 0000000..a8b3580
Binary files /dev/null and b/Logs/QML-MNIST/version001/NYU/version_0/events.out.tfevents.1656942962.DESKTOP-HFMS9NT.3487.0 differ
diff --git a/Logs/QML-MNIST/version001/NYU/version_0/hparams.yaml b/Logs/QML-MNIST/version001/NYU/version_0/hparams.yaml
new file mode 100644
index 0000000..1958d58
--- /dev/null
+++ b/Logs/QML-MNIST/version001/NYU/version_0/hparams.yaml
@@ -0,0 +1,141 @@
+opt:
+  optimizer: Adam
+  lr: 1.0e-05
+net:
+  model: Simple_QHN
+  num_classes: 2
+  LSTM:
+    input_size: 116
+    hidden_size: 128
+    num_layers: 1
+    bidirectional: true
+  Simple_QHN:
+    n_qubits: 2
+    shift: 0.6
+    lstm_hidden: 128
+    linear_out: 64
+    backend: aer_simulator
+  hybrid:
+    n_qubits: 3
+    simulator: aer_simulator
+    shift: 0.5
+    shots: 100
+    dense_type: 2
+  roi_rank:
+  - 0
+  - 1
+  - 2
+  - 3
+  - 4
+  - 5
+  - 6
+  - 7
+  - 8
+  - 9
+  - 10
+  - 11
+  - 12
+  - 13
+  - 14
+  - 15
+  - 16
+  - 17
+  - 18
+  - 19
+  - 20
+  - 21
+  - 22
+  - 23
+  - 24
+  - 25
+  - 26
+  - 27
+  - 28
+  - 29
+  - 30
+  - 31
+  - 32
+  - 33
+  - 34
+  - 35
+  - 36
+  - 37
+  - 38
+  - 39
+  - 40
+  - 41
+  - 42
+  - 43
+  - 44
+  - 45
+  - 46
+  - 47
+  - 48
+  - 49
+  - 50
+  - 51
+  - 52
+  - 53
+  - 54
+  - 55
+  - 56
+  - 57
+  - 58
+  - 59
+  - 60
+  - 61
+  - 62
+  - 63
+  - 64
+  - 65
+  - 66
+  - 67
+  - 68
+  - 69
+  - 70
+  - 71
+  - 72
+  - 73
+  - 74
+  - 75
+  - 76
+  - 77
+  - 78
+  - 79
+  - 80
+  - 81
+  - 82
+  - 83
+  - 84
+  - 85
+  - 86
+  - 87
+  - 88
+  - 89
+  - 90
+  - 91
+  - 92
+  - 93
+  - 94
+  - 95
+  - 96
+  - 97
+  - 98
+  - 99
+  - 100
+  - 101
+  - 102
+  - 103
+  - 104
+  - 105
+  - 106
+  - 107
+  - 108
+  - 109
+  - 110
+  - 111
+  - 112
+  - 113
+  - 114
+  - 115
+inputs: null
diff --git a/Logs/wandb/debug-internal.log b/Logs/wandb/debug-internal.log
new file mode 120000
index 0000000..32d80df
--- /dev/null
+++ b/Logs/wandb/debug-internal.log
@@ -0,0 +1 @@
+run-20220708_112406-16u4oo3c/logs/debug-internal.log
\ No newline at end of file
diff --git a/Logs/wandb/debug.log b/Logs/wandb/debug.log
new file mode 120000
index 0000000..cfe1688
--- /dev/null
+++ b/Logs/wandb/debug.log
@@ -0,0 +1 @@
+run-20220708_112406-16u4oo3c/logs/debug.log
\ No newline at end of file
diff --git a/Logs/wandb/latest-run b/Logs/wandb/latest-run
new file mode 120000
index 0000000..ce66d16
--- /dev/null
+++ b/Logs/wandb/latest-run
@@ -0,0 +1 @@
+run-20220708_112406-16u4oo3c
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_224416-2kr91mtl/files/code/run.py b/Logs/wandb/run-20220704_224416-2kr91mtl/files/code/run.py
new file mode 100644
index 0000000..69d3f58
--- /dev/null
+++ b/Logs/wandb/run-20220704_224416-2kr91mtl/files/code/run.py
@@ -0,0 +1,25 @@
+from omegaconf import OmegaConf
+from pathlib import Path
+
+from src.runners import S_Runner
+
+CONFIG_DIR = Path("Configs")
+
+
+def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+    model_params = OmegaConf.load(CONFIG_DIR / "models.yaml")
+    cfg = OmegaConf.merge(cfg, model_params)
+    cfg.merge_with_cli()
+
+    runner = S_Runner(
+        log=cfg.log,
+        optimizer=cfg.optimizer,
+        loader=cfg.loader,
+        network=cfg.network,
+        data=cfg.data,
+    )
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/Logs/wandb/run-20220704_224416-2kr91mtl/files/config.yaml b/Logs/wandb/run-20220704_224416-2kr91mtl/files/config.yaml
new file mode 100644
index 0000000..af7be49
--- /dev/null
+++ b/Logs/wandb/run-20220704_224416-2kr91mtl/files/config.yaml
@@ -0,0 +1,31 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.12.15
+    code_path: code/run.py
+    framework: lightning
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    m:
+    - 1: trainer/global_step
+      6:
+      - 3
+    python_version: 3.8.10
+    start_time: 1656942256
+    t:
+      1:
+      - 1
+      - 5
+      - 9
+      - 41
+      - 53
+      - 55
+      3:
+      - 7
+      - 23
+      4: 3.8.10
+      5: 0.12.15
+      8:
+      - 5
diff --git a/Logs/wandb/run-20220704_224416-2kr91mtl/files/diff.patch b/Logs/wandb/run-20220704_224416-2kr91mtl/files/diff.patch
new file mode 100644
index 0000000..5570603
--- /dev/null
+++ b/Logs/wandb/run-20220704_224416-2kr91mtl/files/diff.patch
@@ -0,0 +1,394 @@
+diff --git a/Configs/models.yaml b/Configs/models.yaml
+index 2c5c2ba..63b272d 100644
+--- a/Configs/models.yaml
++++ b/Configs/models.yaml
+@@ -8,9 +8,9 @@ LSTM:
+ Simple_QHN:
+   n_qubits: 2
+   shift: 0.6
+-  is_cnot: True
+   lstm_hidden: 128
+   linear_out: 64
++  backend: aer_simulator
+ 
+ 
+ quantum_circuit:
+@@ -18,5 +18,4 @@ quantum_circuit:
+   simulator: aer_simulator
+   shift: 0.5
+   shots: 100
+-  is_cnot: True
+   dense_type: 2
+\ No newline at end of file
+diff --git a/Logs/QML-MNIST/version000/version_0/hparams.yaml b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+index 3851257..000d975 100644
+--- a/Logs/QML-MNIST/version000/version_0/hparams.yaml
++++ b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+@@ -15,6 +15,5 @@ net:
+     simulator: aer_simulator
+     shift: 0.5
+     shots: 100
+-    is_cnot: true
+     dense_type: 2
+ inputs: null
+diff --git a/run.py b/run.py
+index c8e1ccf..69d3f58 100644
+--- a/run.py
++++ b/run.py
+@@ -1,7 +1,7 @@
+ from omegaconf import OmegaConf
+ from pathlib import Path
+ 
+-from src.runners import S_Runner, MNIST_Runner
++from src.runners import S_Runner
+ 
+ CONFIG_DIR = Path("Configs")
+ 
+@@ -11,7 +11,7 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+     cfg = OmegaConf.merge(cfg, model_params)
+     cfg.merge_with_cli()
+ 
+-    runner = MNIST_Runner(
++    runner = S_Runner(
+         log=cfg.log,
+         optimizer=cfg.optimizer,
+         loader=cfg.loader,
+@@ -22,4 +22,4 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+ 
+ 
+ if __name__ == "__main__":
+-    main(cfg=OmegaConf.load(CONFIG_DIR / "config MNIST.yaml"))
++    main()
+diff --git a/src/data/__pycache__/data_etl.cpython-38.pyc b/src/data/__pycache__/data_etl.cpython-38.pyc
+index 288d14c..934cbd5 100644
+Binary files a/src/data/__pycache__/data_etl.cpython-38.pyc and b/src/data/__pycache__/data_etl.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/loader.cpython-38.pyc b/src/data/__pycache__/loader.cpython-38.pyc
+index eddf358..67504ef 100644
+Binary files a/src/data/__pycache__/loader.cpython-38.pyc and b/src/data/__pycache__/loader.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/sampling.cpython-38.pyc b/src/data/__pycache__/sampling.cpython-38.pyc
+index a2cbaf3..a748fd2 100644
+Binary files a/src/data/__pycache__/sampling.cpython-38.pyc and b/src/data/__pycache__/sampling.cpython-38.pyc differ
+diff --git a/src/data/data_etl.py b/src/data/data_etl.py
+index 08700be..dc691b0 100755
+--- a/src/data/data_etl.py
++++ b/src/data/data_etl.py
+@@ -137,24 +137,6 @@ def chunks(lst, n):
+     return [lst[i : i + div] for i in range(0, len(lst), div)]
+ 
+ 
+-@dataclass
+-class Load_MNIST:
+-    def __init__(self, is_train:Boolean):
+-        if is_train:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=True, download=True,
+-                         transform=transforms.Compose([transforms.ToTensor()]))
+-        else:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=False, download=True,
+-                        transform=transforms.Compose([transforms.ToTensor()]))
+-                        
+-    def get_samples(self, n_samples:int):
+-        idx = []
+-        for i in range(10):
+-            idx.append(np.where(self.dataset.targets == i)[0][:n_samples])#각 target에 해당되는 번호를 저장한다. 
+-        idx = np.array(idx).reshape(-1)
+-        self.dataset.data = self.dataset.data[idx]
+-        self.dataset.targets = self.dataset.targets[idx]
+-        return self.dataset
+ 
+ 
+ 
+diff --git a/src/data/loader.py b/src/data/loader.py
+index 288a3b3..158cbdc 100644
+--- a/src/data/loader.py
++++ b/src/data/loader.py
+@@ -5,25 +5,8 @@ from typing import Union, List
+ import torch
+ from torch.utils.data import Dataset
+ 
+-from src.data import Load, SITES_DICT, Load_MNIST
+-
+-class MNISTDataset(Dataset):
+-    def __init__(self, n_samples: int, is_train: Boolean) -> None:
+-        Load = Load_MNIST(is_train)
+-        if n_samples >0:
+-            self.data = Load.get_samples(n_samples=n_samples).data
+-            self.labels = Load.get_samples(n_samples=n_samples).targets  
+-        else:  
+-            self.data = Load.dataset.data
+-            self.labels = Load.dataset.targets
++from src.data import Load, SITES_DICT
+ 
+-    def __len__(self):
+-        return len(self.labels)
+-    
+-    def __getitem__(self, index: int):
+-        data = self.data[index]
+-        label = self.labels[index]
+-        return data, label 
+   
+ class ROIDataset(Dataset):
+     def __init__( self, site: Union[List, str]) -> None:
+diff --git a/src/datamodules/__pycache__/datamodule.cpython-38.pyc b/src/datamodules/__pycache__/datamodule.cpython-38.pyc
+index 6f06b1a..c8350c4 100644
+Binary files a/src/datamodules/__pycache__/datamodule.cpython-38.pyc and b/src/datamodules/__pycache__/datamodule.cpython-38.pyc differ
+diff --git a/src/datamodules/datamodule.py b/src/datamodules/datamodule.py
+index 4fcab3d..00a31df 100644
+--- a/src/datamodules/datamodule.py
++++ b/src/datamodules/datamodule.py
+@@ -7,50 +7,6 @@ from torch.utils.data import DataLoader, Subset
+ from pytorch_lightning import LightningDataModule
+ from src.data import collate_fn, SamplerFactory
+ 
+-@dataclass
+-class MNISTDataModule(LightningDataModule):
+-    
+-    def __init__(self,data: Dict,loader: Dict,dataset: Dict):
+-        super().__init__()
+-        self.prepare_data_per_node = True
+-        self.data = data
+-        self.loader = loader
+-        self.dataset = dataset
+-    def setup(self, stage: Optional[str] = None):
+-        if stage in ("fit", None):
+-            self.train_dataset = self.dataset(n_samples = self.loader.samples.train_sample, is_train=True)
+-            self.val_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-
+-        if stage in ("test", None):
+-            self.test_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-    
+-    def train_dataloader(self):
+-        conf = deepcopy(self.loader.train)
+-        batch_size = conf.pop("batch_size")
+-        conf.shuffle = False
+-        # return DataLoader(self.train_dataset, **conf, collate_fn=collate_fn)
+-
+-        return DataLoader(
+-            self.train_dataset,
+-            **conf,
+-            collate_fn=collate_fn,
+-            batch_sampler=SamplerFactory().get(
+-                class_idxs=[
+-                    np.where(self.train_dataset.labels == i)[0].tolist()
+-                    for i in range(10)
+-                ],
+-                batch_size=batch_size,
+-                n_batches=len(self.train_dataset)//batch_size + 5,
+-                alpha=1.0,
+-                kind="random",
+-            ),
+-        )
+-
+-    def val_dataloader(self):
+-        return DataLoader(self.val_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-    def test_dataloader(self):
+-        return DataLoader(self.test_dataset, **self.loader.eval, collate_fn=collate_fn)
+ 
+ @dataclass
+ class DataModule(LightningDataModule):
+diff --git a/src/layers/__pycache__/__init__.cpython-38.pyc b/src/layers/__pycache__/__init__.cpython-38.pyc
+index 31453e2..a73c856 100644
+Binary files a/src/layers/__pycache__/__init__.cpython-38.pyc and b/src/layers/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/layers/__pycache__/quantum_layers.cpython-38.pyc b/src/layers/__pycache__/quantum_layers.cpython-38.pyc
+index 4f219f9..3e30b2e 100644
+Binary files a/src/layers/__pycache__/quantum_layers.cpython-38.pyc and b/src/layers/__pycache__/quantum_layers.cpython-38.pyc differ
+diff --git a/src/layers/quantum_layers.py b/src/layers/quantum_layers.py
+index 976aa6a..4dfccfa 100644
+--- a/src/layers/quantum_layers.py
++++ b/src/layers/quantum_layers.py
+@@ -35,7 +35,7 @@ class QuantumCircuit:
+         for theta, qubit in zip(self.theta, self.all_qubits):
+             self._circuit.ry(theta, qubit)
+ 
+-        for i in [2,3,4,5]:
++        for i in range(2, self.n_qubit):
+             self._circuit.cx(0,i)
+ 
+         self._circuit.measure_all()
+diff --git a/src/models/__pycache__/__init__.cpython-38.pyc b/src/models/__pycache__/__init__.cpython-38.pyc
+index f5a0d8c..9643058 100644
+Binary files a/src/models/__pycache__/__init__.cpython-38.pyc and b/src/models/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/models/__pycache__/qhn.cpython-38.pyc b/src/models/__pycache__/qhn.cpython-38.pyc
+index 56e2a1f..8b4a754 100644
+Binary files a/src/models/__pycache__/qhn.cpython-38.pyc and b/src/models/__pycache__/qhn.cpython-38.pyc differ
+diff --git a/src/models/qhn.py b/src/models/qhn.py
+index c74fec4..2e08fba 100644
+--- a/src/models/qhn.py
++++ b/src/models/qhn.py
+@@ -27,10 +27,10 @@ class Simple_QHN(nn.Module):
+         self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+         self.hybrid = Hybrid(
+             params.n_qubits,
+-            qiskit.Aer.get_backend("aer_simulator"),
++            params.backend,
+             100,
+             shift=params.shift,
+-            is_cnot=params.is_cnot,
++            
+         )
+         self.fc3 = nn.Linear(params.n_qubits * 2, 2)
+         self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+@@ -55,35 +55,3 @@ class Simple_QHN(nn.Module):
+         return x
+ 
+ 
+-class MNIST_QHN(nn.Module):
+-    def __init__(self, params: Optional[Dict] = None, *args, **kwargs) -> None:
+-        super(MNIST_QHN, self).__init__()
+-        params = params.MNIST_QHN
+-        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
+-        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
+-        self.dropout = nn.Dropout2d()
+-        self.fc1 = nn.Linear(256, params.linear_out)
+-        self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+-        self.hybrid = Hybrid(
+-            params.n_qubits,
+-            backend = "aer_simulator",
+-            shots = params.shots,
+-            shift=params.shift,
+-        )
+-        self.fc3 = nn.Linear(2**params.n_qubits, 10)
+-        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+-
+-    def forward(self, x):
+-        x = x.unsqueeze(1)
+-        x = F.relu(self.conv1(x))
+-        x = F.max_pool2d(x, 2)
+-        x = F.relu(self.conv2(x))
+-        x = F.max_pool2d(x, 2)
+-        
+-        x = x.view(x.shape[0],-1)
+-        x = F.relu(self.fc1(x))
+-        x = self.fc2(x)
+-        x = torch.tanh(x) * torch.ones_like(x) * torch.tensor(np.pi / 2)
+-        x = self.hybrid(x).to(self.device)
+-        x = F.softmax(self.fc3(x), dim=1)
+-        return x
+diff --git a/src/runners/__pycache__/base.cpython-38.pyc b/src/runners/__pycache__/base.cpython-38.pyc
+index 6fae27c..b7d08e9 100644
+Binary files a/src/runners/__pycache__/base.cpython-38.pyc and b/src/runners/__pycache__/base.cpython-38.pyc differ
+diff --git a/src/runners/__pycache__/runner.cpython-38.pyc b/src/runners/__pycache__/runner.cpython-38.pyc
+index 9813d76..3792f76 100644
+Binary files a/src/runners/__pycache__/runner.cpython-38.pyc and b/src/runners/__pycache__/runner.cpython-38.pyc differ
+diff --git a/src/runners/runner.py b/src/runners/runner.py
+index 86b61a5..5fb03f3 100644
+--- a/src/runners/runner.py
++++ b/src/runners/runner.py
+@@ -6,8 +6,8 @@ from typing import Optional
+ from copy import deepcopy
+ 
+ from src.runners import Base_Runner
+-from src.data import ROIDataset, SITES_DICT, MNISTDataset
+-from src.datamodules import DataModule, MNISTDataModule
++from src.data import ROIDataset, SITES_DICT
++from src.datamodules import DataModule
+ from src.tasks import ClassificationTask
+ from src.utils import plot_paper
+ from src.callbacks import wandb_callback as wbc
+@@ -177,106 +177,4 @@ class S_Runner(Base_Runner):
+ 
+         return
+ 
+-class MNIST_Runner(Base_Runner):
+-    def get_callbacks(self):
+-        """
+-        Write only callbacks that logger is not necessary
+-        """
+-        checkpoint_callback = ModelCheckpoint(
+-            dirpath=os.path.join(
+-                self.log.checkpoint_path,
+-                self.log.project_name,
+-                f"version{self.version:03d}",
+-            ),
+-            filename=os.path.join(f"model"),
+-            monitor=f"Accuracy/val",
+-            mode="max",
+-            verbose=False,
+-            save_top_k=1,
+-        )
+-
+-        callbacks = dict(
+-            filter(lambda item: item[0].endswith("callback"), vars().items())
+-        ).values()
+-        callbacks = list(callbacks)
+-        return callbacks if len(callbacks) > 0 else None
+-
+-    def run(self, profiler: Optional[str] = None):
+-        os.makedirs(
+-            os.path.join(self.log.checkpoint_path, self.log.project_name), exist_ok=True
+-        )
+-        self.version = len(
+-            os.listdir(os.path.join(self.log.checkpoint_path, self.log.project_name))
+-        )
+-
+-
+-        final_results = list()
+-
+-
+-
+-        dm = self.get_datamodule(dataset=MNISTDataset, datamodule=MNISTDataModule)
+-        model = self.get_network(Task=ClassificationTask)
+-        model.apply(initialize_weights)
+-
+-        trainer = Trainer(
+-            logger=[
+-                TensorBoardLogger(
+-                    save_dir=self.log.log_path,
+-                    name=os.path.join(
+-                        self.log.project_name,
+-                        f"version{self.version:03d}"
+-                    ),
+-                    default_hp_metric=False,
+-                    version=None,
+-                    # log_graph=True, # inavailable due to bug
+-                ),
+-                WandbLogger(
+-                    project=self.log.project_name,
+-                    save_dir=self.log.log_path,
+-                ),
+-            ],
+-            # ! use all gpu
+-            # gpus=-1,
+-            # auto_select_gpus=True,
+-            # ! use 2 gpu
+-            # devices=2,
+-            # accelerator="auto",
+-            # strategy="ddp",
+-            # ! use gpu 0
+-            devices=[0],
+-            accelerator="gpu",
+-            #devices=[self.log.device.gpu],
+-            #accelerator="cpu",
+-            check_val_every_n_epoch=self.log.val_log_freq_epoch,
+-            log_every_n_steps=1,
+-            num_sanity_val_steps=0,
+-            max_epochs=self.log.epoch,
+-            profiler=profiler,
+-            callbacks=[
+-                *self.get_callbacks(),
+-                wbc.WatchModel(),
+-                wbc.LogConfusionMatrix(),
+-                wbc.LogF1PrecRecHeatmap(),
+-                # tbc.WatchModel(),
+-                # tbc.LogConfusionMatrix(),
+-                # tbc.LogF1PrecRecHeatmap(),
+-            ],
+-            precision=self.log.precision,
+-            # gradient_clip_val=0.5,
+-        )
+-        trainer.test_site_prefix = model.prefix
+-        trainer.fit(model, datamodule=dm)
+-        trainer.test(model, datamodule=dm, ckpt_path="best")
+-        final_results.append(
+-            trainer.callback_metrics[f"Accuracy/test"]
+-        )
+-
+-    try:
+-        import wandb
+-
+-        wb_logger = wbc.get_wandb_logger(trainer=Trainer)
+-        
+-    except Exception as e:
+-        print(e)
+-
+     
diff --git a/Logs/wandb/run-20220704_224416-2kr91mtl/files/output.log b/Logs/wandb/run-20220704_224416-2kr91mtl/files/output.log
new file mode 100644
index 0000000..474a19c
--- /dev/null
+++ b/Logs/wandb/run-20220704_224416-2kr91mtl/files/output.log
@@ -0,0 +1,57 @@
+GPU available: True, used: True
+TPU available: False, using: 0 TPU cores
+IPU available: False, using: 0 IPUs
+HPU available: False, using: 0 HPUs
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1162, in _run
+    self._data_connector.prepare_data()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py", line 114, in prepare_data
+    dm_prepare_data_per_node = datamodule.prepare_data_per_node
+AttributeError: 'DataModule' object has no attribute 'prepare_data_per_node'
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1162, in _run
+    self._data_connector.prepare_data()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py", line 114, in prepare_data
+    dm_prepare_data_per_node = datamodule.prepare_data_per_node
+AttributeError: 'DataModule' object has no attribute 'prepare_data_per_node'
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1162, in _run
+    self._data_connector.prepare_data()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py", line 114, in prepare_data
+    dm_prepare_data_per_node = datamodule.prepare_data_per_node
diff --git a/Logs/wandb/run-20220704_224416-2kr91mtl/files/requirements.txt b/Logs/wandb/run-20220704_224416-2kr91mtl/files/requirements.txt
new file mode 100644
index 0000000..f9f8632
--- /dev/null
+++ b/Logs/wandb/run-20220704_224416-2kr91mtl/files/requirements.txt
@@ -0,0 +1,166 @@
+absl-py==1.0.0
+aiohttp==3.8.1
+aiosignal==1.2.0
+antlr4-python3-runtime==4.8
+async-timeout==4.0.2
+attrs==19.3.0
+automat==0.8.0
+autopep8==1.6.0
+blinker==1.4
+cachetools==5.0.0
+certifi==2019.11.28
+chardet==3.0.4
+charset-normalizer==2.0.12
+click==7.0
+cloud-init==22.1
+colorama==0.4.3
+command-not-found==0.3
+configobj==5.0.6
+constantly==15.1.0
+cryptography==2.8
+cssselect==1.1.0
+cycler==0.11.0
+dbus-python==1.2.16
+dill==0.3.4
+distro-info==0.23ubuntu1
+distro==1.4.0
+docker-pycreds==0.4.0
+entrypoints==0.3
+filelock==3.6.0
+fonttools==4.33.3
+frozenlist==1.3.0
+fsspec==2022.3.0
+gistory==0.44
+gitdb==4.0.9
+gitpython==3.1.27
+google-auth-oauthlib==0.4.6
+google-auth==2.6.6
+grpcio==1.44.0
+httplib2==0.14.0
+hyperlink==19.0.0
+idna==2.8
+importlib-metadata==4.11.3
+incremental==16.10.1
+itemadapter==0.5.0
+itemloaders==1.0.4
+jinja2==2.10.1
+jmespath==1.0.0
+joblib==1.1.0
+jsonpatch==1.22
+jsonpointer==2.0
+jsonschema==3.2.0
+keyring==18.0.1
+kiwisolver==1.4.2
+language-selector==0.1
+launchpadlib==1.10.13
+lazr.restfulclient==0.14.2
+lazr.uri==1.0.3
+lxml==4.8.0
+markdown==3.3.6
+markupsafe==1.1.0
+matplotlib==3.5.1
+more-itertools==4.2.0
+mpmath==1.2.1
+multidict==6.0.2
+netifaces==0.10.4
+nibabel==3.2.2
+nilearn==0.9.1
+ntlm-auth==1.5.0
+numpy==1.22.3
+oauthlib==3.1.0
+omegaconf==2.1.2
+packaging==21.3
+pandas==1.4.2
+parsel==1.6.0
+pathtools==0.1.2
+pbr==5.8.1
+pexpect==4.6.0
+pillow==9.1.0
+pip==20.0.2
+ply==3.11
+promise==2.3
+protego==0.2.1
+protobuf==3.20.1
+psutil==5.9.0
+pyasn1-modules==0.2.1
+pyasn1==0.4.2
+pycodestyle==2.8.0
+pydeprecate==0.3.2
+pydispatcher==2.0.5
+pygame==2.1.2
+pygobject==3.36.0
+pyhamcrest==1.9.0
+pyjwt==1.7.1
+pymacaroons==0.13.0
+pynacl==1.3.0
+pyopenssl==19.0.0
+pyparsing==3.0.8
+pyrsistent==0.15.5
+pyserial==3.4
+python-apt==2.0.0+ubuntu0.20.4.7
+python-constraint==1.4.0
+python-dateutil==2.8.2
+python-debian==0.1.36ubuntu1
+pytorch-lightning==1.6.2
+pytz==2022.1
+pyyaml==6.0
+qiskit-aer==0.10.4
+qiskit-ibmq-provider==0.19.1
+qiskit-ignis==0.7.0
+qiskit-terra==0.20.1
+qiskit==0.36.1
+queuelib==1.6.2
+requests-file==1.5.1
+requests-ntlm==1.1.0
+requests-oauthlib==1.3.1
+requests-unixsocket==0.2.0
+requests==2.22.0
+retworkx==0.11.0
+rsa==4.8
+scikit-learn==1.0.2
+scipy==1.8.0
+seaborn==0.11.2
+secretstorage==2.3.1
+sentry-sdk==1.5.10
+service-identity==18.1.0
+setproctitle==1.2.3
+setuptools==45.2.0
+shortuuid==1.0.8
+simplejson==3.16.0
+six==1.14.0
+sklearn==0.0
+smmap==5.0.0
+sos==4.3
+ssh-import-id==5.10
+stevedore==3.5.0
+symengine==0.9.2
+sympy==1.10.1
+systemd-python==234
+tensorboard-data-server==0.6.1
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.8.0
+threadpoolctl==3.1.0
+tldextract==3.2.1
+toml==0.10.2
+torch-tb-profiler==0.4.0
+torch==1.12.0
+torchmetrics==0.8.1
+torchvision==0.13.0
+tqdm==4.64.0
+tweedledum==1.1.1
+twisted==18.9.0
+typing-extensions==4.2.0
+ubuntu-advantage-tools==27.7
+ufw==0.36
+unattended-upgrades==0.1
+urllib3==1.25.8
+w3lib==1.22.0
+wadllib==1.3.3
+wandb==0.12.15
+websocket-client==1.3.2
+websockets==10.3
+werkzeug==2.1.2
+wheel==0.34.2
+yarl==1.7.2
+zipp==1.0.0
+zope.interface==4.7.1
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-metadata.json b/Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-metadata.json
new file mode 100644
index 0000000..d05035e
--- /dev/null
+++ b/Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-metadata.json
@@ -0,0 +1,24 @@
+{
+    "os": "Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2022-07-04T13:44:18.208697",
+    "startedAt": "2022-07-04T13:44:16.398022",
+    "docker": null,
+    "gpu": "NVIDIA GeForce GTX 1650",
+    "gpu_count": 1,
+    "cpu_count": 8,
+    "cuda": null,
+    "args": [],
+    "state": "running",
+    "program": "/home/sol/git/QML2/run.py",
+    "codePath": "run.py",
+    "git": {
+        "remote": "https://github.com/PVNkT/QML2.git",
+        "commit": "1f3b04ad3f84cb17a3713da979f0adf1e2028c86"
+    },
+    "email": "3031902@gmail.com",
+    "root": "/home/sol/git/QML2",
+    "host": "DESKTOP-HFMS9NT",
+    "username": "sol",
+    "executable": "/bin/python3"
+}
diff --git a/Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-summary.json b/Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-summary.json
new file mode 100644
index 0000000..971d479
--- /dev/null
+++ b/Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 1}}
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_224416-2kr91mtl/logs/debug-internal.log b/Logs/wandb/run-20220704_224416-2kr91mtl/logs/debug-internal.log
new file mode 100644
index 0000000..ef0f022
--- /dev/null
+++ b/Logs/wandb/run-20220704_224416-2kr91mtl/logs/debug-internal.log
@@ -0,0 +1,176 @@
+2022-07-04 22:44:16,407 INFO    StreamThr :2221 [internal.py:wandb_internal():90] W&B internal server running at pid: 2221, started at: 2022-07-04 22:44:16.406704
+2022-07-04 22:44:16,411 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: status
+2022-07-04 22:44:16,413 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: status
+2022-07-04 22:44:16,415 DEBUG   SenderThread:2221 [sender.py:send():232] send: header
+2022-07-04 22:44:16,415 INFO    WriterThread:2221 [datastore.py:open_for_write():75] open: Logs/wandb/run-20220704_224416-2kr91mtl/run-2kr91mtl.wandb
+2022-07-04 22:44:16,420 DEBUG   SenderThread:2221 [sender.py:send():232] send: run
+2022-07-04 22:44:16,423 INFO    SenderThread:2221 [sender.py:_maybe_setup_resume():489] checking resume status for None/QML-MNIST/2kr91mtl
+2022-07-04 22:44:17,053 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: check_version
+2022-07-04 22:44:17,057 INFO    SenderThread:2221 [dir_watcher.py:__init__():166] watching files in: Logs/wandb/run-20220704_224416-2kr91mtl/files
+2022-07-04 22:44:17,057 INFO    SenderThread:2221 [sender.py:_start_run_threads():811] run started: 2kr91mtl with start time 1656942256
+2022-07-04 22:44:17,058 DEBUG   SenderThread:2221 [sender.py:send():232] send: summary
+2022-07-04 22:44:17,058 INFO    SenderThread:2221 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:44:17,058 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: check_version
+2022-07-04 22:44:17,234 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: run_start
+2022-07-04 22:44:18,058 INFO    Thread-12 :2221 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-summary.json
+2022-07-04 22:44:18,208 DEBUG   HandlerThread:2221 [meta.py:__init__():35] meta init
+2022-07-04 22:44:18,208 DEBUG   HandlerThread:2221 [meta.py:__init__():49] meta init done
+2022-07-04 22:44:18,208 DEBUG   HandlerThread:2221 [meta.py:probe():209] probe
+2022-07-04 22:44:18,218 DEBUG   HandlerThread:2221 [meta.py:_setup_git():199] setup git
+2022-07-04 22:44:18,227 DEBUG   HandlerThread:2221 [meta.py:_setup_git():206] setup git done
+2022-07-04 22:44:18,227 DEBUG   HandlerThread:2221 [meta.py:_save_code():87] save code
+2022-07-04 22:44:18,233 DEBUG   HandlerThread:2221 [meta.py:_save_code():108] save code done
+2022-07-04 22:44:18,233 DEBUG   HandlerThread:2221 [meta.py:_save_patches():125] save patches
+2022-07-04 22:44:18,268 DEBUG   HandlerThread:2221 [meta.py:_save_patches():167] save patches done
+2022-07-04 22:44:18,268 DEBUG   HandlerThread:2221 [meta.py:_save_pip():53] save pip
+2022-07-04 22:44:18,268 DEBUG   HandlerThread:2221 [meta.py:_save_pip():67] save pip done
+2022-07-04 22:44:18,268 DEBUG   HandlerThread:2221 [meta.py:probe():247] probe done
+2022-07-04 22:44:18,328 DEBUG   SenderThread:2221 [sender.py:send():232] send: files
+2022-07-04 22:44:18,328 INFO    SenderThread:2221 [sender.py:_save_file():946] saving file wandb-metadata.json with policy now
+2022-07-04 22:44:18,329 INFO    SenderThread:2221 [sender.py:_save_file():946] saving file code/run.py with policy now
+2022-07-04 22:44:18,329 INFO    SenderThread:2221 [sender.py:_save_file():946] saving file diff.patch with policy now
+2022-07-04 22:44:18,337 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: stop_status
+2022-07-04 22:44:18,338 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: stop_status
+2022-07-04 22:44:18,408 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:18,660 DEBUG   SenderThread:2221 [sender.py:send():232] send: telemetry
+2022-07-04 22:44:18,662 DEBUG   SenderThread:2221 [sender.py:send():232] send: metric
+2022-07-04 22:44:18,662 DEBUG   SenderThread:2221 [sender.py:send():232] send: telemetry
+2022-07-04 22:44:18,662 DEBUG   SenderThread:2221 [sender.py:send():232] send: metric
+2022-07-04 22:44:18,662 WARNING SenderThread:2221 [sender.py:send_metric():904] Seen metric with glob (shouldn't happen)
+2022-07-04 22:44:18,662 DEBUG   SenderThread:2221 [sender.py:send():232] send: exit
+2022-07-04 22:44:18,662 INFO    SenderThread:2221 [sender.py:send_exit():368] handling exit code: 1
+2022-07-04 22:44:18,663 INFO    SenderThread:2221 [sender.py:send_exit():370] handling runtime: 1
+2022-07-04 22:44:18,663 INFO    SenderThread:2221 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:44:18,663 INFO    SenderThread:2221 [sender.py:send_exit():376] send defer
+2022-07-04 22:44:18,663 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:18,664 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:18,664 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 0
+2022-07-04 22:44:18,664 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:18,664 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 0
+2022-07-04 22:44:18,664 INFO    SenderThread:2221 [sender.py:transition_state():389] send defer: 1
+2022-07-04 22:44:18,665 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:18,665 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 1
+2022-07-04 22:44:19,058 INFO    Thread-12 :2221 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-summary.json
+2022-07-04 22:44:19,058 INFO    Thread-12 :2221 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224416-2kr91mtl/files/code/run.py
+2022-07-04 22:44:19,058 INFO    Thread-12 :2221 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-metadata.json
+2022-07-04 22:44:19,058 INFO    Thread-12 :2221 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224416-2kr91mtl/files/requirements.txt
+2022-07-04 22:44:19,059 INFO    Thread-12 :2221 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224416-2kr91mtl/files/output.log
+2022-07-04 22:44:19,059 INFO    Thread-12 :2221 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224416-2kr91mtl/files/diff.patch
+2022-07-04 22:44:19,059 INFO    Thread-12 :2221 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224416-2kr91mtl/files/code
+2022-07-04 22:44:19,464 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:19,465 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:19,465 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 1
+2022-07-04 22:44:19,465 INFO    SenderThread:2221 [sender.py:transition_state():389] send defer: 2
+2022-07-04 22:44:19,465 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:19,465 DEBUG   SenderThread:2221 [sender.py:send():232] send: stats
+2022-07-04 22:44:19,465 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:19,466 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 2
+2022-07-04 22:44:19,466 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:19,466 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 2
+2022-07-04 22:44:19,466 INFO    SenderThread:2221 [sender.py:transition_state():389] send defer: 3
+2022-07-04 22:44:19,468 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:19,468 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 3
+2022-07-04 22:44:19,468 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:19,468 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 3
+2022-07-04 22:44:19,468 INFO    SenderThread:2221 [sender.py:transition_state():389] send defer: 4
+2022-07-04 22:44:19,470 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:19,470 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 4
+2022-07-04 22:44:19,470 DEBUG   SenderThread:2221 [sender.py:send():232] send: summary
+2022-07-04 22:44:19,471 INFO    SenderThread:2221 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:44:19,471 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:19,471 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 4
+2022-07-04 22:44:19,471 INFO    SenderThread:2221 [sender.py:transition_state():389] send defer: 5
+2022-07-04 22:44:19,472 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:19,472 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 5
+2022-07-04 22:44:19,473 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:19,473 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 5
+2022-07-04 22:44:19,483 INFO    Thread-18 :2221 [upload_job.py:push():137] Uploaded file /tmp/tmpqecvav7nwandb/3i0zq0my-diff.patch
+2022-07-04 22:44:19,489 INFO    Thread-16 :2221 [upload_job.py:push():137] Uploaded file /tmp/tmpqecvav7nwandb/5p7qq2di-wandb-metadata.json
+2022-07-04 22:44:19,492 INFO    Thread-17 :2221 [upload_job.py:push():137] Uploaded file /tmp/tmpqecvav7nwandb/1ayttym8-code/run.py
+2022-07-04 22:44:19,571 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:20,058 INFO    Thread-12 :2221 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-summary.json
+2022-07-04 22:44:20,345 INFO    SenderThread:2221 [sender.py:transition_state():389] send defer: 6
+2022-07-04 22:44:20,346 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:20,346 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:20,347 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 6
+2022-07-04 22:44:20,347 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:20,347 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 6
+2022-07-04 22:44:20,347 INFO    SenderThread:2221 [dir_watcher.py:finish():279] shutting down directory watcher
+2022-07-04 22:44:20,448 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:21,059 INFO    Thread-12 :2221 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224416-2kr91mtl/files/output.log
+2022-07-04 22:44:21,059 INFO    SenderThread:2221 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224416-2kr91mtl/files/config.yaml
+2022-07-04 22:44:21,059 INFO    SenderThread:2221 [dir_watcher.py:finish():309] scan: Logs/wandb/run-20220704_224416-2kr91mtl/files
+2022-07-04 22:44:21,060 INFO    SenderThread:2221 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224416-2kr91mtl/files/output.log output.log
+2022-07-04 22:44:21,060 INFO    SenderThread:2221 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224416-2kr91mtl/files/config.yaml config.yaml
+2022-07-04 22:44:21,060 INFO    SenderThread:2221 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224416-2kr91mtl/files/diff.patch diff.patch
+2022-07-04 22:44:21,060 INFO    SenderThread:2221 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-metadata.json wandb-metadata.json
+2022-07-04 22:44:21,060 INFO    SenderThread:2221 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224416-2kr91mtl/files/requirements.txt requirements.txt
+2022-07-04 22:44:21,066 INFO    SenderThread:2221 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-summary.json wandb-summary.json
+2022-07-04 22:44:21,067 INFO    SenderThread:2221 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224416-2kr91mtl/files/code/run.py code/run.py
+2022-07-04 22:44:21,067 INFO    SenderThread:2221 [sender.py:transition_state():389] send defer: 7
+2022-07-04 22:44:21,067 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:21,072 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:21,072 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 7
+2022-07-04 22:44:21,073 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:21,073 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 7
+2022-07-04 22:44:21,073 INFO    SenderThread:2221 [file_pusher.py:finish():145] shutting down file pusher
+2022-07-04 22:44:21,173 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:21,174 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:21,275 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:21,275 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:21,377 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:21,378 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:21,480 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:21,480 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:21,581 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:21,582 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:21,683 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:21,683 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:21,784 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:21,784 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:21,886 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:21,886 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:21,984 INFO    Thread-19 :2221 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224416-2kr91mtl/files/output.log
+2022-07-04 22:44:21,987 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:21,987 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:21,989 INFO    Thread-21 :2221 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224416-2kr91mtl/files/requirements.txt
+2022-07-04 22:44:22,008 INFO    Thread-20 :2221 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224416-2kr91mtl/files/config.yaml
+2022-07-04 22:44:22,010 INFO    Thread-22 :2221 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224416-2kr91mtl/files/wandb-summary.json
+2022-07-04 22:44:22,089 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:22,089 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:22,191 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:22,191 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:22,212 INFO    Thread-11 :2221 [sender.py:transition_state():389] send defer: 8
+2022-07-04 22:44:22,213 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:22,213 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 8
+2022-07-04 22:44:22,213 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:22,213 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 8
+2022-07-04 22:44:22,292 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:22,679 INFO    SenderThread:2221 [sender.py:transition_state():389] send defer: 9
+2022-07-04 22:44:22,679 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:22,680 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:22,680 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 9
+2022-07-04 22:44:22,680 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:22,680 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 9
+2022-07-04 22:44:22,680 INFO    SenderThread:2221 [sender.py:transition_state():389] send defer: 10
+2022-07-04 22:44:22,681 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:44:22,681 INFO    HandlerThread:2221 [handler.py:handle_request_defer():164] handle defer: 10
+2022-07-04 22:44:22,681 DEBUG   SenderThread:2221 [sender.py:send():232] send: final
+2022-07-04 22:44:22,681 DEBUG   SenderThread:2221 [sender.py:send():232] send: footer
+2022-07-04 22:44:22,681 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:44:22,681 INFO    SenderThread:2221 [sender.py:send_request_defer():385] handle sender defer: 10
+2022-07-04 22:44:22,781 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:44:22,781 DEBUG   SenderThread:2221 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:44:22,781 INFO    SenderThread:2221 [file_pusher.py:join():150] waiting for file pusher
+2022-07-04 22:44:23,445 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: sampled_history
+2022-07-04 22:44:23,446 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: get_summary
+2022-07-04 22:44:23,446 INFO    MainThread:2221 [wandb_run.py:_footer_history_summary_info():3102] rendering history
+2022-07-04 22:44:23,446 INFO    MainThread:2221 [wandb_run.py:_footer_history_summary_info():3134] rendering summary
+2022-07-04 22:44:23,446 INFO    MainThread:2221 [wandb_run.py:_footer_sync_info():3057] logging synced files
+2022-07-04 22:44:23,447 DEBUG   HandlerThread:2221 [handler.py:handle_request():141] handle_request: shutdown
+2022-07-04 22:44:23,447 INFO    HandlerThread:2221 [handler.py:finish():806] shutting down handler
+2022-07-04 22:44:23,681 INFO    WriterThread:2221 [datastore.py:close():279] close: Logs/wandb/run-20220704_224416-2kr91mtl/run-2kr91mtl.wandb
+2022-07-04 22:44:24,344 INFO    SenderThread:2221 [sender.py:finish():1106] shutting down sender
+2022-07-04 22:44:24,344 INFO    SenderThread:2221 [file_pusher.py:finish():145] shutting down file pusher
+2022-07-04 22:44:24,344 INFO    SenderThread:2221 [file_pusher.py:join():150] waiting for file pusher
+2022-07-04 22:44:24,449 INFO    MainThread:2221 [internal.py:handle_exit():80] Internal process exited
diff --git a/Logs/wandb/run-20220704_224416-2kr91mtl/logs/debug.log b/Logs/wandb/run-20220704_224416-2kr91mtl/logs/debug.log
new file mode 100644
index 0000000..1312471
--- /dev/null
+++ b/Logs/wandb/run-20220704_224416-2kr91mtl/logs/debug.log
@@ -0,0 +1,24 @@
+2022-07-04 22:44:16,399 INFO    MainThread:2178 [wandb_setup.py:_flush():75] Loading settings from /home/sol/.config/wandb/settings
+2022-07-04 22:44:16,399 INFO    MainThread:2178 [wandb_setup.py:_flush():75] Loading settings from /home/sol/git/QML2/wandb/settings
+2022-07-04 22:44:16,399 INFO    MainThread:2178 [wandb_setup.py:_flush():75] Loading settings from environment variables: {'_require_service': 'True'}
+2022-07-04 22:44:16,399 INFO    MainThread:2178 [wandb_setup.py:_flush():75] Inferring run settings from compute environment: {'program_relpath': 'run.py', 'program': '/home/sol/git/QML2/run.py'}
+2022-07-04 22:44:16,399 INFO    MainThread:2178 [wandb_init.py:_log_setup():437] Logging user logs to Logs/wandb/run-20220704_224416-2kr91mtl/logs/debug.log
+2022-07-04 22:44:16,399 INFO    MainThread:2178 [wandb_init.py:_log_setup():438] Logging internal logs to Logs/wandb/run-20220704_224416-2kr91mtl/logs/debug-internal.log
+2022-07-04 22:44:16,400 INFO    MainThread:2178 [wandb_init.py:init():471] calling init triggers
+2022-07-04 22:44:16,400 INFO    MainThread:2178 [wandb_init.py:init():474] wandb.init called with sweep_config: {}
+config: {}
+2022-07-04 22:44:16,400 INFO    MainThread:2178 [wandb_init.py:init():524] starting backend
+2022-07-04 22:44:16,401 INFO    MainThread:2178 [backend.py:_multiprocessing_setup():97] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2022-07-04 22:44:16,403 INFO    MainThread:2178 [wandb_init.py:init():533] backend started and connected
+2022-07-04 22:44:16,406 INFO    MainThread:2178 [wandb_init.py:init():597] updated telemetry
+2022-07-04 22:44:16,418 INFO    MainThread:2178 [wandb_init.py:init():628] communicating run to backend with 30 second timeout
+2022-07-04 22:44:17,052 INFO    MainThread:2178 [wandb_run.py:_on_init():1923] communicating current version
+2022-07-04 22:44:17,228 INFO    MainThread:2178 [wandb_run.py:_on_init():1927] got version response upgrade_message: "wandb version 0.12.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2022-07-04 22:44:17,228 INFO    MainThread:2178 [wandb_init.py:init():659] starting run threads in backend
+2022-07-04 22:44:18,333 INFO    MainThread:2178 [wandb_run.py:_console_start():1897] atexit reg
+2022-07-04 22:44:18,334 INFO    MainThread:2178 [wandb_run.py:_redirect():1770] redirect: SettingsConsole.WRAP
+2022-07-04 22:44:18,335 INFO    MainThread:2178 [wandb_run.py:_redirect():1807] Wrapping output streams.
+2022-07-04 22:44:18,336 INFO    MainThread:2178 [wandb_run.py:_redirect():1831] Redirects installed.
+2022-07-04 22:44:18,337 INFO    MainThread:2178 [wandb_init.py:init():684] run started, returning control to user process
+2022-07-04 22:44:24,449 WARNING MsgRouterThr:2178 [router.py:message_loop():76] message_loop has been closed
diff --git a/Logs/wandb/run-20220704_224416-2kr91mtl/run-2kr91mtl.wandb b/Logs/wandb/run-20220704_224416-2kr91mtl/run-2kr91mtl.wandb
new file mode 100644
index 0000000..2c9bf41
Binary files /dev/null and b/Logs/wandb/run-20220704_224416-2kr91mtl/run-2kr91mtl.wandb differ
diff --git a/Logs/wandb/run-20220704_224732-2j80fpj7/files/code/run.py b/Logs/wandb/run-20220704_224732-2j80fpj7/files/code/run.py
new file mode 100644
index 0000000..69d3f58
--- /dev/null
+++ b/Logs/wandb/run-20220704_224732-2j80fpj7/files/code/run.py
@@ -0,0 +1,25 @@
+from omegaconf import OmegaConf
+from pathlib import Path
+
+from src.runners import S_Runner
+
+CONFIG_DIR = Path("Configs")
+
+
+def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+    model_params = OmegaConf.load(CONFIG_DIR / "models.yaml")
+    cfg = OmegaConf.merge(cfg, model_params)
+    cfg.merge_with_cli()
+
+    runner = S_Runner(
+        log=cfg.log,
+        optimizer=cfg.optimizer,
+        loader=cfg.loader,
+        network=cfg.network,
+        data=cfg.data,
+    )
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/Logs/wandb/run-20220704_224732-2j80fpj7/files/config.yaml b/Logs/wandb/run-20220704_224732-2j80fpj7/files/config.yaml
new file mode 100644
index 0000000..f82621e
--- /dev/null
+++ b/Logs/wandb/run-20220704_224732-2j80fpj7/files/config.yaml
@@ -0,0 +1,31 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.12.15
+    code_path: code/run.py
+    framework: lightning
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    m:
+    - 1: trainer/global_step
+      6:
+      - 3
+    python_version: 3.8.10
+    start_time: 1656942452
+    t:
+      1:
+      - 1
+      - 5
+      - 9
+      - 41
+      - 53
+      - 55
+      3:
+      - 7
+      - 23
+      4: 3.8.10
+      5: 0.12.15
+      8:
+      - 5
diff --git a/Logs/wandb/run-20220704_224732-2j80fpj7/files/diff.patch b/Logs/wandb/run-20220704_224732-2j80fpj7/files/diff.patch
new file mode 100644
index 0000000..bb68ced
--- /dev/null
+++ b/Logs/wandb/run-20220704_224732-2j80fpj7/files/diff.patch
@@ -0,0 +1,404 @@
+diff --git a/Configs/models.yaml b/Configs/models.yaml
+index 2c5c2ba..63b272d 100644
+--- a/Configs/models.yaml
++++ b/Configs/models.yaml
+@@ -8,9 +8,9 @@ LSTM:
+ Simple_QHN:
+   n_qubits: 2
+   shift: 0.6
+-  is_cnot: True
+   lstm_hidden: 128
+   linear_out: 64
++  backend: aer_simulator
+ 
+ 
+ quantum_circuit:
+@@ -18,5 +18,4 @@ quantum_circuit:
+   simulator: aer_simulator
+   shift: 0.5
+   shots: 100
+-  is_cnot: True
+   dense_type: 2
+\ No newline at end of file
+diff --git a/Logs/QML-MNIST/version000/version_0/hparams.yaml b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+index 3851257..000d975 100644
+--- a/Logs/QML-MNIST/version000/version_0/hparams.yaml
++++ b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+@@ -15,6 +15,5 @@ net:
+     simulator: aer_simulator
+     shift: 0.5
+     shots: 100
+-    is_cnot: true
+     dense_type: 2
+ inputs: null
+diff --git a/run.py b/run.py
+index c8e1ccf..69d3f58 100644
+--- a/run.py
++++ b/run.py
+@@ -1,7 +1,7 @@
+ from omegaconf import OmegaConf
+ from pathlib import Path
+ 
+-from src.runners import S_Runner, MNIST_Runner
++from src.runners import S_Runner
+ 
+ CONFIG_DIR = Path("Configs")
+ 
+@@ -11,7 +11,7 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+     cfg = OmegaConf.merge(cfg, model_params)
+     cfg.merge_with_cli()
+ 
+-    runner = MNIST_Runner(
++    runner = S_Runner(
+         log=cfg.log,
+         optimizer=cfg.optimizer,
+         loader=cfg.loader,
+@@ -22,4 +22,4 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+ 
+ 
+ if __name__ == "__main__":
+-    main(cfg=OmegaConf.load(CONFIG_DIR / "config MNIST.yaml"))
++    main()
+diff --git a/src/data/__pycache__/data_etl.cpython-38.pyc b/src/data/__pycache__/data_etl.cpython-38.pyc
+index 288d14c..934cbd5 100644
+Binary files a/src/data/__pycache__/data_etl.cpython-38.pyc and b/src/data/__pycache__/data_etl.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/loader.cpython-38.pyc b/src/data/__pycache__/loader.cpython-38.pyc
+index eddf358..67504ef 100644
+Binary files a/src/data/__pycache__/loader.cpython-38.pyc and b/src/data/__pycache__/loader.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/sampling.cpython-38.pyc b/src/data/__pycache__/sampling.cpython-38.pyc
+index a2cbaf3..a748fd2 100644
+Binary files a/src/data/__pycache__/sampling.cpython-38.pyc and b/src/data/__pycache__/sampling.cpython-38.pyc differ
+diff --git a/src/data/data_etl.py b/src/data/data_etl.py
+index 08700be..dc691b0 100755
+--- a/src/data/data_etl.py
++++ b/src/data/data_etl.py
+@@ -137,24 +137,6 @@ def chunks(lst, n):
+     return [lst[i : i + div] for i in range(0, len(lst), div)]
+ 
+ 
+-@dataclass
+-class Load_MNIST:
+-    def __init__(self, is_train:Boolean):
+-        if is_train:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=True, download=True,
+-                         transform=transforms.Compose([transforms.ToTensor()]))
+-        else:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=False, download=True,
+-                        transform=transforms.Compose([transforms.ToTensor()]))
+-                        
+-    def get_samples(self, n_samples:int):
+-        idx = []
+-        for i in range(10):
+-            idx.append(np.where(self.dataset.targets == i)[0][:n_samples])#각 target에 해당되는 번호를 저장한다. 
+-        idx = np.array(idx).reshape(-1)
+-        self.dataset.data = self.dataset.data[idx]
+-        self.dataset.targets = self.dataset.targets[idx]
+-        return self.dataset
+ 
+ 
+ 
+diff --git a/src/data/loader.py b/src/data/loader.py
+index 288a3b3..158cbdc 100644
+--- a/src/data/loader.py
++++ b/src/data/loader.py
+@@ -5,25 +5,8 @@ from typing import Union, List
+ import torch
+ from torch.utils.data import Dataset
+ 
+-from src.data import Load, SITES_DICT, Load_MNIST
+-
+-class MNISTDataset(Dataset):
+-    def __init__(self, n_samples: int, is_train: Boolean) -> None:
+-        Load = Load_MNIST(is_train)
+-        if n_samples >0:
+-            self.data = Load.get_samples(n_samples=n_samples).data
+-            self.labels = Load.get_samples(n_samples=n_samples).targets  
+-        else:  
+-            self.data = Load.dataset.data
+-            self.labels = Load.dataset.targets
++from src.data import Load, SITES_DICT
+ 
+-    def __len__(self):
+-        return len(self.labels)
+-    
+-    def __getitem__(self, index: int):
+-        data = self.data[index]
+-        label = self.labels[index]
+-        return data, label 
+   
+ class ROIDataset(Dataset):
+     def __init__( self, site: Union[List, str]) -> None:
+diff --git a/src/datamodules/__pycache__/datamodule.cpython-38.pyc b/src/datamodules/__pycache__/datamodule.cpython-38.pyc
+index 6f06b1a..295b581 100644
+Binary files a/src/datamodules/__pycache__/datamodule.cpython-38.pyc and b/src/datamodules/__pycache__/datamodule.cpython-38.pyc differ
+diff --git a/src/datamodules/datamodule.py b/src/datamodules/datamodule.py
+index 4fcab3d..b73f2f1 100644
+--- a/src/datamodules/datamodule.py
++++ b/src/datamodules/datamodule.py
+@@ -7,56 +7,15 @@ from torch.utils.data import DataLoader, Subset
+ from pytorch_lightning import LightningDataModule
+ from src.data import collate_fn, SamplerFactory
+ 
++
+ @dataclass
+-class MNISTDataModule(LightningDataModule):
+-    
+-    def __init__(self,data: Dict,loader: Dict,dataset: Dict):
++class DataModule(LightningDataModule):
++    def __init__(self, data: Dict, loader: Dict, dataset: Dict):
+         super().__init__()
+-        self.prepare_data_per_node = True
+         self.data = data
+         self.loader = loader
+         self.dataset = dataset
+-    def setup(self, stage: Optional[str] = None):
+-        if stage in ("fit", None):
+-            self.train_dataset = self.dataset(n_samples = self.loader.samples.train_sample, is_train=True)
+-            self.val_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-
+-        if stage in ("test", None):
+-            self.test_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-    
+-    def train_dataloader(self):
+-        conf = deepcopy(self.loader.train)
+-        batch_size = conf.pop("batch_size")
+-        conf.shuffle = False
+-        # return DataLoader(self.train_dataset, **conf, collate_fn=collate_fn)
+-
+-        return DataLoader(
+-            self.train_dataset,
+-            **conf,
+-            collate_fn=collate_fn,
+-            batch_sampler=SamplerFactory().get(
+-                class_idxs=[
+-                    np.where(self.train_dataset.labels == i)[0].tolist()
+-                    for i in range(10)
+-                ],
+-                batch_size=batch_size,
+-                n_batches=len(self.train_dataset)//batch_size + 5,
+-                alpha=1.0,
+-                kind="random",
+-            ),
+-        )
+-
+-    def val_dataloader(self):
+-        return DataLoader(self.val_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-    def test_dataloader(self):
+-        return DataLoader(self.test_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-@dataclass
+-class DataModule(LightningDataModule):
+-    data: Dict
+-    loader: Dict
+-    dataset: Dict
++        self.prepare_data_per_node = True
+ 
+     def setup(self, stage: Optional[str] = None):
+         if stage in ("fit", None):
+diff --git a/src/layers/__pycache__/__init__.cpython-38.pyc b/src/layers/__pycache__/__init__.cpython-38.pyc
+index 31453e2..a73c856 100644
+Binary files a/src/layers/__pycache__/__init__.cpython-38.pyc and b/src/layers/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/layers/__pycache__/quantum_layers.cpython-38.pyc b/src/layers/__pycache__/quantum_layers.cpython-38.pyc
+index 4f219f9..3e30b2e 100644
+Binary files a/src/layers/__pycache__/quantum_layers.cpython-38.pyc and b/src/layers/__pycache__/quantum_layers.cpython-38.pyc differ
+diff --git a/src/layers/quantum_layers.py b/src/layers/quantum_layers.py
+index 976aa6a..4dfccfa 100644
+--- a/src/layers/quantum_layers.py
++++ b/src/layers/quantum_layers.py
+@@ -35,7 +35,7 @@ class QuantumCircuit:
+         for theta, qubit in zip(self.theta, self.all_qubits):
+             self._circuit.ry(theta, qubit)
+ 
+-        for i in [2,3,4,5]:
++        for i in range(2, self.n_qubit):
+             self._circuit.cx(0,i)
+ 
+         self._circuit.measure_all()
+diff --git a/src/models/__pycache__/__init__.cpython-38.pyc b/src/models/__pycache__/__init__.cpython-38.pyc
+index f5a0d8c..9643058 100644
+Binary files a/src/models/__pycache__/__init__.cpython-38.pyc and b/src/models/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/models/__pycache__/qhn.cpython-38.pyc b/src/models/__pycache__/qhn.cpython-38.pyc
+index 56e2a1f..8b4a754 100644
+Binary files a/src/models/__pycache__/qhn.cpython-38.pyc and b/src/models/__pycache__/qhn.cpython-38.pyc differ
+diff --git a/src/models/qhn.py b/src/models/qhn.py
+index c74fec4..2e08fba 100644
+--- a/src/models/qhn.py
++++ b/src/models/qhn.py
+@@ -27,10 +27,10 @@ class Simple_QHN(nn.Module):
+         self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+         self.hybrid = Hybrid(
+             params.n_qubits,
+-            qiskit.Aer.get_backend("aer_simulator"),
++            params.backend,
+             100,
+             shift=params.shift,
+-            is_cnot=params.is_cnot,
++            
+         )
+         self.fc3 = nn.Linear(params.n_qubits * 2, 2)
+         self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+@@ -55,35 +55,3 @@ class Simple_QHN(nn.Module):
+         return x
+ 
+ 
+-class MNIST_QHN(nn.Module):
+-    def __init__(self, params: Optional[Dict] = None, *args, **kwargs) -> None:
+-        super(MNIST_QHN, self).__init__()
+-        params = params.MNIST_QHN
+-        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
+-        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
+-        self.dropout = nn.Dropout2d()
+-        self.fc1 = nn.Linear(256, params.linear_out)
+-        self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+-        self.hybrid = Hybrid(
+-            params.n_qubits,
+-            backend = "aer_simulator",
+-            shots = params.shots,
+-            shift=params.shift,
+-        )
+-        self.fc3 = nn.Linear(2**params.n_qubits, 10)
+-        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+-
+-    def forward(self, x):
+-        x = x.unsqueeze(1)
+-        x = F.relu(self.conv1(x))
+-        x = F.max_pool2d(x, 2)
+-        x = F.relu(self.conv2(x))
+-        x = F.max_pool2d(x, 2)
+-        
+-        x = x.view(x.shape[0],-1)
+-        x = F.relu(self.fc1(x))
+-        x = self.fc2(x)
+-        x = torch.tanh(x) * torch.ones_like(x) * torch.tensor(np.pi / 2)
+-        x = self.hybrid(x).to(self.device)
+-        x = F.softmax(self.fc3(x), dim=1)
+-        return x
+diff --git a/src/runners/__pycache__/base.cpython-38.pyc b/src/runners/__pycache__/base.cpython-38.pyc
+index 6fae27c..b7d08e9 100644
+Binary files a/src/runners/__pycache__/base.cpython-38.pyc and b/src/runners/__pycache__/base.cpython-38.pyc differ
+diff --git a/src/runners/__pycache__/runner.cpython-38.pyc b/src/runners/__pycache__/runner.cpython-38.pyc
+index 9813d76..3792f76 100644
+Binary files a/src/runners/__pycache__/runner.cpython-38.pyc and b/src/runners/__pycache__/runner.cpython-38.pyc differ
+diff --git a/src/runners/runner.py b/src/runners/runner.py
+index 86b61a5..5fb03f3 100644
+--- a/src/runners/runner.py
++++ b/src/runners/runner.py
+@@ -6,8 +6,8 @@ from typing import Optional
+ from copy import deepcopy
+ 
+ from src.runners import Base_Runner
+-from src.data import ROIDataset, SITES_DICT, MNISTDataset
+-from src.datamodules import DataModule, MNISTDataModule
++from src.data import ROIDataset, SITES_DICT
++from src.datamodules import DataModule
+ from src.tasks import ClassificationTask
+ from src.utils import plot_paper
+ from src.callbacks import wandb_callback as wbc
+@@ -177,106 +177,4 @@ class S_Runner(Base_Runner):
+ 
+         return
+ 
+-class MNIST_Runner(Base_Runner):
+-    def get_callbacks(self):
+-        """
+-        Write only callbacks that logger is not necessary
+-        """
+-        checkpoint_callback = ModelCheckpoint(
+-            dirpath=os.path.join(
+-                self.log.checkpoint_path,
+-                self.log.project_name,
+-                f"version{self.version:03d}",
+-            ),
+-            filename=os.path.join(f"model"),
+-            monitor=f"Accuracy/val",
+-            mode="max",
+-            verbose=False,
+-            save_top_k=1,
+-        )
+-
+-        callbacks = dict(
+-            filter(lambda item: item[0].endswith("callback"), vars().items())
+-        ).values()
+-        callbacks = list(callbacks)
+-        return callbacks if len(callbacks) > 0 else None
+-
+-    def run(self, profiler: Optional[str] = None):
+-        os.makedirs(
+-            os.path.join(self.log.checkpoint_path, self.log.project_name), exist_ok=True
+-        )
+-        self.version = len(
+-            os.listdir(os.path.join(self.log.checkpoint_path, self.log.project_name))
+-        )
+-
+-
+-        final_results = list()
+-
+-
+-
+-        dm = self.get_datamodule(dataset=MNISTDataset, datamodule=MNISTDataModule)
+-        model = self.get_network(Task=ClassificationTask)
+-        model.apply(initialize_weights)
+-
+-        trainer = Trainer(
+-            logger=[
+-                TensorBoardLogger(
+-                    save_dir=self.log.log_path,
+-                    name=os.path.join(
+-                        self.log.project_name,
+-                        f"version{self.version:03d}"
+-                    ),
+-                    default_hp_metric=False,
+-                    version=None,
+-                    # log_graph=True, # inavailable due to bug
+-                ),
+-                WandbLogger(
+-                    project=self.log.project_name,
+-                    save_dir=self.log.log_path,
+-                ),
+-            ],
+-            # ! use all gpu
+-            # gpus=-1,
+-            # auto_select_gpus=True,
+-            # ! use 2 gpu
+-            # devices=2,
+-            # accelerator="auto",
+-            # strategy="ddp",
+-            # ! use gpu 0
+-            devices=[0],
+-            accelerator="gpu",
+-            #devices=[self.log.device.gpu],
+-            #accelerator="cpu",
+-            check_val_every_n_epoch=self.log.val_log_freq_epoch,
+-            log_every_n_steps=1,
+-            num_sanity_val_steps=0,
+-            max_epochs=self.log.epoch,
+-            profiler=profiler,
+-            callbacks=[
+-                *self.get_callbacks(),
+-                wbc.WatchModel(),
+-                wbc.LogConfusionMatrix(),
+-                wbc.LogF1PrecRecHeatmap(),
+-                # tbc.WatchModel(),
+-                # tbc.LogConfusionMatrix(),
+-                # tbc.LogF1PrecRecHeatmap(),
+-            ],
+-            precision=self.log.precision,
+-            # gradient_clip_val=0.5,
+-        )
+-        trainer.test_site_prefix = model.prefix
+-        trainer.fit(model, datamodule=dm)
+-        trainer.test(model, datamodule=dm, ckpt_path="best")
+-        final_results.append(
+-            trainer.callback_metrics[f"Accuracy/test"]
+-        )
+-
+-    try:
+-        import wandb
+-
+-        wb_logger = wbc.get_wandb_logger(trainer=Trainer)
+-        
+-    except Exception as e:
+-        print(e)
+-
+     
diff --git a/Logs/wandb/run-20220704_224732-2j80fpj7/files/output.log b/Logs/wandb/run-20220704_224732-2j80fpj7/files/output.log
new file mode 100644
index 0000000..44bfba8
--- /dev/null
+++ b/Logs/wandb/run-20220704_224732-2j80fpj7/files/output.log
@@ -0,0 +1,63 @@
+GPU available: True, used: True
+TPU available: False, using: 0 TPU cores
+IPU available: False, using: 0 IPUs
+HPU available: False, using: 0 HPUs
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1172, in _run
+    self._call_setup_hook()  # allow user to setup lightning_module in accelerator environment
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1490, in _call_setup_hook
+    self.datamodule.setup(stage=fn)
+  File "/home/sol/git/QML2/src/datamodules/datamodule.py", line 22, in setup
+    self.train_dataset = self.dataset(is_train = True, site = self.data.train_site)
+TypeError: __init__() got an unexpected keyword argument 'is_train'
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1172, in _run
+    self._call_setup_hook()  # allow user to setup lightning_module in accelerator environment
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1490, in _call_setup_hook
+    self.datamodule.setup(stage=fn)
+  File "/home/sol/git/QML2/src/datamodules/datamodule.py", line 22, in setup
+    self.train_dataset = self.dataset(is_train = True, site = self.data.train_site)
+TypeError: __init__() got an unexpected keyword argument 'is_train'
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1172, in _run
+    self._call_setup_hook()  # allow user to setup lightning_module in accelerator environment
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1490, in _call_setup_hook
+    self.datamodule.setup(stage=fn)
+  File "/home/sol/git/QML2/src/datamodules/datamodule.py", line 22, in setup
+    self.train_dataset = self.dataset(is_train = True, site = self.data.train_site)
diff --git a/Logs/wandb/run-20220704_224732-2j80fpj7/files/requirements.txt b/Logs/wandb/run-20220704_224732-2j80fpj7/files/requirements.txt
new file mode 100644
index 0000000..f9f8632
--- /dev/null
+++ b/Logs/wandb/run-20220704_224732-2j80fpj7/files/requirements.txt
@@ -0,0 +1,166 @@
+absl-py==1.0.0
+aiohttp==3.8.1
+aiosignal==1.2.0
+antlr4-python3-runtime==4.8
+async-timeout==4.0.2
+attrs==19.3.0
+automat==0.8.0
+autopep8==1.6.0
+blinker==1.4
+cachetools==5.0.0
+certifi==2019.11.28
+chardet==3.0.4
+charset-normalizer==2.0.12
+click==7.0
+cloud-init==22.1
+colorama==0.4.3
+command-not-found==0.3
+configobj==5.0.6
+constantly==15.1.0
+cryptography==2.8
+cssselect==1.1.0
+cycler==0.11.0
+dbus-python==1.2.16
+dill==0.3.4
+distro-info==0.23ubuntu1
+distro==1.4.0
+docker-pycreds==0.4.0
+entrypoints==0.3
+filelock==3.6.0
+fonttools==4.33.3
+frozenlist==1.3.0
+fsspec==2022.3.0
+gistory==0.44
+gitdb==4.0.9
+gitpython==3.1.27
+google-auth-oauthlib==0.4.6
+google-auth==2.6.6
+grpcio==1.44.0
+httplib2==0.14.0
+hyperlink==19.0.0
+idna==2.8
+importlib-metadata==4.11.3
+incremental==16.10.1
+itemadapter==0.5.0
+itemloaders==1.0.4
+jinja2==2.10.1
+jmespath==1.0.0
+joblib==1.1.0
+jsonpatch==1.22
+jsonpointer==2.0
+jsonschema==3.2.0
+keyring==18.0.1
+kiwisolver==1.4.2
+language-selector==0.1
+launchpadlib==1.10.13
+lazr.restfulclient==0.14.2
+lazr.uri==1.0.3
+lxml==4.8.0
+markdown==3.3.6
+markupsafe==1.1.0
+matplotlib==3.5.1
+more-itertools==4.2.0
+mpmath==1.2.1
+multidict==6.0.2
+netifaces==0.10.4
+nibabel==3.2.2
+nilearn==0.9.1
+ntlm-auth==1.5.0
+numpy==1.22.3
+oauthlib==3.1.0
+omegaconf==2.1.2
+packaging==21.3
+pandas==1.4.2
+parsel==1.6.0
+pathtools==0.1.2
+pbr==5.8.1
+pexpect==4.6.0
+pillow==9.1.0
+pip==20.0.2
+ply==3.11
+promise==2.3
+protego==0.2.1
+protobuf==3.20.1
+psutil==5.9.0
+pyasn1-modules==0.2.1
+pyasn1==0.4.2
+pycodestyle==2.8.0
+pydeprecate==0.3.2
+pydispatcher==2.0.5
+pygame==2.1.2
+pygobject==3.36.0
+pyhamcrest==1.9.0
+pyjwt==1.7.1
+pymacaroons==0.13.0
+pynacl==1.3.0
+pyopenssl==19.0.0
+pyparsing==3.0.8
+pyrsistent==0.15.5
+pyserial==3.4
+python-apt==2.0.0+ubuntu0.20.4.7
+python-constraint==1.4.0
+python-dateutil==2.8.2
+python-debian==0.1.36ubuntu1
+pytorch-lightning==1.6.2
+pytz==2022.1
+pyyaml==6.0
+qiskit-aer==0.10.4
+qiskit-ibmq-provider==0.19.1
+qiskit-ignis==0.7.0
+qiskit-terra==0.20.1
+qiskit==0.36.1
+queuelib==1.6.2
+requests-file==1.5.1
+requests-ntlm==1.1.0
+requests-oauthlib==1.3.1
+requests-unixsocket==0.2.0
+requests==2.22.0
+retworkx==0.11.0
+rsa==4.8
+scikit-learn==1.0.2
+scipy==1.8.0
+seaborn==0.11.2
+secretstorage==2.3.1
+sentry-sdk==1.5.10
+service-identity==18.1.0
+setproctitle==1.2.3
+setuptools==45.2.0
+shortuuid==1.0.8
+simplejson==3.16.0
+six==1.14.0
+sklearn==0.0
+smmap==5.0.0
+sos==4.3
+ssh-import-id==5.10
+stevedore==3.5.0
+symengine==0.9.2
+sympy==1.10.1
+systemd-python==234
+tensorboard-data-server==0.6.1
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.8.0
+threadpoolctl==3.1.0
+tldextract==3.2.1
+toml==0.10.2
+torch-tb-profiler==0.4.0
+torch==1.12.0
+torchmetrics==0.8.1
+torchvision==0.13.0
+tqdm==4.64.0
+tweedledum==1.1.1
+twisted==18.9.0
+typing-extensions==4.2.0
+ubuntu-advantage-tools==27.7
+ufw==0.36
+unattended-upgrades==0.1
+urllib3==1.25.8
+w3lib==1.22.0
+wadllib==1.3.3
+wandb==0.12.15
+websocket-client==1.3.2
+websockets==10.3
+werkzeug==2.1.2
+wheel==0.34.2
+yarl==1.7.2
+zipp==1.0.0
+zope.interface==4.7.1
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-metadata.json b/Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-metadata.json
new file mode 100644
index 0000000..161ed4c
--- /dev/null
+++ b/Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-metadata.json
@@ -0,0 +1,24 @@
+{
+    "os": "Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2022-07-04T13:47:36.036457",
+    "startedAt": "2022-07-04T13:47:32.853086",
+    "docker": null,
+    "gpu": "NVIDIA GeForce GTX 1650",
+    "gpu_count": 1,
+    "cpu_count": 8,
+    "cuda": null,
+    "args": [],
+    "state": "running",
+    "program": "/home/sol/git/QML2/run.py",
+    "codePath": "run.py",
+    "git": {
+        "remote": "https://github.com/PVNkT/QML2.git",
+        "commit": "1f3b04ad3f84cb17a3713da979f0adf1e2028c86"
+    },
+    "email": "3031902@gmail.com",
+    "root": "/home/sol/git/QML2",
+    "host": "DESKTOP-HFMS9NT",
+    "username": "sol",
+    "executable": "/bin/python3"
+}
diff --git a/Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-summary.json b/Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-summary.json
new file mode 100644
index 0000000..971d479
--- /dev/null
+++ b/Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 1}}
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_224732-2j80fpj7/logs/debug-internal.log b/Logs/wandb/run-20220704_224732-2j80fpj7/logs/debug-internal.log
new file mode 100644
index 0000000..e99a8ec
--- /dev/null
+++ b/Logs/wandb/run-20220704_224732-2j80fpj7/logs/debug-internal.log
@@ -0,0 +1,238 @@
+2022-07-04 22:47:32,864 INFO    StreamThr :2422 [internal.py:wandb_internal():90] W&B internal server running at pid: 2422, started at: 2022-07-04 22:47:32.864180
+2022-07-04 22:47:32,869 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: status
+2022-07-04 22:47:32,869 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: status
+2022-07-04 22:47:32,871 INFO    WriterThread:2422 [datastore.py:open_for_write():75] open: Logs/wandb/run-20220704_224732-2j80fpj7/run-2j80fpj7.wandb
+2022-07-04 22:47:32,872 DEBUG   SenderThread:2422 [sender.py:send():232] send: header
+2022-07-04 22:47:32,879 DEBUG   SenderThread:2422 [sender.py:send():232] send: run
+2022-07-04 22:47:32,882 INFO    SenderThread:2422 [sender.py:_maybe_setup_resume():489] checking resume status for None/QML-MNIST/2j80fpj7
+2022-07-04 22:47:33,554 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: check_version
+2022-07-04 22:47:33,556 INFO    SenderThread:2422 [dir_watcher.py:__init__():166] watching files in: Logs/wandb/run-20220704_224732-2j80fpj7/files
+2022-07-04 22:47:33,556 INFO    SenderThread:2422 [sender.py:_start_run_threads():811] run started: 2j80fpj7 with start time 1656942452
+2022-07-04 22:47:33,556 DEBUG   SenderThread:2422 [sender.py:send():232] send: summary
+2022-07-04 22:47:33,556 INFO    SenderThread:2422 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:47:33,556 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: check_version
+2022-07-04 22:47:34,556 INFO    Thread-12 :2422 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-summary.json
+2022-07-04 22:47:34,756 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: run_start
+2022-07-04 22:47:36,036 DEBUG   HandlerThread:2422 [meta.py:__init__():35] meta init
+2022-07-04 22:47:36,036 DEBUG   HandlerThread:2422 [meta.py:__init__():49] meta init done
+2022-07-04 22:47:36,036 DEBUG   HandlerThread:2422 [meta.py:probe():209] probe
+2022-07-04 22:47:36,046 DEBUG   HandlerThread:2422 [meta.py:_setup_git():199] setup git
+2022-07-04 22:47:36,055 DEBUG   HandlerThread:2422 [meta.py:_setup_git():206] setup git done
+2022-07-04 22:47:36,055 DEBUG   HandlerThread:2422 [meta.py:_save_code():87] save code
+2022-07-04 22:47:36,060 DEBUG   HandlerThread:2422 [meta.py:_save_code():108] save code done
+2022-07-04 22:47:36,061 DEBUG   HandlerThread:2422 [meta.py:_save_patches():125] save patches
+2022-07-04 22:47:36,096 DEBUG   HandlerThread:2422 [meta.py:_save_patches():167] save patches done
+2022-07-04 22:47:36,096 DEBUG   HandlerThread:2422 [meta.py:_save_pip():53] save pip
+2022-07-04 22:47:36,097 DEBUG   HandlerThread:2422 [meta.py:_save_pip():67] save pip done
+2022-07-04 22:47:36,097 DEBUG   HandlerThread:2422 [meta.py:probe():247] probe done
+2022-07-04 22:47:36,149 DEBUG   SenderThread:2422 [sender.py:send():232] send: files
+2022-07-04 22:47:36,149 INFO    SenderThread:2422 [sender.py:_save_file():946] saving file wandb-metadata.json with policy now
+2022-07-04 22:47:36,149 INFO    SenderThread:2422 [sender.py:_save_file():946] saving file code/run.py with policy now
+2022-07-04 22:47:36,151 INFO    SenderThread:2422 [sender.py:_save_file():946] saving file diff.patch with policy now
+2022-07-04 22:47:36,164 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: stop_status
+2022-07-04 22:47:36,165 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: stop_status
+2022-07-04 22:47:36,214 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:36,515 DEBUG   SenderThread:2422 [sender.py:send():232] send: telemetry
+2022-07-04 22:47:36,515 DEBUG   SenderThread:2422 [sender.py:send():232] send: metric
+2022-07-04 22:47:36,516 DEBUG   SenderThread:2422 [sender.py:send():232] send: telemetry
+2022-07-04 22:47:36,516 DEBUG   SenderThread:2422 [sender.py:send():232] send: metric
+2022-07-04 22:47:36,516 WARNING SenderThread:2422 [sender.py:send_metric():904] Seen metric with glob (shouldn't happen)
+2022-07-04 22:47:36,516 DEBUG   SenderThread:2422 [sender.py:send():232] send: exit
+2022-07-04 22:47:36,516 INFO    SenderThread:2422 [sender.py:send_exit():368] handling exit code: 1
+2022-07-04 22:47:36,516 INFO    SenderThread:2422 [sender.py:send_exit():370] handling runtime: 1
+2022-07-04 22:47:36,518 INFO    SenderThread:2422 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:47:36,518 INFO    SenderThread:2422 [sender.py:send_exit():376] send defer
+2022-07-04 22:47:36,518 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:36,519 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:36,520 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 0
+2022-07-04 22:47:36,520 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:36,520 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 0
+2022-07-04 22:47:36,520 INFO    SenderThread:2422 [sender.py:transition_state():389] send defer: 1
+2022-07-04 22:47:36,520 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:36,520 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 1
+2022-07-04 22:47:36,556 INFO    Thread-12 :2422 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-summary.json
+2022-07-04 22:47:36,556 INFO    Thread-12 :2422 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-metadata.json
+2022-07-04 22:47:36,557 INFO    Thread-12 :2422 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224732-2j80fpj7/files/diff.patch
+2022-07-04 22:47:36,558 INFO    Thread-12 :2422 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224732-2j80fpj7/files/code/run.py
+2022-07-04 22:47:36,558 INFO    Thread-12 :2422 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224732-2j80fpj7/files/requirements.txt
+2022-07-04 22:47:36,558 INFO    Thread-12 :2422 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224732-2j80fpj7/files/output.log
+2022-07-04 22:47:36,558 INFO    Thread-12 :2422 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224732-2j80fpj7/files/code
+2022-07-04 22:47:37,004 INFO    Thread-16 :2422 [upload_job.py:push():137] Uploaded file /tmp/tmp4hgtftmrwandb/1vj6kabb-wandb-metadata.json
+2022-07-04 22:47:37,359 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:37,359 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:37,360 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 1
+2022-07-04 22:47:37,360 INFO    SenderThread:2422 [sender.py:transition_state():389] send defer: 2
+2022-07-04 22:47:37,360 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:37,361 DEBUG   SenderThread:2422 [sender.py:send():232] send: stats
+2022-07-04 22:47:37,362 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:37,362 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 2
+2022-07-04 22:47:37,363 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:37,363 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 2
+2022-07-04 22:47:37,363 INFO    SenderThread:2422 [sender.py:transition_state():389] send defer: 3
+2022-07-04 22:47:37,363 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:37,363 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 3
+2022-07-04 22:47:37,364 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:37,364 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 3
+2022-07-04 22:47:37,364 INFO    SenderThread:2422 [sender.py:transition_state():389] send defer: 4
+2022-07-04 22:47:37,365 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:37,365 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 4
+2022-07-04 22:47:37,366 DEBUG   SenderThread:2422 [sender.py:send():232] send: summary
+2022-07-04 22:47:37,366 INFO    SenderThread:2422 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:47:37,366 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:37,366 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 4
+2022-07-04 22:47:37,366 INFO    SenderThread:2422 [sender.py:transition_state():389] send defer: 5
+2022-07-04 22:47:37,367 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:37,367 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 5
+2022-07-04 22:47:37,367 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:37,367 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 5
+2022-07-04 22:47:37,463 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:37,557 INFO    Thread-12 :2422 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-summary.json
+2022-07-04 22:47:37,768 INFO    SenderThread:2422 [sender.py:transition_state():389] send defer: 6
+2022-07-04 22:47:37,769 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:37,770 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:37,770 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 6
+2022-07-04 22:47:37,770 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:37,770 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 6
+2022-07-04 22:47:37,770 INFO    SenderThread:2422 [dir_watcher.py:finish():279] shutting down directory watcher
+2022-07-04 22:47:37,779 INFO    Thread-18 :2422 [upload_job.py:push():137] Uploaded file /tmp/tmp4hgtftmrwandb/fmhfdcrp-diff.patch
+2022-07-04 22:47:37,871 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:38,558 INFO    SenderThread:2422 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224732-2j80fpj7/files/output.log
+2022-07-04 22:47:38,558 INFO    SenderThread:2422 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224732-2j80fpj7/files/config.yaml
+2022-07-04 22:47:38,559 INFO    SenderThread:2422 [dir_watcher.py:finish():309] scan: Logs/wandb/run-20220704_224732-2j80fpj7/files
+2022-07-04 22:47:38,559 INFO    SenderThread:2422 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224732-2j80fpj7/files/output.log output.log
+2022-07-04 22:47:38,559 INFO    SenderThread:2422 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224732-2j80fpj7/files/config.yaml config.yaml
+2022-07-04 22:47:38,559 INFO    SenderThread:2422 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224732-2j80fpj7/files/diff.patch diff.patch
+2022-07-04 22:47:38,559 INFO    SenderThread:2422 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-metadata.json wandb-metadata.json
+2022-07-04 22:47:38,560 INFO    SenderThread:2422 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224732-2j80fpj7/files/requirements.txt requirements.txt
+2022-07-04 22:47:38,563 INFO    SenderThread:2422 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-summary.json wandb-summary.json
+2022-07-04 22:47:38,567 INFO    SenderThread:2422 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224732-2j80fpj7/files/code/run.py code/run.py
+2022-07-04 22:47:38,570 INFO    SenderThread:2422 [sender.py:transition_state():389] send defer: 7
+2022-07-04 22:47:38,570 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:38,571 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:38,571 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 7
+2022-07-04 22:47:38,571 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:38,571 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 7
+2022-07-04 22:47:38,571 INFO    SenderThread:2422 [file_pusher.py:finish():145] shutting down file pusher
+2022-07-04 22:47:38,674 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:38,675 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:38,770 INFO    Thread-17 :2422 [upload_job.py:push():137] Uploaded file /tmp/tmp4hgtftmrwandb/1cb0jp20-code/run.py
+2022-07-04 22:47:38,776 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:38,776 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:38,877 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:38,878 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:38,979 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:38,979 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:39,080 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:39,081 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:39,182 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:39,182 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:39,283 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:39,283 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:39,384 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:39,385 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:39,486 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:39,486 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:39,587 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:39,587 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:39,689 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:39,689 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:39,791 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:39,791 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:39,818 INFO    Thread-22 :2422 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224732-2j80fpj7/files/wandb-summary.json
+2022-07-04 22:47:39,892 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:39,892 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:39,993 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:39,994 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:40,095 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:40,095 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:40,196 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:40,196 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:40,298 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:40,298 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:40,399 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:40,399 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:40,501 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:40,501 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:40,532 INFO    Thread-21 :2422 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224732-2j80fpj7/files/requirements.txt
+2022-07-04 22:47:40,602 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:40,602 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:40,703 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:40,703 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:40,805 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:40,805 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:40,906 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:40,906 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:41,007 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:41,007 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:41,109 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:41,109 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:41,210 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:41,210 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:41,311 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:41,312 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:41,413 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:41,413 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:41,515 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:41,515 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:41,616 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:41,616 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:41,717 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:41,718 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:41,819 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:41,819 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:41,920 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:41,921 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:42,022 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:42,022 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:42,123 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:42,124 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:42,225 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:42,225 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:42,326 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:42,327 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:42,393 INFO    Thread-20 :2422 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224732-2j80fpj7/files/config.yaml
+2022-07-04 22:47:42,428 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:42,428 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:42,530 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:42,530 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:42,631 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:42,631 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:42,715 INFO    Thread-19 :2422 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224732-2j80fpj7/files/output.log
+2022-07-04 22:47:42,732 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:42,733 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:42,834 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:42,834 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:42,916 INFO    Thread-11 :2422 [sender.py:transition_state():389] send defer: 8
+2022-07-04 22:47:42,916 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:42,916 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 8
+2022-07-04 22:47:42,916 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:42,916 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 8
+2022-07-04 22:47:42,935 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:43,218 INFO    SenderThread:2422 [sender.py:transition_state():389] send defer: 9
+2022-07-04 22:47:43,218 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:43,219 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:43,219 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 9
+2022-07-04 22:47:43,219 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:43,219 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 9
+2022-07-04 22:47:43,219 INFO    SenderThread:2422 [sender.py:transition_state():389] send defer: 10
+2022-07-04 22:47:43,220 DEBUG   SenderThread:2422 [sender.py:send():232] send: final
+2022-07-04 22:47:43,220 DEBUG   SenderThread:2422 [sender.py:send():232] send: footer
+2022-07-04 22:47:43,220 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:47:43,220 INFO    HandlerThread:2422 [handler.py:handle_request_defer():164] handle defer: 10
+2022-07-04 22:47:43,220 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:47:43,220 INFO    SenderThread:2422 [sender.py:send_request_defer():385] handle sender defer: 10
+2022-07-04 22:47:43,319 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:47:43,320 DEBUG   SenderThread:2422 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:47:43,320 INFO    SenderThread:2422 [file_pusher.py:join():150] waiting for file pusher
+2022-07-04 22:47:45,132 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: sampled_history
+2022-07-04 22:47:45,133 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: get_summary
+2022-07-04 22:47:45,133 INFO    MainThread:2422 [wandb_run.py:_footer_history_summary_info():3102] rendering history
+2022-07-04 22:47:45,133 INFO    MainThread:2422 [wandb_run.py:_footer_history_summary_info():3134] rendering summary
+2022-07-04 22:47:45,133 INFO    MainThread:2422 [wandb_run.py:_footer_sync_info():3057] logging synced files
+2022-07-04 22:47:45,134 DEBUG   HandlerThread:2422 [handler.py:handle_request():141] handle_request: shutdown
+2022-07-04 22:47:45,134 INFO    HandlerThread:2422 [handler.py:finish():806] shutting down handler
+2022-07-04 22:47:45,220 INFO    WriterThread:2422 [datastore.py:close():279] close: Logs/wandb/run-20220704_224732-2j80fpj7/run-2j80fpj7.wandb
+2022-07-04 22:47:46,031 INFO    SenderThread:2422 [sender.py:finish():1106] shutting down sender
+2022-07-04 22:47:46,031 INFO    SenderThread:2422 [file_pusher.py:finish():145] shutting down file pusher
+2022-07-04 22:47:46,031 INFO    SenderThread:2422 [file_pusher.py:join():150] waiting for file pusher
+2022-07-04 22:47:46,137 INFO    MainThread:2422 [internal.py:handle_exit():80] Internal process exited
diff --git a/Logs/wandb/run-20220704_224732-2j80fpj7/logs/debug.log b/Logs/wandb/run-20220704_224732-2j80fpj7/logs/debug.log
new file mode 100644
index 0000000..841aea1
--- /dev/null
+++ b/Logs/wandb/run-20220704_224732-2j80fpj7/logs/debug.log
@@ -0,0 +1,24 @@
+2022-07-04 22:47:32,854 INFO    MainThread:2387 [wandb_setup.py:_flush():75] Loading settings from /home/sol/.config/wandb/settings
+2022-07-04 22:47:32,854 INFO    MainThread:2387 [wandb_setup.py:_flush():75] Loading settings from /home/sol/git/QML2/wandb/settings
+2022-07-04 22:47:32,854 INFO    MainThread:2387 [wandb_setup.py:_flush():75] Loading settings from environment variables: {'_require_service': 'True'}
+2022-07-04 22:47:32,854 INFO    MainThread:2387 [wandb_setup.py:_flush():75] Inferring run settings from compute environment: {'program_relpath': 'run.py', 'program': '/home/sol/git/QML2/run.py'}
+2022-07-04 22:47:32,854 INFO    MainThread:2387 [wandb_init.py:_log_setup():437] Logging user logs to Logs/wandb/run-20220704_224732-2j80fpj7/logs/debug.log
+2022-07-04 22:47:32,854 INFO    MainThread:2387 [wandb_init.py:_log_setup():438] Logging internal logs to Logs/wandb/run-20220704_224732-2j80fpj7/logs/debug-internal.log
+2022-07-04 22:47:32,855 INFO    MainThread:2387 [wandb_init.py:init():471] calling init triggers
+2022-07-04 22:47:32,855 INFO    MainThread:2387 [wandb_init.py:init():474] wandb.init called with sweep_config: {}
+config: {}
+2022-07-04 22:47:32,855 INFO    MainThread:2387 [wandb_init.py:init():524] starting backend
+2022-07-04 22:47:32,856 INFO    MainThread:2387 [backend.py:_multiprocessing_setup():97] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2022-07-04 22:47:32,858 INFO    MainThread:2387 [wandb_init.py:init():533] backend started and connected
+2022-07-04 22:47:32,864 INFO    MainThread:2387 [wandb_init.py:init():597] updated telemetry
+2022-07-04 22:47:32,878 INFO    MainThread:2387 [wandb_init.py:init():628] communicating run to backend with 30 second timeout
+2022-07-04 22:47:33,549 INFO    MainThread:2387 [wandb_run.py:_on_init():1923] communicating current version
+2022-07-04 22:47:34,750 INFO    MainThread:2387 [wandb_run.py:_on_init():1927] got version response upgrade_message: "wandb version 0.12.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2022-07-04 22:47:34,750 INFO    MainThread:2387 [wandb_init.py:init():659] starting run threads in backend
+2022-07-04 22:47:36,159 INFO    MainThread:2387 [wandb_run.py:_console_start():1897] atexit reg
+2022-07-04 22:47:36,161 INFO    MainThread:2387 [wandb_run.py:_redirect():1770] redirect: SettingsConsole.WRAP
+2022-07-04 22:47:36,162 INFO    MainThread:2387 [wandb_run.py:_redirect():1807] Wrapping output streams.
+2022-07-04 22:47:36,163 INFO    MainThread:2387 [wandb_run.py:_redirect():1831] Redirects installed.
+2022-07-04 22:47:36,163 INFO    MainThread:2387 [wandb_init.py:init():684] run started, returning control to user process
+2022-07-04 22:47:46,137 WARNING MsgRouterThr:2387 [router.py:message_loop():76] message_loop has been closed
diff --git a/Logs/wandb/run-20220704_224732-2j80fpj7/run-2j80fpj7.wandb b/Logs/wandb/run-20220704_224732-2j80fpj7/run-2j80fpj7.wandb
new file mode 100644
index 0000000..403485e
Binary files /dev/null and b/Logs/wandb/run-20220704_224732-2j80fpj7/run-2j80fpj7.wandb differ
diff --git a/Logs/wandb/run-20220704_224806-2ltswfst/files/code/run.py b/Logs/wandb/run-20220704_224806-2ltswfst/files/code/run.py
new file mode 100644
index 0000000..69d3f58
--- /dev/null
+++ b/Logs/wandb/run-20220704_224806-2ltswfst/files/code/run.py
@@ -0,0 +1,25 @@
+from omegaconf import OmegaConf
+from pathlib import Path
+
+from src.runners import S_Runner
+
+CONFIG_DIR = Path("Configs")
+
+
+def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+    model_params = OmegaConf.load(CONFIG_DIR / "models.yaml")
+    cfg = OmegaConf.merge(cfg, model_params)
+    cfg.merge_with_cli()
+
+    runner = S_Runner(
+        log=cfg.log,
+        optimizer=cfg.optimizer,
+        loader=cfg.loader,
+        network=cfg.network,
+        data=cfg.data,
+    )
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/Logs/wandb/run-20220704_224806-2ltswfst/files/config.yaml b/Logs/wandb/run-20220704_224806-2ltswfst/files/config.yaml
new file mode 100644
index 0000000..183ca74
--- /dev/null
+++ b/Logs/wandb/run-20220704_224806-2ltswfst/files/config.yaml
@@ -0,0 +1,31 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.12.15
+    code_path: code/run.py
+    framework: lightning
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    m:
+    - 1: trainer/global_step
+      6:
+      - 3
+    python_version: 3.8.10
+    start_time: 1656942486
+    t:
+      1:
+      - 1
+      - 5
+      - 9
+      - 41
+      - 53
+      - 55
+      3:
+      - 7
+      - 23
+      4: 3.8.10
+      5: 0.12.15
+      8:
+      - 5
diff --git a/Logs/wandb/run-20220704_224806-2ltswfst/files/diff.patch b/Logs/wandb/run-20220704_224806-2ltswfst/files/diff.patch
new file mode 100644
index 0000000..36d1343
--- /dev/null
+++ b/Logs/wandb/run-20220704_224806-2ltswfst/files/diff.patch
@@ -0,0 +1,415 @@
+diff --git a/Configs/models.yaml b/Configs/models.yaml
+index 2c5c2ba..63b272d 100644
+--- a/Configs/models.yaml
++++ b/Configs/models.yaml
+@@ -8,9 +8,9 @@ LSTM:
+ Simple_QHN:
+   n_qubits: 2
+   shift: 0.6
+-  is_cnot: True
+   lstm_hidden: 128
+   linear_out: 64
++  backend: aer_simulator
+ 
+ 
+ quantum_circuit:
+@@ -18,5 +18,4 @@ quantum_circuit:
+   simulator: aer_simulator
+   shift: 0.5
+   shots: 100
+-  is_cnot: True
+   dense_type: 2
+\ No newline at end of file
+diff --git a/Logs/QML-MNIST/version000/version_0/hparams.yaml b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+index 3851257..000d975 100644
+--- a/Logs/QML-MNIST/version000/version_0/hparams.yaml
++++ b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+@@ -15,6 +15,5 @@ net:
+     simulator: aer_simulator
+     shift: 0.5
+     shots: 100
+-    is_cnot: true
+     dense_type: 2
+ inputs: null
+diff --git a/run.py b/run.py
+index c8e1ccf..69d3f58 100644
+--- a/run.py
++++ b/run.py
+@@ -1,7 +1,7 @@
+ from omegaconf import OmegaConf
+ from pathlib import Path
+ 
+-from src.runners import S_Runner, MNIST_Runner
++from src.runners import S_Runner
+ 
+ CONFIG_DIR = Path("Configs")
+ 
+@@ -11,7 +11,7 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+     cfg = OmegaConf.merge(cfg, model_params)
+     cfg.merge_with_cli()
+ 
+-    runner = MNIST_Runner(
++    runner = S_Runner(
+         log=cfg.log,
+         optimizer=cfg.optimizer,
+         loader=cfg.loader,
+@@ -22,4 +22,4 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+ 
+ 
+ if __name__ == "__main__":
+-    main(cfg=OmegaConf.load(CONFIG_DIR / "config MNIST.yaml"))
++    main()
+diff --git a/src/data/__pycache__/data_etl.cpython-38.pyc b/src/data/__pycache__/data_etl.cpython-38.pyc
+index 288d14c..934cbd5 100644
+Binary files a/src/data/__pycache__/data_etl.cpython-38.pyc and b/src/data/__pycache__/data_etl.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/loader.cpython-38.pyc b/src/data/__pycache__/loader.cpython-38.pyc
+index eddf358..67504ef 100644
+Binary files a/src/data/__pycache__/loader.cpython-38.pyc and b/src/data/__pycache__/loader.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/sampling.cpython-38.pyc b/src/data/__pycache__/sampling.cpython-38.pyc
+index a2cbaf3..a748fd2 100644
+Binary files a/src/data/__pycache__/sampling.cpython-38.pyc and b/src/data/__pycache__/sampling.cpython-38.pyc differ
+diff --git a/src/data/data_etl.py b/src/data/data_etl.py
+index 08700be..dc691b0 100755
+--- a/src/data/data_etl.py
++++ b/src/data/data_etl.py
+@@ -137,24 +137,6 @@ def chunks(lst, n):
+     return [lst[i : i + div] for i in range(0, len(lst), div)]
+ 
+ 
+-@dataclass
+-class Load_MNIST:
+-    def __init__(self, is_train:Boolean):
+-        if is_train:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=True, download=True,
+-                         transform=transforms.Compose([transforms.ToTensor()]))
+-        else:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=False, download=True,
+-                        transform=transforms.Compose([transforms.ToTensor()]))
+-                        
+-    def get_samples(self, n_samples:int):
+-        idx = []
+-        for i in range(10):
+-            idx.append(np.where(self.dataset.targets == i)[0][:n_samples])#각 target에 해당되는 번호를 저장한다. 
+-        idx = np.array(idx).reshape(-1)
+-        self.dataset.data = self.dataset.data[idx]
+-        self.dataset.targets = self.dataset.targets[idx]
+-        return self.dataset
+ 
+ 
+ 
+diff --git a/src/data/loader.py b/src/data/loader.py
+index 288a3b3..158cbdc 100644
+--- a/src/data/loader.py
++++ b/src/data/loader.py
+@@ -5,25 +5,8 @@ from typing import Union, List
+ import torch
+ from torch.utils.data import Dataset
+ 
+-from src.data import Load, SITES_DICT, Load_MNIST
+-
+-class MNISTDataset(Dataset):
+-    def __init__(self, n_samples: int, is_train: Boolean) -> None:
+-        Load = Load_MNIST(is_train)
+-        if n_samples >0:
+-            self.data = Load.get_samples(n_samples=n_samples).data
+-            self.labels = Load.get_samples(n_samples=n_samples).targets  
+-        else:  
+-            self.data = Load.dataset.data
+-            self.labels = Load.dataset.targets
++from src.data import Load, SITES_DICT
+ 
+-    def __len__(self):
+-        return len(self.labels)
+-    
+-    def __getitem__(self, index: int):
+-        data = self.data[index]
+-        label = self.labels[index]
+-        return data, label 
+   
+ class ROIDataset(Dataset):
+     def __init__( self, site: Union[List, str]) -> None:
+diff --git a/src/datamodules/__pycache__/datamodule.cpython-38.pyc b/src/datamodules/__pycache__/datamodule.cpython-38.pyc
+index 6f06b1a..d4ba074 100644
+Binary files a/src/datamodules/__pycache__/datamodule.cpython-38.pyc and b/src/datamodules/__pycache__/datamodule.cpython-38.pyc differ
+diff --git a/src/datamodules/datamodule.py b/src/datamodules/datamodule.py
+index 4fcab3d..0bfab90 100644
+--- a/src/datamodules/datamodule.py
++++ b/src/datamodules/datamodule.py
+@@ -7,64 +7,23 @@ from torch.utils.data import DataLoader, Subset
+ from pytorch_lightning import LightningDataModule
+ from src.data import collate_fn, SamplerFactory
+ 
++
+ @dataclass
+-class MNISTDataModule(LightningDataModule):
+-    
+-    def __init__(self,data: Dict,loader: Dict,dataset: Dict):
++class DataModule(LightningDataModule):
++    def __init__(self, data: Dict, loader: Dict, dataset: Dict):
+         super().__init__()
+-        self.prepare_data_per_node = True
+         self.data = data
+         self.loader = loader
+         self.dataset = dataset
+-    def setup(self, stage: Optional[str] = None):
+-        if stage in ("fit", None):
+-            self.train_dataset = self.dataset(n_samples = self.loader.samples.train_sample, is_train=True)
+-            self.val_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-
+-        if stage in ("test", None):
+-            self.test_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-    
+-    def train_dataloader(self):
+-        conf = deepcopy(self.loader.train)
+-        batch_size = conf.pop("batch_size")
+-        conf.shuffle = False
+-        # return DataLoader(self.train_dataset, **conf, collate_fn=collate_fn)
+-
+-        return DataLoader(
+-            self.train_dataset,
+-            **conf,
+-            collate_fn=collate_fn,
+-            batch_sampler=SamplerFactory().get(
+-                class_idxs=[
+-                    np.where(self.train_dataset.labels == i)[0].tolist()
+-                    for i in range(10)
+-                ],
+-                batch_size=batch_size,
+-                n_batches=len(self.train_dataset)//batch_size + 5,
+-                alpha=1.0,
+-                kind="random",
+-            ),
+-        )
+-
+-    def val_dataloader(self):
+-        return DataLoader(self.val_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-    def test_dataloader(self):
+-        return DataLoader(self.test_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-@dataclass
+-class DataModule(LightningDataModule):
+-    data: Dict
+-    loader: Dict
+-    dataset: Dict
++        self.prepare_data_per_node = True
+ 
+     def setup(self, stage: Optional[str] = None):
+         if stage in ("fit", None):
+-            self.train_dataset = self.dataset(is_train = True, site = self.data.train_site)
+-            self.val_dataset = self.dataset(is_train = False, site = self.data.train_site)
++            self.train_dataset = self.dataset(site = self.data.train_site)
++            self.val_dataset = self.dataset(site = self.data.train_site)
+ 
+         if stage in ("test", None):
+-            self.test_dataset = self.dataset(is_train = False, site = self.data.train_site)
++            self.test_dataset = self.dataset(site = self.data.train_site)
+ 
+     def train_dataloader(self):
+         conf = deepcopy(self.loader.train)
+diff --git a/src/layers/__pycache__/__init__.cpython-38.pyc b/src/layers/__pycache__/__init__.cpython-38.pyc
+index 31453e2..a73c856 100644
+Binary files a/src/layers/__pycache__/__init__.cpython-38.pyc and b/src/layers/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/layers/__pycache__/quantum_layers.cpython-38.pyc b/src/layers/__pycache__/quantum_layers.cpython-38.pyc
+index 4f219f9..3e30b2e 100644
+Binary files a/src/layers/__pycache__/quantum_layers.cpython-38.pyc and b/src/layers/__pycache__/quantum_layers.cpython-38.pyc differ
+diff --git a/src/layers/quantum_layers.py b/src/layers/quantum_layers.py
+index 976aa6a..4dfccfa 100644
+--- a/src/layers/quantum_layers.py
++++ b/src/layers/quantum_layers.py
+@@ -35,7 +35,7 @@ class QuantumCircuit:
+         for theta, qubit in zip(self.theta, self.all_qubits):
+             self._circuit.ry(theta, qubit)
+ 
+-        for i in [2,3,4,5]:
++        for i in range(2, self.n_qubit):
+             self._circuit.cx(0,i)
+ 
+         self._circuit.measure_all()
+diff --git a/src/models/__pycache__/__init__.cpython-38.pyc b/src/models/__pycache__/__init__.cpython-38.pyc
+index f5a0d8c..9643058 100644
+Binary files a/src/models/__pycache__/__init__.cpython-38.pyc and b/src/models/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/models/__pycache__/qhn.cpython-38.pyc b/src/models/__pycache__/qhn.cpython-38.pyc
+index 56e2a1f..8b4a754 100644
+Binary files a/src/models/__pycache__/qhn.cpython-38.pyc and b/src/models/__pycache__/qhn.cpython-38.pyc differ
+diff --git a/src/models/qhn.py b/src/models/qhn.py
+index c74fec4..2e08fba 100644
+--- a/src/models/qhn.py
++++ b/src/models/qhn.py
+@@ -27,10 +27,10 @@ class Simple_QHN(nn.Module):
+         self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+         self.hybrid = Hybrid(
+             params.n_qubits,
+-            qiskit.Aer.get_backend("aer_simulator"),
++            params.backend,
+             100,
+             shift=params.shift,
+-            is_cnot=params.is_cnot,
++            
+         )
+         self.fc3 = nn.Linear(params.n_qubits * 2, 2)
+         self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+@@ -55,35 +55,3 @@ class Simple_QHN(nn.Module):
+         return x
+ 
+ 
+-class MNIST_QHN(nn.Module):
+-    def __init__(self, params: Optional[Dict] = None, *args, **kwargs) -> None:
+-        super(MNIST_QHN, self).__init__()
+-        params = params.MNIST_QHN
+-        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
+-        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
+-        self.dropout = nn.Dropout2d()
+-        self.fc1 = nn.Linear(256, params.linear_out)
+-        self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+-        self.hybrid = Hybrid(
+-            params.n_qubits,
+-            backend = "aer_simulator",
+-            shots = params.shots,
+-            shift=params.shift,
+-        )
+-        self.fc3 = nn.Linear(2**params.n_qubits, 10)
+-        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+-
+-    def forward(self, x):
+-        x = x.unsqueeze(1)
+-        x = F.relu(self.conv1(x))
+-        x = F.max_pool2d(x, 2)
+-        x = F.relu(self.conv2(x))
+-        x = F.max_pool2d(x, 2)
+-        
+-        x = x.view(x.shape[0],-1)
+-        x = F.relu(self.fc1(x))
+-        x = self.fc2(x)
+-        x = torch.tanh(x) * torch.ones_like(x) * torch.tensor(np.pi / 2)
+-        x = self.hybrid(x).to(self.device)
+-        x = F.softmax(self.fc3(x), dim=1)
+-        return x
+diff --git a/src/runners/__pycache__/base.cpython-38.pyc b/src/runners/__pycache__/base.cpython-38.pyc
+index 6fae27c..b7d08e9 100644
+Binary files a/src/runners/__pycache__/base.cpython-38.pyc and b/src/runners/__pycache__/base.cpython-38.pyc differ
+diff --git a/src/runners/__pycache__/runner.cpython-38.pyc b/src/runners/__pycache__/runner.cpython-38.pyc
+index 9813d76..3792f76 100644
+Binary files a/src/runners/__pycache__/runner.cpython-38.pyc and b/src/runners/__pycache__/runner.cpython-38.pyc differ
+diff --git a/src/runners/runner.py b/src/runners/runner.py
+index 86b61a5..5fb03f3 100644
+--- a/src/runners/runner.py
++++ b/src/runners/runner.py
+@@ -6,8 +6,8 @@ from typing import Optional
+ from copy import deepcopy
+ 
+ from src.runners import Base_Runner
+-from src.data import ROIDataset, SITES_DICT, MNISTDataset
+-from src.datamodules import DataModule, MNISTDataModule
++from src.data import ROIDataset, SITES_DICT
++from src.datamodules import DataModule
+ from src.tasks import ClassificationTask
+ from src.utils import plot_paper
+ from src.callbacks import wandb_callback as wbc
+@@ -177,106 +177,4 @@ class S_Runner(Base_Runner):
+ 
+         return
+ 
+-class MNIST_Runner(Base_Runner):
+-    def get_callbacks(self):
+-        """
+-        Write only callbacks that logger is not necessary
+-        """
+-        checkpoint_callback = ModelCheckpoint(
+-            dirpath=os.path.join(
+-                self.log.checkpoint_path,
+-                self.log.project_name,
+-                f"version{self.version:03d}",
+-            ),
+-            filename=os.path.join(f"model"),
+-            monitor=f"Accuracy/val",
+-            mode="max",
+-            verbose=False,
+-            save_top_k=1,
+-        )
+-
+-        callbacks = dict(
+-            filter(lambda item: item[0].endswith("callback"), vars().items())
+-        ).values()
+-        callbacks = list(callbacks)
+-        return callbacks if len(callbacks) > 0 else None
+-
+-    def run(self, profiler: Optional[str] = None):
+-        os.makedirs(
+-            os.path.join(self.log.checkpoint_path, self.log.project_name), exist_ok=True
+-        )
+-        self.version = len(
+-            os.listdir(os.path.join(self.log.checkpoint_path, self.log.project_name))
+-        )
+-
+-
+-        final_results = list()
+-
+-
+-
+-        dm = self.get_datamodule(dataset=MNISTDataset, datamodule=MNISTDataModule)
+-        model = self.get_network(Task=ClassificationTask)
+-        model.apply(initialize_weights)
+-
+-        trainer = Trainer(
+-            logger=[
+-                TensorBoardLogger(
+-                    save_dir=self.log.log_path,
+-                    name=os.path.join(
+-                        self.log.project_name,
+-                        f"version{self.version:03d}"
+-                    ),
+-                    default_hp_metric=False,
+-                    version=None,
+-                    # log_graph=True, # inavailable due to bug
+-                ),
+-                WandbLogger(
+-                    project=self.log.project_name,
+-                    save_dir=self.log.log_path,
+-                ),
+-            ],
+-            # ! use all gpu
+-            # gpus=-1,
+-            # auto_select_gpus=True,
+-            # ! use 2 gpu
+-            # devices=2,
+-            # accelerator="auto",
+-            # strategy="ddp",
+-            # ! use gpu 0
+-            devices=[0],
+-            accelerator="gpu",
+-            #devices=[self.log.device.gpu],
+-            #accelerator="cpu",
+-            check_val_every_n_epoch=self.log.val_log_freq_epoch,
+-            log_every_n_steps=1,
+-            num_sanity_val_steps=0,
+-            max_epochs=self.log.epoch,
+-            profiler=profiler,
+-            callbacks=[
+-                *self.get_callbacks(),
+-                wbc.WatchModel(),
+-                wbc.LogConfusionMatrix(),
+-                wbc.LogF1PrecRecHeatmap(),
+-                # tbc.WatchModel(),
+-                # tbc.LogConfusionMatrix(),
+-                # tbc.LogF1PrecRecHeatmap(),
+-            ],
+-            precision=self.log.precision,
+-            # gradient_clip_val=0.5,
+-        )
+-        trainer.test_site_prefix = model.prefix
+-        trainer.fit(model, datamodule=dm)
+-        trainer.test(model, datamodule=dm, ckpt_path="best")
+-        final_results.append(
+-            trainer.callback_metrics[f"Accuracy/test"]
+-        )
+-
+-    try:
+-        import wandb
+-
+-        wb_logger = wbc.get_wandb_logger(trainer=Trainer)
+-        
+-    except Exception as e:
+-        print(e)
+-
+     
diff --git a/Logs/wandb/run-20220704_224806-2ltswfst/files/output.log b/Logs/wandb/run-20220704_224806-2ltswfst/files/output.log
new file mode 100644
index 0000000..7bc402b
--- /dev/null
+++ b/Logs/wandb/run-20220704_224806-2ltswfst/files/output.log
@@ -0,0 +1,32 @@
+GPU available: True, used: True
+TPU available: False, using: 0 TPU cores
+IPU available: False, using: 0 IPUs
+HPU available: False, using: 0 HPUs
+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
+  | Name        | Type             | Params
+-------------------------------------------------
+0 | model       | Simple_QHN       | 170 K
+1 | get_metrics | MetricCollection | 0
+-------------------------------------------------
+170 K     Trainable params
+0         Non-trainable params
+170 K     Total params
+0.680     Total estimated model params size (MB)
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1222, in _run
+    self._log_hyperparams()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1290, in _log_hyperparams
+    logger.log_hyperparams(hparams_initial)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/rank_zero.py", line 32, in wrapped_fn
diff --git a/Logs/wandb/run-20220704_224806-2ltswfst/files/requirements.txt b/Logs/wandb/run-20220704_224806-2ltswfst/files/requirements.txt
new file mode 100644
index 0000000..f9f8632
--- /dev/null
+++ b/Logs/wandb/run-20220704_224806-2ltswfst/files/requirements.txt
@@ -0,0 +1,166 @@
+absl-py==1.0.0
+aiohttp==3.8.1
+aiosignal==1.2.0
+antlr4-python3-runtime==4.8
+async-timeout==4.0.2
+attrs==19.3.0
+automat==0.8.0
+autopep8==1.6.0
+blinker==1.4
+cachetools==5.0.0
+certifi==2019.11.28
+chardet==3.0.4
+charset-normalizer==2.0.12
+click==7.0
+cloud-init==22.1
+colorama==0.4.3
+command-not-found==0.3
+configobj==5.0.6
+constantly==15.1.0
+cryptography==2.8
+cssselect==1.1.0
+cycler==0.11.0
+dbus-python==1.2.16
+dill==0.3.4
+distro-info==0.23ubuntu1
+distro==1.4.0
+docker-pycreds==0.4.0
+entrypoints==0.3
+filelock==3.6.0
+fonttools==4.33.3
+frozenlist==1.3.0
+fsspec==2022.3.0
+gistory==0.44
+gitdb==4.0.9
+gitpython==3.1.27
+google-auth-oauthlib==0.4.6
+google-auth==2.6.6
+grpcio==1.44.0
+httplib2==0.14.0
+hyperlink==19.0.0
+idna==2.8
+importlib-metadata==4.11.3
+incremental==16.10.1
+itemadapter==0.5.0
+itemloaders==1.0.4
+jinja2==2.10.1
+jmespath==1.0.0
+joblib==1.1.0
+jsonpatch==1.22
+jsonpointer==2.0
+jsonschema==3.2.0
+keyring==18.0.1
+kiwisolver==1.4.2
+language-selector==0.1
+launchpadlib==1.10.13
+lazr.restfulclient==0.14.2
+lazr.uri==1.0.3
+lxml==4.8.0
+markdown==3.3.6
+markupsafe==1.1.0
+matplotlib==3.5.1
+more-itertools==4.2.0
+mpmath==1.2.1
+multidict==6.0.2
+netifaces==0.10.4
+nibabel==3.2.2
+nilearn==0.9.1
+ntlm-auth==1.5.0
+numpy==1.22.3
+oauthlib==3.1.0
+omegaconf==2.1.2
+packaging==21.3
+pandas==1.4.2
+parsel==1.6.0
+pathtools==0.1.2
+pbr==5.8.1
+pexpect==4.6.0
+pillow==9.1.0
+pip==20.0.2
+ply==3.11
+promise==2.3
+protego==0.2.1
+protobuf==3.20.1
+psutil==5.9.0
+pyasn1-modules==0.2.1
+pyasn1==0.4.2
+pycodestyle==2.8.0
+pydeprecate==0.3.2
+pydispatcher==2.0.5
+pygame==2.1.2
+pygobject==3.36.0
+pyhamcrest==1.9.0
+pyjwt==1.7.1
+pymacaroons==0.13.0
+pynacl==1.3.0
+pyopenssl==19.0.0
+pyparsing==3.0.8
+pyrsistent==0.15.5
+pyserial==3.4
+python-apt==2.0.0+ubuntu0.20.4.7
+python-constraint==1.4.0
+python-dateutil==2.8.2
+python-debian==0.1.36ubuntu1
+pytorch-lightning==1.6.2
+pytz==2022.1
+pyyaml==6.0
+qiskit-aer==0.10.4
+qiskit-ibmq-provider==0.19.1
+qiskit-ignis==0.7.0
+qiskit-terra==0.20.1
+qiskit==0.36.1
+queuelib==1.6.2
+requests-file==1.5.1
+requests-ntlm==1.1.0
+requests-oauthlib==1.3.1
+requests-unixsocket==0.2.0
+requests==2.22.0
+retworkx==0.11.0
+rsa==4.8
+scikit-learn==1.0.2
+scipy==1.8.0
+seaborn==0.11.2
+secretstorage==2.3.1
+sentry-sdk==1.5.10
+service-identity==18.1.0
+setproctitle==1.2.3
+setuptools==45.2.0
+shortuuid==1.0.8
+simplejson==3.16.0
+six==1.14.0
+sklearn==0.0
+smmap==5.0.0
+sos==4.3
+ssh-import-id==5.10
+stevedore==3.5.0
+symengine==0.9.2
+sympy==1.10.1
+systemd-python==234
+tensorboard-data-server==0.6.1
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.8.0
+threadpoolctl==3.1.0
+tldextract==3.2.1
+toml==0.10.2
+torch-tb-profiler==0.4.0
+torch==1.12.0
+torchmetrics==0.8.1
+torchvision==0.13.0
+tqdm==4.64.0
+tweedledum==1.1.1
+twisted==18.9.0
+typing-extensions==4.2.0
+ubuntu-advantage-tools==27.7
+ufw==0.36
+unattended-upgrades==0.1
+urllib3==1.25.8
+w3lib==1.22.0
+wadllib==1.3.3
+wandb==0.12.15
+websocket-client==1.3.2
+websockets==10.3
+werkzeug==2.1.2
+wheel==0.34.2
+yarl==1.7.2
+zipp==1.0.0
+zope.interface==4.7.1
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-metadata.json b/Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-metadata.json
new file mode 100644
index 0000000..c21a805
--- /dev/null
+++ b/Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-metadata.json
@@ -0,0 +1,24 @@
+{
+    "os": "Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2022-07-04T13:48:09.186929",
+    "startedAt": "2022-07-04T13:48:06.927528",
+    "docker": null,
+    "gpu": "NVIDIA GeForce GTX 1650",
+    "gpu_count": 1,
+    "cpu_count": 8,
+    "cuda": null,
+    "args": [],
+    "state": "running",
+    "program": "/home/sol/git/QML2/run.py",
+    "codePath": "run.py",
+    "git": {
+        "remote": "https://github.com/PVNkT/QML2.git",
+        "commit": "1f3b04ad3f84cb17a3713da979f0adf1e2028c86"
+    },
+    "email": "3031902@gmail.com",
+    "root": "/home/sol/git/QML2",
+    "host": "DESKTOP-HFMS9NT",
+    "username": "sol",
+    "executable": "/bin/python3"
+}
diff --git a/Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-summary.json b/Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-summary.json
new file mode 100644
index 0000000..e682bae
--- /dev/null
+++ b/Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 5}}
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_224806-2ltswfst/logs/debug-internal.log b/Logs/wandb/run-20220704_224806-2ltswfst/logs/debug-internal.log
new file mode 100644
index 0000000..c2e8a74
--- /dev/null
+++ b/Logs/wandb/run-20220704_224806-2ltswfst/logs/debug-internal.log
@@ -0,0 +1,188 @@
+2022-07-04 22:48:06,936 INFO    StreamThr :2639 [internal.py:wandb_internal():90] W&B internal server running at pid: 2639, started at: 2022-07-04 22:48:06.935988
+2022-07-04 22:48:06,939 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: status
+2022-07-04 22:48:06,941 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: status
+2022-07-04 22:48:06,943 DEBUG   SenderThread:2639 [sender.py:send():232] send: header
+2022-07-04 22:48:06,943 INFO    WriterThread:2639 [datastore.py:open_for_write():75] open: Logs/wandb/run-20220704_224806-2ltswfst/run-2ltswfst.wandb
+2022-07-04 22:48:06,951 DEBUG   SenderThread:2639 [sender.py:send():232] send: run
+2022-07-04 22:48:06,953 INFO    SenderThread:2639 [sender.py:_maybe_setup_resume():489] checking resume status for None/QML-MNIST/2ltswfst
+2022-07-04 22:48:07,926 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: check_version
+2022-07-04 22:48:07,930 INFO    SenderThread:2639 [dir_watcher.py:__init__():166] watching files in: Logs/wandb/run-20220704_224806-2ltswfst/files
+2022-07-04 22:48:07,930 INFO    SenderThread:2639 [sender.py:_start_run_threads():811] run started: 2ltswfst with start time 1656942486
+2022-07-04 22:48:07,931 DEBUG   SenderThread:2639 [sender.py:send():232] send: summary
+2022-07-04 22:48:07,931 INFO    SenderThread:2639 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:48:07,931 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: check_version
+2022-07-04 22:48:08,230 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: run_start
+2022-07-04 22:48:08,938 INFO    Thread-12 :2639 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-summary.json
+2022-07-04 22:48:09,186 DEBUG   HandlerThread:2639 [meta.py:__init__():35] meta init
+2022-07-04 22:48:09,186 DEBUG   HandlerThread:2639 [meta.py:__init__():49] meta init done
+2022-07-04 22:48:09,186 DEBUG   HandlerThread:2639 [meta.py:probe():209] probe
+2022-07-04 22:48:09,195 DEBUG   HandlerThread:2639 [meta.py:_setup_git():199] setup git
+2022-07-04 22:48:09,203 DEBUG   HandlerThread:2639 [meta.py:_setup_git():206] setup git done
+2022-07-04 22:48:09,203 DEBUG   HandlerThread:2639 [meta.py:_save_code():87] save code
+2022-07-04 22:48:09,207 DEBUG   HandlerThread:2639 [meta.py:_save_code():108] save code done
+2022-07-04 22:48:09,208 DEBUG   HandlerThread:2639 [meta.py:_save_patches():125] save patches
+2022-07-04 22:48:09,248 DEBUG   HandlerThread:2639 [meta.py:_save_patches():167] save patches done
+2022-07-04 22:48:09,248 DEBUG   HandlerThread:2639 [meta.py:_save_pip():53] save pip
+2022-07-04 22:48:09,249 DEBUG   HandlerThread:2639 [meta.py:_save_pip():67] save pip done
+2022-07-04 22:48:09,249 DEBUG   HandlerThread:2639 [meta.py:probe():247] probe done
+2022-07-04 22:48:09,310 DEBUG   SenderThread:2639 [sender.py:send():232] send: files
+2022-07-04 22:48:09,311 INFO    SenderThread:2639 [sender.py:_save_file():946] saving file wandb-metadata.json with policy now
+2022-07-04 22:48:09,311 INFO    SenderThread:2639 [sender.py:_save_file():946] saving file code/run.py with policy now
+2022-07-04 22:48:09,312 INFO    SenderThread:2639 [sender.py:_save_file():946] saving file diff.patch with policy now
+2022-07-04 22:48:09,320 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: stop_status
+2022-07-04 22:48:09,321 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: stop_status
+2022-07-04 22:48:09,932 INFO    Thread-12 :2639 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224806-2ltswfst/files/diff.patch
+2022-07-04 22:48:09,932 INFO    Thread-12 :2639 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224806-2ltswfst/files/code/run.py
+2022-07-04 22:48:09,933 INFO    Thread-12 :2639 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-metadata.json
+2022-07-04 22:48:09,933 INFO    Thread-12 :2639 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224806-2ltswfst/files/output.log
+2022-07-04 22:48:09,933 INFO    Thread-12 :2639 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224806-2ltswfst/files/requirements.txt
+2022-07-04 22:48:09,934 INFO    Thread-12 :2639 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_224806-2ltswfst/files/code
+2022-07-04 22:48:09,967 DEBUG   SenderThread:2639 [sender.py:send():232] send: telemetry
+2022-07-04 22:48:09,968 DEBUG   SenderThread:2639 [sender.py:send():232] send: metric
+2022-07-04 22:48:09,968 DEBUG   SenderThread:2639 [sender.py:send():232] send: telemetry
+2022-07-04 22:48:09,968 DEBUG   SenderThread:2639 [sender.py:send():232] send: metric
+2022-07-04 22:48:09,968 WARNING SenderThread:2639 [sender.py:send_metric():904] Seen metric with glob (shouldn't happen)
+2022-07-04 22:48:10,634 INFO    Thread-16 :2639 [upload_job.py:push():137] Uploaded file /tmp/tmp_gzaz1a0wandb/osezd9lj-wandb-metadata.json
+2022-07-04 22:48:10,759 INFO    Thread-18 :2639 [upload_job.py:push():137] Uploaded file /tmp/tmp_gzaz1a0wandb/2444jlp4-diff.patch
+2022-07-04 22:48:11,580 INFO    Thread-17 :2639 [upload_job.py:push():137] Uploaded file /tmp/tmp_gzaz1a0wandb/2mebbt54-code/run.py
+2022-07-04 22:48:11,932 INFO    Thread-12 :2639 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224806-2ltswfst/files/output.log
+2022-07-04 22:48:13,319 DEBUG   SenderThread:2639 [sender.py:send():232] send: exit
+2022-07-04 22:48:13,320 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:13,320 INFO    SenderThread:2639 [sender.py:send_exit():368] handling exit code: 1
+2022-07-04 22:48:13,321 INFO    SenderThread:2639 [sender.py:send_exit():370] handling runtime: 5
+2022-07-04 22:48:13,324 INFO    SenderThread:2639 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:48:13,324 INFO    SenderThread:2639 [sender.py:send_exit():376] send defer
+2022-07-04 22:48:13,324 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:13,325 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:13,325 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 0
+2022-07-04 22:48:13,325 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:13,326 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 0
+2022-07-04 22:48:13,326 INFO    SenderThread:2639 [sender.py:transition_state():389] send defer: 1
+2022-07-04 22:48:13,326 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:13,326 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 1
+2022-07-04 22:48:13,358 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:13,358 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 1
+2022-07-04 22:48:13,358 INFO    SenderThread:2639 [sender.py:transition_state():389] send defer: 2
+2022-07-04 22:48:13,358 DEBUG   SenderThread:2639 [sender.py:send():232] send: stats
+2022-07-04 22:48:13,359 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:13,359 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 2
+2022-07-04 22:48:13,359 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:13,359 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 2
+2022-07-04 22:48:13,359 INFO    SenderThread:2639 [sender.py:transition_state():389] send defer: 3
+2022-07-04 22:48:13,360 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:13,360 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 3
+2022-07-04 22:48:13,360 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:13,360 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 3
+2022-07-04 22:48:13,360 INFO    SenderThread:2639 [sender.py:transition_state():389] send defer: 4
+2022-07-04 22:48:13,360 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:13,360 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 4
+2022-07-04 22:48:13,361 DEBUG   SenderThread:2639 [sender.py:send():232] send: summary
+2022-07-04 22:48:13,361 INFO    SenderThread:2639 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:48:13,361 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:13,361 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 4
+2022-07-04 22:48:13,361 INFO    SenderThread:2639 [sender.py:transition_state():389] send defer: 5
+2022-07-04 22:48:13,362 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:13,362 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 5
+2022-07-04 22:48:13,362 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:13,362 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 5
+2022-07-04 22:48:13,427 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:13,932 INFO    Thread-12 :2639 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-summary.json
+2022-07-04 22:48:13,933 INFO    Thread-12 :2639 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224806-2ltswfst/files/output.log
+2022-07-04 22:48:16,742 INFO    SenderThread:2639 [sender.py:transition_state():389] send defer: 6
+2022-07-04 22:48:16,742 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:16,742 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:16,742 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 6
+2022-07-04 22:48:16,743 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:16,743 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 6
+2022-07-04 22:48:16,743 INFO    SenderThread:2639 [dir_watcher.py:finish():279] shutting down directory watcher
+2022-07-04 22:48:16,843 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:16,934 INFO    SenderThread:2639 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_224806-2ltswfst/files/config.yaml
+2022-07-04 22:48:16,934 INFO    SenderThread:2639 [dir_watcher.py:finish():309] scan: Logs/wandb/run-20220704_224806-2ltswfst/files
+2022-07-04 22:48:16,934 INFO    SenderThread:2639 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224806-2ltswfst/files/output.log output.log
+2022-07-04 22:48:16,934 INFO    SenderThread:2639 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224806-2ltswfst/files/config.yaml config.yaml
+2022-07-04 22:48:16,935 INFO    SenderThread:2639 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224806-2ltswfst/files/diff.patch diff.patch
+2022-07-04 22:48:16,935 INFO    SenderThread:2639 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-metadata.json wandb-metadata.json
+2022-07-04 22:48:16,935 INFO    SenderThread:2639 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224806-2ltswfst/files/requirements.txt requirements.txt
+2022-07-04 22:48:16,935 INFO    SenderThread:2639 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-summary.json wandb-summary.json
+2022-07-04 22:48:16,938 INFO    SenderThread:2639 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_224806-2ltswfst/files/code/run.py code/run.py
+2022-07-04 22:48:16,940 INFO    SenderThread:2639 [sender.py:transition_state():389] send defer: 7
+2022-07-04 22:48:16,940 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:16,945 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:16,946 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 7
+2022-07-04 22:48:16,946 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:16,946 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 7
+2022-07-04 22:48:16,946 INFO    SenderThread:2639 [file_pusher.py:finish():145] shutting down file pusher
+2022-07-04 22:48:17,046 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:17,046 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:17,147 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:17,148 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:17,249 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:17,249 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:17,350 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:17,350 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:17,452 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:17,452 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:17,553 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:17,554 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:17,655 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:17,655 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:17,756 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:17,757 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:17,800 INFO    Thread-21 :2639 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224806-2ltswfst/files/requirements.txt
+2022-07-04 22:48:17,805 INFO    Thread-19 :2639 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224806-2ltswfst/files/output.log
+2022-07-04 22:48:17,826 INFO    Thread-22 :2639 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224806-2ltswfst/files/wandb-summary.json
+2022-07-04 22:48:17,858 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:17,858 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:17,959 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:17,960 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:18,061 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:18,061 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:18,162 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:18,162 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:18,264 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:18,264 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:18,366 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:18,366 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:18,468 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:18,468 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:18,569 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:18,570 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:18,601 INFO    Thread-20 :2639 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_224806-2ltswfst/files/config.yaml
+2022-07-04 22:48:18,671 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:18,671 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:18,772 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:18,772 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:18,802 INFO    Thread-11 :2639 [sender.py:transition_state():389] send defer: 8
+2022-07-04 22:48:18,802 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:18,802 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 8
+2022-07-04 22:48:18,803 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:18,803 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 8
+2022-07-04 22:48:18,874 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:19,046 INFO    SenderThread:2639 [sender.py:transition_state():389] send defer: 9
+2022-07-04 22:48:19,046 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:19,047 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:19,047 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 9
+2022-07-04 22:48:19,048 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:19,048 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 9
+2022-07-04 22:48:19,048 INFO    SenderThread:2639 [sender.py:transition_state():389] send defer: 10
+2022-07-04 22:48:19,049 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:48:19,049 INFO    HandlerThread:2639 [handler.py:handle_request_defer():164] handle defer: 10
+2022-07-04 22:48:19,049 DEBUG   SenderThread:2639 [sender.py:send():232] send: final
+2022-07-04 22:48:19,050 DEBUG   SenderThread:2639 [sender.py:send():232] send: footer
+2022-07-04 22:48:19,050 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:48:19,050 INFO    SenderThread:2639 [sender.py:send_request_defer():385] handle sender defer: 10
+2022-07-04 22:48:19,149 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:48:19,150 DEBUG   SenderThread:2639 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:48:19,150 INFO    SenderThread:2639 [file_pusher.py:join():150] waiting for file pusher
+2022-07-04 22:48:20,177 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: sampled_history
+2022-07-04 22:48:20,177 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: get_summary
+2022-07-04 22:48:20,178 INFO    MainThread:2639 [wandb_run.py:_footer_history_summary_info():3102] rendering history
+2022-07-04 22:48:20,178 INFO    MainThread:2639 [wandb_run.py:_footer_history_summary_info():3134] rendering summary
+2022-07-04 22:48:20,178 INFO    MainThread:2639 [wandb_run.py:_footer_sync_info():3057] logging synced files
+2022-07-04 22:48:20,178 DEBUG   HandlerThread:2639 [handler.py:handle_request():141] handle_request: shutdown
+2022-07-04 22:48:20,178 INFO    HandlerThread:2639 [handler.py:finish():806] shutting down handler
+2022-07-04 22:48:21,049 INFO    WriterThread:2639 [datastore.py:close():279] close: Logs/wandb/run-20220704_224806-2ltswfst/run-2ltswfst.wandb
+2022-07-04 22:48:21,076 INFO    SenderThread:2639 [sender.py:finish():1106] shutting down sender
+2022-07-04 22:48:21,076 INFO    SenderThread:2639 [file_pusher.py:finish():145] shutting down file pusher
+2022-07-04 22:48:21,076 INFO    SenderThread:2639 [file_pusher.py:join():150] waiting for file pusher
+2022-07-04 22:48:21,181 INFO    MainThread:2639 [internal.py:handle_exit():80] Internal process exited
diff --git a/Logs/wandb/run-20220704_224806-2ltswfst/logs/debug.log b/Logs/wandb/run-20220704_224806-2ltswfst/logs/debug.log
new file mode 100644
index 0000000..8ff9424
--- /dev/null
+++ b/Logs/wandb/run-20220704_224806-2ltswfst/logs/debug.log
@@ -0,0 +1,24 @@
+2022-07-04 22:48:06,929 INFO    MainThread:2604 [wandb_setup.py:_flush():75] Loading settings from /home/sol/.config/wandb/settings
+2022-07-04 22:48:06,929 INFO    MainThread:2604 [wandb_setup.py:_flush():75] Loading settings from /home/sol/git/QML2/wandb/settings
+2022-07-04 22:48:06,929 INFO    MainThread:2604 [wandb_setup.py:_flush():75] Loading settings from environment variables: {'_require_service': 'True'}
+2022-07-04 22:48:06,929 INFO    MainThread:2604 [wandb_setup.py:_flush():75] Inferring run settings from compute environment: {'program_relpath': 'run.py', 'program': '/home/sol/git/QML2/run.py'}
+2022-07-04 22:48:06,929 INFO    MainThread:2604 [wandb_init.py:_log_setup():437] Logging user logs to Logs/wandb/run-20220704_224806-2ltswfst/logs/debug.log
+2022-07-04 22:48:06,929 INFO    MainThread:2604 [wandb_init.py:_log_setup():438] Logging internal logs to Logs/wandb/run-20220704_224806-2ltswfst/logs/debug-internal.log
+2022-07-04 22:48:06,929 INFO    MainThread:2604 [wandb_init.py:init():471] calling init triggers
+2022-07-04 22:48:06,929 INFO    MainThread:2604 [wandb_init.py:init():474] wandb.init called with sweep_config: {}
+config: {}
+2022-07-04 22:48:06,929 INFO    MainThread:2604 [wandb_init.py:init():524] starting backend
+2022-07-04 22:48:06,931 INFO    MainThread:2604 [backend.py:_multiprocessing_setup():97] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2022-07-04 22:48:06,932 INFO    MainThread:2604 [wandb_init.py:init():533] backend started and connected
+2022-07-04 22:48:06,936 INFO    MainThread:2604 [wandb_init.py:init():597] updated telemetry
+2022-07-04 22:48:06,950 INFO    MainThread:2604 [wandb_init.py:init():628] communicating run to backend with 30 second timeout
+2022-07-04 22:48:07,923 INFO    MainThread:2604 [wandb_run.py:_on_init():1923] communicating current version
+2022-07-04 22:48:08,223 INFO    MainThread:2604 [wandb_run.py:_on_init():1927] got version response upgrade_message: "wandb version 0.12.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2022-07-04 22:48:08,223 INFO    MainThread:2604 [wandb_init.py:init():659] starting run threads in backend
+2022-07-04 22:48:09,318 INFO    MainThread:2604 [wandb_run.py:_console_start():1897] atexit reg
+2022-07-04 22:48:09,318 INFO    MainThread:2604 [wandb_run.py:_redirect():1770] redirect: SettingsConsole.WRAP
+2022-07-04 22:48:09,319 INFO    MainThread:2604 [wandb_run.py:_redirect():1807] Wrapping output streams.
+2022-07-04 22:48:09,321 INFO    MainThread:2604 [wandb_run.py:_redirect():1831] Redirects installed.
+2022-07-04 22:48:09,321 INFO    MainThread:2604 [wandb_init.py:init():684] run started, returning control to user process
+2022-07-04 22:48:21,181 WARNING MsgRouterThr:2604 [router.py:message_loop():76] message_loop has been closed
diff --git a/Logs/wandb/run-20220704_224806-2ltswfst/run-2ltswfst.wandb b/Logs/wandb/run-20220704_224806-2ltswfst/run-2ltswfst.wandb
new file mode 100644
index 0000000..59abe73
Binary files /dev/null and b/Logs/wandb/run-20220704_224806-2ltswfst/run-2ltswfst.wandb differ
diff --git a/Logs/wandb/run-20220704_225226-39dyvroy/files/code/run.py b/Logs/wandb/run-20220704_225226-39dyvroy/files/code/run.py
new file mode 100644
index 0000000..69d3f58
--- /dev/null
+++ b/Logs/wandb/run-20220704_225226-39dyvroy/files/code/run.py
@@ -0,0 +1,25 @@
+from omegaconf import OmegaConf
+from pathlib import Path
+
+from src.runners import S_Runner
+
+CONFIG_DIR = Path("Configs")
+
+
+def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+    model_params = OmegaConf.load(CONFIG_DIR / "models.yaml")
+    cfg = OmegaConf.merge(cfg, model_params)
+    cfg.merge_with_cli()
+
+    runner = S_Runner(
+        log=cfg.log,
+        optimizer=cfg.optimizer,
+        loader=cfg.loader,
+        network=cfg.network,
+        data=cfg.data,
+    )
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/Logs/wandb/run-20220704_225226-39dyvroy/files/config.yaml b/Logs/wandb/run-20220704_225226-39dyvroy/files/config.yaml
new file mode 100644
index 0000000..8ff1d1e
--- /dev/null
+++ b/Logs/wandb/run-20220704_225226-39dyvroy/files/config.yaml
@@ -0,0 +1,208 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.12.15
+    code_path: code/run.py
+    framework: lightning
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    m:
+    - 1: trainer/global_step
+      6:
+      - 3
+    python_version: 3.8.10
+    start_time: 1656942746
+    t:
+      1:
+      - 1
+      - 5
+      - 9
+      - 41
+      - 53
+      - 55
+      3:
+      - 1
+      - 7
+      - 23
+      4: 3.8.10
+      5: 0.12.15
+      8:
+      - 5
+inputs:
+  desc: null
+  value: None
+net/LSTM/bidirectional:
+  desc: null
+  value: true
+net/LSTM/hidden_size:
+  desc: null
+  value: 128
+net/LSTM/input_size:
+  desc: null
+  value: 116
+net/LSTM/num_layers:
+  desc: null
+  value: 1
+net/Simple_QHN/backend:
+  desc: null
+  value: aer_simulator
+net/Simple_QHN/linear_out:
+  desc: null
+  value: 64
+net/Simple_QHN/lstm_hidden:
+  desc: null
+  value: 128
+net/Simple_QHN/n_qubits:
+  desc: null
+  value: 2
+net/Simple_QHN/shift:
+  desc: null
+  value: 0.6
+net/hybrid/dense_type:
+  desc: null
+  value: 2
+net/hybrid/n_qubits:
+  desc: null
+  value: 3
+net/hybrid/shift:
+  desc: null
+  value: 0.5
+net/hybrid/shots:
+  desc: null
+  value: 100
+net/hybrid/simulator:
+  desc: null
+  value: aer_simulator
+net/model:
+  desc: null
+  value: Simple_QHN
+net/num_classes:
+  desc: null
+  value: 2
+net/roi_rank:
+  desc: null
+  value:
+  - 0
+  - 1
+  - 2
+  - 3
+  - 4
+  - 5
+  - 6
+  - 7
+  - 8
+  - 9
+  - 10
+  - 11
+  - 12
+  - 13
+  - 14
+  - 15
+  - 16
+  - 17
+  - 18
+  - 19
+  - 20
+  - 21
+  - 22
+  - 23
+  - 24
+  - 25
+  - 26
+  - 27
+  - 28
+  - 29
+  - 30
+  - 31
+  - 32
+  - 33
+  - 34
+  - 35
+  - 36
+  - 37
+  - 38
+  - 39
+  - 40
+  - 41
+  - 42
+  - 43
+  - 44
+  - 45
+  - 46
+  - 47
+  - 48
+  - 49
+  - 50
+  - 51
+  - 52
+  - 53
+  - 54
+  - 55
+  - 56
+  - 57
+  - 58
+  - 59
+  - 60
+  - 61
+  - 62
+  - 63
+  - 64
+  - 65
+  - 66
+  - 67
+  - 68
+  - 69
+  - 70
+  - 71
+  - 72
+  - 73
+  - 74
+  - 75
+  - 76
+  - 77
+  - 78
+  - 79
+  - 80
+  - 81
+  - 82
+  - 83
+  - 84
+  - 85
+  - 86
+  - 87
+  - 88
+  - 89
+  - 90
+  - 91
+  - 92
+  - 93
+  - 94
+  - 95
+  - 96
+  - 97
+  - 98
+  - 99
+  - 100
+  - 101
+  - 102
+  - 103
+  - 104
+  - 105
+  - 106
+  - 107
+  - 108
+  - 109
+  - 110
+  - 111
+  - 112
+  - 113
+  - 114
+  - 115
+opt/lr:
+  desc: null
+  value: 1.0e-05
+opt/optimizer:
+  desc: null
+  value: Adam
diff --git a/Logs/wandb/run-20220704_225226-39dyvroy/files/diff.patch b/Logs/wandb/run-20220704_225226-39dyvroy/files/diff.patch
new file mode 100644
index 0000000..c121913
--- /dev/null
+++ b/Logs/wandb/run-20220704_225226-39dyvroy/files/diff.patch
@@ -0,0 +1,446 @@
+diff --git a/Configs/config.yaml b/Configs/config.yaml
+index 541dd0b..aa00027 100644
+--- a/Configs/config.yaml
++++ b/Configs/config.yaml
+@@ -31,12 +31,7 @@ data:
+ network:
+   model: ${log.model_name}
+   num_classes: 2
+-  encoder: ${SCCNN_bn}
+-  attention: ${Attention}
+   LSTM: ${LSTM}
+-  decoder: ${Decoder}
+-  is_encoder_shared: True
+-  encoderType: SCCNN_bn
+   Simple_QHN: ${Simple_QHN}
+   hybrid: ${quantum_circuit}
+ 
+@@ -45,5 +40,4 @@ optimizer:
+   lr: 1e-5
+ 
+ 
+-runner: OneSiteHoldout_Runner
+ 
+diff --git a/Configs/models.yaml b/Configs/models.yaml
+index 2c5c2ba..e561fb7 100644
+--- a/Configs/models.yaml
++++ b/Configs/models.yaml
+@@ -1,6 +1,6 @@
+ 
+ LSTM:
+-  input_size: ${SCCNN.conv_block4.out_f}
++  input_size: 116
+   hidden_size: 128
+   num_layers: 1
+   bidirectional: True
+@@ -8,9 +8,9 @@ LSTM:
+ Simple_QHN:
+   n_qubits: 2
+   shift: 0.6
+-  is_cnot: True
+   lstm_hidden: 128
+   linear_out: 64
++  backend: aer_simulator
+ 
+ 
+ quantum_circuit:
+@@ -18,5 +18,4 @@ quantum_circuit:
+   simulator: aer_simulator
+   shift: 0.5
+   shots: 100
+-  is_cnot: True
+   dense_type: 2
+\ No newline at end of file
+diff --git a/Logs/QML-MNIST/version000/version_0/hparams.yaml b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+index 3851257..000d975 100644
+--- a/Logs/QML-MNIST/version000/version_0/hparams.yaml
++++ b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+@@ -15,6 +15,5 @@ net:
+     simulator: aer_simulator
+     shift: 0.5
+     shots: 100
+-    is_cnot: true
+     dense_type: 2
+ inputs: null
+diff --git a/run.py b/run.py
+index c8e1ccf..69d3f58 100644
+--- a/run.py
++++ b/run.py
+@@ -1,7 +1,7 @@
+ from omegaconf import OmegaConf
+ from pathlib import Path
+ 
+-from src.runners import S_Runner, MNIST_Runner
++from src.runners import S_Runner
+ 
+ CONFIG_DIR = Path("Configs")
+ 
+@@ -11,7 +11,7 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+     cfg = OmegaConf.merge(cfg, model_params)
+     cfg.merge_with_cli()
+ 
+-    runner = MNIST_Runner(
++    runner = S_Runner(
+         log=cfg.log,
+         optimizer=cfg.optimizer,
+         loader=cfg.loader,
+@@ -22,4 +22,4 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+ 
+ 
+ if __name__ == "__main__":
+-    main(cfg=OmegaConf.load(CONFIG_DIR / "config MNIST.yaml"))
++    main()
+diff --git a/src/data/__pycache__/data_etl.cpython-38.pyc b/src/data/__pycache__/data_etl.cpython-38.pyc
+index 288d14c..934cbd5 100644
+Binary files a/src/data/__pycache__/data_etl.cpython-38.pyc and b/src/data/__pycache__/data_etl.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/loader.cpython-38.pyc b/src/data/__pycache__/loader.cpython-38.pyc
+index eddf358..67504ef 100644
+Binary files a/src/data/__pycache__/loader.cpython-38.pyc and b/src/data/__pycache__/loader.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/sampling.cpython-38.pyc b/src/data/__pycache__/sampling.cpython-38.pyc
+index a2cbaf3..a748fd2 100644
+Binary files a/src/data/__pycache__/sampling.cpython-38.pyc and b/src/data/__pycache__/sampling.cpython-38.pyc differ
+diff --git a/src/data/data_etl.py b/src/data/data_etl.py
+index 08700be..dc691b0 100755
+--- a/src/data/data_etl.py
++++ b/src/data/data_etl.py
+@@ -137,24 +137,6 @@ def chunks(lst, n):
+     return [lst[i : i + div] for i in range(0, len(lst), div)]
+ 
+ 
+-@dataclass
+-class Load_MNIST:
+-    def __init__(self, is_train:Boolean):
+-        if is_train:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=True, download=True,
+-                         transform=transforms.Compose([transforms.ToTensor()]))
+-        else:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=False, download=True,
+-                        transform=transforms.Compose([transforms.ToTensor()]))
+-                        
+-    def get_samples(self, n_samples:int):
+-        idx = []
+-        for i in range(10):
+-            idx.append(np.where(self.dataset.targets == i)[0][:n_samples])#각 target에 해당되는 번호를 저장한다. 
+-        idx = np.array(idx).reshape(-1)
+-        self.dataset.data = self.dataset.data[idx]
+-        self.dataset.targets = self.dataset.targets[idx]
+-        return self.dataset
+ 
+ 
+ 
+diff --git a/src/data/loader.py b/src/data/loader.py
+index 288a3b3..158cbdc 100644
+--- a/src/data/loader.py
++++ b/src/data/loader.py
+@@ -5,25 +5,8 @@ from typing import Union, List
+ import torch
+ from torch.utils.data import Dataset
+ 
+-from src.data import Load, SITES_DICT, Load_MNIST
+-
+-class MNISTDataset(Dataset):
+-    def __init__(self, n_samples: int, is_train: Boolean) -> None:
+-        Load = Load_MNIST(is_train)
+-        if n_samples >0:
+-            self.data = Load.get_samples(n_samples=n_samples).data
+-            self.labels = Load.get_samples(n_samples=n_samples).targets  
+-        else:  
+-            self.data = Load.dataset.data
+-            self.labels = Load.dataset.targets
++from src.data import Load, SITES_DICT
+ 
+-    def __len__(self):
+-        return len(self.labels)
+-    
+-    def __getitem__(self, index: int):
+-        data = self.data[index]
+-        label = self.labels[index]
+-        return data, label 
+   
+ class ROIDataset(Dataset):
+     def __init__( self, site: Union[List, str]) -> None:
+diff --git a/src/datamodules/__pycache__/datamodule.cpython-38.pyc b/src/datamodules/__pycache__/datamodule.cpython-38.pyc
+index 6f06b1a..d4ba074 100644
+Binary files a/src/datamodules/__pycache__/datamodule.cpython-38.pyc and b/src/datamodules/__pycache__/datamodule.cpython-38.pyc differ
+diff --git a/src/datamodules/datamodule.py b/src/datamodules/datamodule.py
+index 4fcab3d..0bfab90 100644
+--- a/src/datamodules/datamodule.py
++++ b/src/datamodules/datamodule.py
+@@ -7,64 +7,23 @@ from torch.utils.data import DataLoader, Subset
+ from pytorch_lightning import LightningDataModule
+ from src.data import collate_fn, SamplerFactory
+ 
++
+ @dataclass
+-class MNISTDataModule(LightningDataModule):
+-    
+-    def __init__(self,data: Dict,loader: Dict,dataset: Dict):
++class DataModule(LightningDataModule):
++    def __init__(self, data: Dict, loader: Dict, dataset: Dict):
+         super().__init__()
+-        self.prepare_data_per_node = True
+         self.data = data
+         self.loader = loader
+         self.dataset = dataset
+-    def setup(self, stage: Optional[str] = None):
+-        if stage in ("fit", None):
+-            self.train_dataset = self.dataset(n_samples = self.loader.samples.train_sample, is_train=True)
+-            self.val_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-
+-        if stage in ("test", None):
+-            self.test_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-    
+-    def train_dataloader(self):
+-        conf = deepcopy(self.loader.train)
+-        batch_size = conf.pop("batch_size")
+-        conf.shuffle = False
+-        # return DataLoader(self.train_dataset, **conf, collate_fn=collate_fn)
+-
+-        return DataLoader(
+-            self.train_dataset,
+-            **conf,
+-            collate_fn=collate_fn,
+-            batch_sampler=SamplerFactory().get(
+-                class_idxs=[
+-                    np.where(self.train_dataset.labels == i)[0].tolist()
+-                    for i in range(10)
+-                ],
+-                batch_size=batch_size,
+-                n_batches=len(self.train_dataset)//batch_size + 5,
+-                alpha=1.0,
+-                kind="random",
+-            ),
+-        )
+-
+-    def val_dataloader(self):
+-        return DataLoader(self.val_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-    def test_dataloader(self):
+-        return DataLoader(self.test_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-@dataclass
+-class DataModule(LightningDataModule):
+-    data: Dict
+-    loader: Dict
+-    dataset: Dict
++        self.prepare_data_per_node = True
+ 
+     def setup(self, stage: Optional[str] = None):
+         if stage in ("fit", None):
+-            self.train_dataset = self.dataset(is_train = True, site = self.data.train_site)
+-            self.val_dataset = self.dataset(is_train = False, site = self.data.train_site)
++            self.train_dataset = self.dataset(site = self.data.train_site)
++            self.val_dataset = self.dataset(site = self.data.train_site)
+ 
+         if stage in ("test", None):
+-            self.test_dataset = self.dataset(is_train = False, site = self.data.train_site)
++            self.test_dataset = self.dataset(site = self.data.train_site)
+ 
+     def train_dataloader(self):
+         conf = deepcopy(self.loader.train)
+diff --git a/src/layers/__pycache__/__init__.cpython-38.pyc b/src/layers/__pycache__/__init__.cpython-38.pyc
+index 31453e2..a73c856 100644
+Binary files a/src/layers/__pycache__/__init__.cpython-38.pyc and b/src/layers/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/layers/__pycache__/quantum_layers.cpython-38.pyc b/src/layers/__pycache__/quantum_layers.cpython-38.pyc
+index 4f219f9..3e30b2e 100644
+Binary files a/src/layers/__pycache__/quantum_layers.cpython-38.pyc and b/src/layers/__pycache__/quantum_layers.cpython-38.pyc differ
+diff --git a/src/layers/quantum_layers.py b/src/layers/quantum_layers.py
+index 976aa6a..4dfccfa 100644
+--- a/src/layers/quantum_layers.py
++++ b/src/layers/quantum_layers.py
+@@ -35,7 +35,7 @@ class QuantumCircuit:
+         for theta, qubit in zip(self.theta, self.all_qubits):
+             self._circuit.ry(theta, qubit)
+ 
+-        for i in [2,3,4,5]:
++        for i in range(2, self.n_qubit):
+             self._circuit.cx(0,i)
+ 
+         self._circuit.measure_all()
+diff --git a/src/models/__pycache__/__init__.cpython-38.pyc b/src/models/__pycache__/__init__.cpython-38.pyc
+index f5a0d8c..9643058 100644
+Binary files a/src/models/__pycache__/__init__.cpython-38.pyc and b/src/models/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/models/__pycache__/qhn.cpython-38.pyc b/src/models/__pycache__/qhn.cpython-38.pyc
+index 56e2a1f..8b4a754 100644
+Binary files a/src/models/__pycache__/qhn.cpython-38.pyc and b/src/models/__pycache__/qhn.cpython-38.pyc differ
+diff --git a/src/models/qhn.py b/src/models/qhn.py
+index c74fec4..2e08fba 100644
+--- a/src/models/qhn.py
++++ b/src/models/qhn.py
+@@ -27,10 +27,10 @@ class Simple_QHN(nn.Module):
+         self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+         self.hybrid = Hybrid(
+             params.n_qubits,
+-            qiskit.Aer.get_backend("aer_simulator"),
++            params.backend,
+             100,
+             shift=params.shift,
+-            is_cnot=params.is_cnot,
++            
+         )
+         self.fc3 = nn.Linear(params.n_qubits * 2, 2)
+         self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+@@ -55,35 +55,3 @@ class Simple_QHN(nn.Module):
+         return x
+ 
+ 
+-class MNIST_QHN(nn.Module):
+-    def __init__(self, params: Optional[Dict] = None, *args, **kwargs) -> None:
+-        super(MNIST_QHN, self).__init__()
+-        params = params.MNIST_QHN
+-        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
+-        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
+-        self.dropout = nn.Dropout2d()
+-        self.fc1 = nn.Linear(256, params.linear_out)
+-        self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+-        self.hybrid = Hybrid(
+-            params.n_qubits,
+-            backend = "aer_simulator",
+-            shots = params.shots,
+-            shift=params.shift,
+-        )
+-        self.fc3 = nn.Linear(2**params.n_qubits, 10)
+-        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+-
+-    def forward(self, x):
+-        x = x.unsqueeze(1)
+-        x = F.relu(self.conv1(x))
+-        x = F.max_pool2d(x, 2)
+-        x = F.relu(self.conv2(x))
+-        x = F.max_pool2d(x, 2)
+-        
+-        x = x.view(x.shape[0],-1)
+-        x = F.relu(self.fc1(x))
+-        x = self.fc2(x)
+-        x = torch.tanh(x) * torch.ones_like(x) * torch.tensor(np.pi / 2)
+-        x = self.hybrid(x).to(self.device)
+-        x = F.softmax(self.fc3(x), dim=1)
+-        return x
+diff --git a/src/runners/__pycache__/base.cpython-38.pyc b/src/runners/__pycache__/base.cpython-38.pyc
+index 6fae27c..b7d08e9 100644
+Binary files a/src/runners/__pycache__/base.cpython-38.pyc and b/src/runners/__pycache__/base.cpython-38.pyc differ
+diff --git a/src/runners/__pycache__/runner.cpython-38.pyc b/src/runners/__pycache__/runner.cpython-38.pyc
+index 9813d76..3792f76 100644
+Binary files a/src/runners/__pycache__/runner.cpython-38.pyc and b/src/runners/__pycache__/runner.cpython-38.pyc differ
+diff --git a/src/runners/runner.py b/src/runners/runner.py
+index 86b61a5..5fb03f3 100644
+--- a/src/runners/runner.py
++++ b/src/runners/runner.py
+@@ -6,8 +6,8 @@ from typing import Optional
+ from copy import deepcopy
+ 
+ from src.runners import Base_Runner
+-from src.data import ROIDataset, SITES_DICT, MNISTDataset
+-from src.datamodules import DataModule, MNISTDataModule
++from src.data import ROIDataset, SITES_DICT
++from src.datamodules import DataModule
+ from src.tasks import ClassificationTask
+ from src.utils import plot_paper
+ from src.callbacks import wandb_callback as wbc
+@@ -177,106 +177,4 @@ class S_Runner(Base_Runner):
+ 
+         return
+ 
+-class MNIST_Runner(Base_Runner):
+-    def get_callbacks(self):
+-        """
+-        Write only callbacks that logger is not necessary
+-        """
+-        checkpoint_callback = ModelCheckpoint(
+-            dirpath=os.path.join(
+-                self.log.checkpoint_path,
+-                self.log.project_name,
+-                f"version{self.version:03d}",
+-            ),
+-            filename=os.path.join(f"model"),
+-            monitor=f"Accuracy/val",
+-            mode="max",
+-            verbose=False,
+-            save_top_k=1,
+-        )
+-
+-        callbacks = dict(
+-            filter(lambda item: item[0].endswith("callback"), vars().items())
+-        ).values()
+-        callbacks = list(callbacks)
+-        return callbacks if len(callbacks) > 0 else None
+-
+-    def run(self, profiler: Optional[str] = None):
+-        os.makedirs(
+-            os.path.join(self.log.checkpoint_path, self.log.project_name), exist_ok=True
+-        )
+-        self.version = len(
+-            os.listdir(os.path.join(self.log.checkpoint_path, self.log.project_name))
+-        )
+-
+-
+-        final_results = list()
+-
+-
+-
+-        dm = self.get_datamodule(dataset=MNISTDataset, datamodule=MNISTDataModule)
+-        model = self.get_network(Task=ClassificationTask)
+-        model.apply(initialize_weights)
+-
+-        trainer = Trainer(
+-            logger=[
+-                TensorBoardLogger(
+-                    save_dir=self.log.log_path,
+-                    name=os.path.join(
+-                        self.log.project_name,
+-                        f"version{self.version:03d}"
+-                    ),
+-                    default_hp_metric=False,
+-                    version=None,
+-                    # log_graph=True, # inavailable due to bug
+-                ),
+-                WandbLogger(
+-                    project=self.log.project_name,
+-                    save_dir=self.log.log_path,
+-                ),
+-            ],
+-            # ! use all gpu
+-            # gpus=-1,
+-            # auto_select_gpus=True,
+-            # ! use 2 gpu
+-            # devices=2,
+-            # accelerator="auto",
+-            # strategy="ddp",
+-            # ! use gpu 0
+-            devices=[0],
+-            accelerator="gpu",
+-            #devices=[self.log.device.gpu],
+-            #accelerator="cpu",
+-            check_val_every_n_epoch=self.log.val_log_freq_epoch,
+-            log_every_n_steps=1,
+-            num_sanity_val_steps=0,
+-            max_epochs=self.log.epoch,
+-            profiler=profiler,
+-            callbacks=[
+-                *self.get_callbacks(),
+-                wbc.WatchModel(),
+-                wbc.LogConfusionMatrix(),
+-                wbc.LogF1PrecRecHeatmap(),
+-                # tbc.WatchModel(),
+-                # tbc.LogConfusionMatrix(),
+-                # tbc.LogF1PrecRecHeatmap(),
+-            ],
+-            precision=self.log.precision,
+-            # gradient_clip_val=0.5,
+-        )
+-        trainer.test_site_prefix = model.prefix
+-        trainer.fit(model, datamodule=dm)
+-        trainer.test(model, datamodule=dm, ckpt_path="best")
+-        final_results.append(
+-            trainer.callback_metrics[f"Accuracy/test"]
+-        )
+-
+-    try:
+-        import wandb
+-
+-        wb_logger = wbc.get_wandb_logger(trainer=Trainer)
+-        
+-    except Exception as e:
+-        print(e)
+-
+     
diff --git a/Logs/wandb/run-20220704_225226-39dyvroy/files/output.log b/Logs/wandb/run-20220704_225226-39dyvroy/files/output.log
new file mode 100644
index 0000000..1905dcd
--- /dev/null
+++ b/Logs/wandb/run-20220704_225226-39dyvroy/files/output.log
@@ -0,0 +1,267 @@
+GPU available: True, used: True
+TPU available: False, using: 0 TPU cores
+IPU available: False, using: 0 IPUs
+HPU available: False, using: 0 HPUs
+Epoch 0:   0%|                                                                                 | 0/14 [00:00<?, ?it/s]
+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
+  | Name        | Type             | Params
+-------------------------------------------------
+0 | model       | Simple_QHN       | 170 K
+1 | get_metrics | MetricCollection | 0
+-------------------------------------------------
+170 K     Trainable params
+0         Non-trainable params
+170 K     Total params
+0.680     Total estimated model params size (MB)
+Missing logger folder: Logs/QML-MNIST/version001/NYU
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
+    results = self._run_stage()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
+    return self._run_train()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
+    self.fit_loop.run()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
+    self._outputs = self.epoch_loop.run(self._data_fetcher)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
+    batch_output = self.batch_loop.run(batch, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
+    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
+    result = self._run_optimization(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
+    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
+    self.trainer._call_lightning_module_hook(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
+    optimizer.step(closure=optimizer_closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
+    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
+    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
+    return optimizer.step(closure=closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
+    return wrapped(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
+    loss = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
+    closure_result = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
+    self._result = self.closure(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
+    step_output = self._step_fn()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
+    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
+    return self.model.training_step(*args, **kwargs)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 61, in training_step
+    loss, preds, targets = self._shared_step(batch, batch_idx)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 40, in _shared_step
+    preds = self.model(x)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/git/QML2/src/models/qhn.py", line 54, in forward
+    x = F.softmax(self.fc3(x), dim=1)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
+    return F.linear(input, self.weight, self.bias)
+RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x8 and 4x2)
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
+    results = self._run_stage()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
+    return self._run_train()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
+    self.fit_loop.run()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
+    self._outputs = self.epoch_loop.run(self._data_fetcher)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
+    batch_output = self.batch_loop.run(batch, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
+    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
+    result = self._run_optimization(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
+    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
+    self.trainer._call_lightning_module_hook(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
+    optimizer.step(closure=optimizer_closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
+    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
+    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
+    return optimizer.step(closure=closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
+    return wrapped(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
+    loss = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
+    closure_result = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
+    self._result = self.closure(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
+    step_output = self._step_fn()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
+    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
+    return self.model.training_step(*args, **kwargs)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 61, in training_step
+    loss, preds, targets = self._shared_step(batch, batch_idx)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 40, in _shared_step
+    preds = self.model(x)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/git/QML2/src/models/qhn.py", line 54, in forward
+    x = F.softmax(self.fc3(x), dim=1)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
+    return F.linear(input, self.weight, self.bias)
+RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x8 and 4x2)
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
+    results = self._run_stage()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
+    return self._run_train()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
+    self.fit_loop.run()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
+    self._outputs = self.epoch_loop.run(self._data_fetcher)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
+    batch_output = self.batch_loop.run(batch, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
+    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
+    result = self._run_optimization(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
+    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
+    self.trainer._call_lightning_module_hook(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
+    optimizer.step(closure=optimizer_closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
+    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
+    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
+    return optimizer.step(closure=closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
+    return wrapped(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
+    loss = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
+    closure_result = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
+    self._result = self.closure(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
+    step_output = self._step_fn()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
+    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
+    return self.model.training_step(*args, **kwargs)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 61, in training_step
+    loss, preds, targets = self._shared_step(batch, batch_idx)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 40, in _shared_step
+    preds = self.model(x)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/git/QML2/src/models/qhn.py", line 54, in forward
+    x = F.softmax(self.fc3(x), dim=1)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
+    return F.linear(input, self.weight, self.bias)
diff --git a/Logs/wandb/run-20220704_225226-39dyvroy/files/requirements.txt b/Logs/wandb/run-20220704_225226-39dyvroy/files/requirements.txt
new file mode 100644
index 0000000..f9f8632
--- /dev/null
+++ b/Logs/wandb/run-20220704_225226-39dyvroy/files/requirements.txt
@@ -0,0 +1,166 @@
+absl-py==1.0.0
+aiohttp==3.8.1
+aiosignal==1.2.0
+antlr4-python3-runtime==4.8
+async-timeout==4.0.2
+attrs==19.3.0
+automat==0.8.0
+autopep8==1.6.0
+blinker==1.4
+cachetools==5.0.0
+certifi==2019.11.28
+chardet==3.0.4
+charset-normalizer==2.0.12
+click==7.0
+cloud-init==22.1
+colorama==0.4.3
+command-not-found==0.3
+configobj==5.0.6
+constantly==15.1.0
+cryptography==2.8
+cssselect==1.1.0
+cycler==0.11.0
+dbus-python==1.2.16
+dill==0.3.4
+distro-info==0.23ubuntu1
+distro==1.4.0
+docker-pycreds==0.4.0
+entrypoints==0.3
+filelock==3.6.0
+fonttools==4.33.3
+frozenlist==1.3.0
+fsspec==2022.3.0
+gistory==0.44
+gitdb==4.0.9
+gitpython==3.1.27
+google-auth-oauthlib==0.4.6
+google-auth==2.6.6
+grpcio==1.44.0
+httplib2==0.14.0
+hyperlink==19.0.0
+idna==2.8
+importlib-metadata==4.11.3
+incremental==16.10.1
+itemadapter==0.5.0
+itemloaders==1.0.4
+jinja2==2.10.1
+jmespath==1.0.0
+joblib==1.1.0
+jsonpatch==1.22
+jsonpointer==2.0
+jsonschema==3.2.0
+keyring==18.0.1
+kiwisolver==1.4.2
+language-selector==0.1
+launchpadlib==1.10.13
+lazr.restfulclient==0.14.2
+lazr.uri==1.0.3
+lxml==4.8.0
+markdown==3.3.6
+markupsafe==1.1.0
+matplotlib==3.5.1
+more-itertools==4.2.0
+mpmath==1.2.1
+multidict==6.0.2
+netifaces==0.10.4
+nibabel==3.2.2
+nilearn==0.9.1
+ntlm-auth==1.5.0
+numpy==1.22.3
+oauthlib==3.1.0
+omegaconf==2.1.2
+packaging==21.3
+pandas==1.4.2
+parsel==1.6.0
+pathtools==0.1.2
+pbr==5.8.1
+pexpect==4.6.0
+pillow==9.1.0
+pip==20.0.2
+ply==3.11
+promise==2.3
+protego==0.2.1
+protobuf==3.20.1
+psutil==5.9.0
+pyasn1-modules==0.2.1
+pyasn1==0.4.2
+pycodestyle==2.8.0
+pydeprecate==0.3.2
+pydispatcher==2.0.5
+pygame==2.1.2
+pygobject==3.36.0
+pyhamcrest==1.9.0
+pyjwt==1.7.1
+pymacaroons==0.13.0
+pynacl==1.3.0
+pyopenssl==19.0.0
+pyparsing==3.0.8
+pyrsistent==0.15.5
+pyserial==3.4
+python-apt==2.0.0+ubuntu0.20.4.7
+python-constraint==1.4.0
+python-dateutil==2.8.2
+python-debian==0.1.36ubuntu1
+pytorch-lightning==1.6.2
+pytz==2022.1
+pyyaml==6.0
+qiskit-aer==0.10.4
+qiskit-ibmq-provider==0.19.1
+qiskit-ignis==0.7.0
+qiskit-terra==0.20.1
+qiskit==0.36.1
+queuelib==1.6.2
+requests-file==1.5.1
+requests-ntlm==1.1.0
+requests-oauthlib==1.3.1
+requests-unixsocket==0.2.0
+requests==2.22.0
+retworkx==0.11.0
+rsa==4.8
+scikit-learn==1.0.2
+scipy==1.8.0
+seaborn==0.11.2
+secretstorage==2.3.1
+sentry-sdk==1.5.10
+service-identity==18.1.0
+setproctitle==1.2.3
+setuptools==45.2.0
+shortuuid==1.0.8
+simplejson==3.16.0
+six==1.14.0
+sklearn==0.0
+smmap==5.0.0
+sos==4.3
+ssh-import-id==5.10
+stevedore==3.5.0
+symengine==0.9.2
+sympy==1.10.1
+systemd-python==234
+tensorboard-data-server==0.6.1
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.8.0
+threadpoolctl==3.1.0
+tldextract==3.2.1
+toml==0.10.2
+torch-tb-profiler==0.4.0
+torch==1.12.0
+torchmetrics==0.8.1
+torchvision==0.13.0
+tqdm==4.64.0
+tweedledum==1.1.1
+twisted==18.9.0
+typing-extensions==4.2.0
+ubuntu-advantage-tools==27.7
+ufw==0.36
+unattended-upgrades==0.1
+urllib3==1.25.8
+w3lib==1.22.0
+wadllib==1.3.3
+wandb==0.12.15
+websocket-client==1.3.2
+websockets==10.3
+werkzeug==2.1.2
+wheel==0.34.2
+yarl==1.7.2
+zipp==1.0.0
+zope.interface==4.7.1
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-metadata.json b/Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-metadata.json
new file mode 100644
index 0000000..8079d99
--- /dev/null
+++ b/Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-metadata.json
@@ -0,0 +1,24 @@
+{
+    "os": "Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2022-07-04T13:52:27.884559",
+    "startedAt": "2022-07-04T13:52:26.140483",
+    "docker": null,
+    "gpu": "NVIDIA GeForce GTX 1650",
+    "gpu_count": 1,
+    "cpu_count": 8,
+    "cuda": null,
+    "args": [],
+    "state": "running",
+    "program": "/home/sol/git/QML2/run.py",
+    "codePath": "run.py",
+    "git": {
+        "remote": "https://github.com/PVNkT/QML2.git",
+        "commit": "1f3b04ad3f84cb17a3713da979f0adf1e2028c86"
+    },
+    "email": "3031902@gmail.com",
+    "root": "/home/sol/git/QML2",
+    "host": "DESKTOP-HFMS9NT",
+    "username": "sol",
+    "executable": "/bin/python3"
+}
diff --git a/Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-summary.json b/Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-summary.json
new file mode 100644
index 0000000..e682bae
--- /dev/null
+++ b/Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 5}}
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_225226-39dyvroy/logs/debug-internal.log b/Logs/wandb/run-20220704_225226-39dyvroy/logs/debug-internal.log
new file mode 100644
index 0000000..2b56cc2
--- /dev/null
+++ b/Logs/wandb/run-20220704_225226-39dyvroy/logs/debug-internal.log
@@ -0,0 +1,181 @@
+2022-07-04 22:52:26,149 INFO    StreamThr :2895 [internal.py:wandb_internal():90] W&B internal server running at pid: 2895, started at: 2022-07-04 22:52:26.149114
+2022-07-04 22:52:26,153 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: status
+2022-07-04 22:52:26,154 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: status
+2022-07-04 22:52:26,156 DEBUG   SenderThread:2895 [sender.py:send():232] send: header
+2022-07-04 22:52:26,156 INFO    WriterThread:2895 [datastore.py:open_for_write():75] open: Logs/wandb/run-20220704_225226-39dyvroy/run-39dyvroy.wandb
+2022-07-04 22:52:26,162 DEBUG   SenderThread:2895 [sender.py:send():232] send: run
+2022-07-04 22:52:26,164 INFO    SenderThread:2895 [sender.py:_maybe_setup_resume():489] checking resume status for None/QML-MNIST/39dyvroy
+2022-07-04 22:52:26,772 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: check_version
+2022-07-04 22:52:26,774 INFO    SenderThread:2895 [dir_watcher.py:__init__():166] watching files in: Logs/wandb/run-20220704_225226-39dyvroy/files
+2022-07-04 22:52:26,774 INFO    SenderThread:2895 [sender.py:_start_run_threads():811] run started: 39dyvroy with start time 1656942746
+2022-07-04 22:52:26,774 DEBUG   SenderThread:2895 [sender.py:send():232] send: summary
+2022-07-04 22:52:26,774 INFO    SenderThread:2895 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:52:26,775 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: check_version
+2022-07-04 22:52:26,897 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: run_start
+2022-07-04 22:52:27,776 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-summary.json
+2022-07-04 22:52:27,884 DEBUG   HandlerThread:2895 [meta.py:__init__():35] meta init
+2022-07-04 22:52:27,884 DEBUG   HandlerThread:2895 [meta.py:__init__():49] meta init done
+2022-07-04 22:52:27,884 DEBUG   HandlerThread:2895 [meta.py:probe():209] probe
+2022-07-04 22:52:27,896 DEBUG   HandlerThread:2895 [meta.py:_setup_git():199] setup git
+2022-07-04 22:52:27,907 DEBUG   HandlerThread:2895 [meta.py:_setup_git():206] setup git done
+2022-07-04 22:52:27,907 DEBUG   HandlerThread:2895 [meta.py:_save_code():87] save code
+2022-07-04 22:52:27,911 DEBUG   HandlerThread:2895 [meta.py:_save_code():108] save code done
+2022-07-04 22:52:27,911 DEBUG   HandlerThread:2895 [meta.py:_save_patches():125] save patches
+2022-07-04 22:52:27,944 DEBUG   HandlerThread:2895 [meta.py:_save_patches():167] save patches done
+2022-07-04 22:52:27,944 DEBUG   HandlerThread:2895 [meta.py:_save_pip():53] save pip
+2022-07-04 22:52:27,945 DEBUG   HandlerThread:2895 [meta.py:_save_pip():67] save pip done
+2022-07-04 22:52:27,945 DEBUG   HandlerThread:2895 [meta.py:probe():247] probe done
+2022-07-04 22:52:28,001 DEBUG   SenderThread:2895 [sender.py:send():232] send: files
+2022-07-04 22:52:28,002 INFO    SenderThread:2895 [sender.py:_save_file():946] saving file wandb-metadata.json with policy now
+2022-07-04 22:52:28,003 INFO    SenderThread:2895 [sender.py:_save_file():946] saving file code/run.py with policy now
+2022-07-04 22:52:28,005 INFO    SenderThread:2895 [sender.py:_save_file():946] saving file diff.patch with policy now
+2022-07-04 22:52:28,017 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: stop_status
+2022-07-04 22:52:28,021 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: stop_status
+2022-07-04 22:52:28,507 DEBUG   SenderThread:2895 [sender.py:send():232] send: telemetry
+2022-07-04 22:52:28,508 DEBUG   SenderThread:2895 [sender.py:send():232] send: metric
+2022-07-04 22:52:28,509 DEBUG   SenderThread:2895 [sender.py:send():232] send: telemetry
+2022-07-04 22:52:28,509 DEBUG   SenderThread:2895 [sender.py:send():232] send: metric
+2022-07-04 22:52:28,509 WARNING SenderThread:2895 [sender.py:send_metric():904] Seen metric with glob (shouldn't happen)
+2022-07-04 22:52:28,776 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225226-39dyvroy/files/diff.patch
+2022-07-04 22:52:28,776 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225226-39dyvroy/files/code/run.py
+2022-07-04 22:52:28,776 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225226-39dyvroy/files/output.log
+2022-07-04 22:52:28,777 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225226-39dyvroy/files/requirements.txt
+2022-07-04 22:52:28,777 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-metadata.json
+2022-07-04 22:52:28,777 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225226-39dyvroy/files/code
+2022-07-04 22:52:28,945 INFO    Thread-16 :2895 [upload_job.py:push():137] Uploaded file /tmp/tmpl9b5z1qzwandb/1gcbg3c7-wandb-metadata.json
+2022-07-04 22:52:29,146 INFO    Thread-18 :2895 [upload_job.py:push():137] Uploaded file /tmp/tmpl9b5z1qzwandb/1qzhhmn3-diff.patch
+2022-07-04 22:52:30,323 INFO    Thread-17 :2895 [upload_job.py:push():137] Uploaded file /tmp/tmpl9b5z1qzwandb/37wt9pig-code/run.py
+2022-07-04 22:52:30,775 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225226-39dyvroy/files/output.log
+2022-07-04 22:52:31,732 DEBUG   SenderThread:2895 [sender.py:send():232] send: config
+2022-07-04 22:52:31,743 DEBUG   SenderThread:2895 [sender.py:send():232] send: telemetry
+2022-07-04 22:52:32,621 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:32,621 DEBUG   SenderThread:2895 [sender.py:send():232] send: exit
+2022-07-04 22:52:32,621 INFO    SenderThread:2895 [sender.py:send_exit():368] handling exit code: 1
+2022-07-04 22:52:32,621 INFO    SenderThread:2895 [sender.py:send_exit():370] handling runtime: 5
+2022-07-04 22:52:32,622 INFO    SenderThread:2895 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:52:32,622 INFO    SenderThread:2895 [sender.py:send_exit():376] send defer
+2022-07-04 22:52:32,622 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:32,623 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:32,623 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 0
+2022-07-04 22:52:32,623 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:32,623 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 0
+2022-07-04 22:52:32,623 INFO    SenderThread:2895 [sender.py:transition_state():389] send defer: 1
+2022-07-04 22:52:32,623 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:32,624 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 1
+2022-07-04 22:52:32,663 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:32,663 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 1
+2022-07-04 22:52:32,663 INFO    SenderThread:2895 [sender.py:transition_state():389] send defer: 2
+2022-07-04 22:52:32,663 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:32,663 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 2
+2022-07-04 22:52:32,664 DEBUG   SenderThread:2895 [sender.py:send():232] send: stats
+2022-07-04 22:52:32,664 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:32,664 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 2
+2022-07-04 22:52:32,664 INFO    SenderThread:2895 [sender.py:transition_state():389] send defer: 3
+2022-07-04 22:52:32,664 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:32,664 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 3
+2022-07-04 22:52:32,665 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:32,665 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 3
+2022-07-04 22:52:32,665 INFO    SenderThread:2895 [sender.py:transition_state():389] send defer: 4
+2022-07-04 22:52:32,665 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:32,665 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 4
+2022-07-04 22:52:32,665 DEBUG   SenderThread:2895 [sender.py:send():232] send: summary
+2022-07-04 22:52:32,666 INFO    SenderThread:2895 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:52:32,666 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:32,666 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 4
+2022-07-04 22:52:32,666 INFO    SenderThread:2895 [sender.py:transition_state():389] send defer: 5
+2022-07-04 22:52:32,666 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:32,666 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 5
+2022-07-04 22:52:32,666 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:32,666 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 5
+2022-07-04 22:52:32,724 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:32,781 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225226-39dyvroy/files/output.log
+2022-07-04 22:52:32,782 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-summary.json
+2022-07-04 22:52:34,331 INFO    SenderThread:2895 [sender.py:transition_state():389] send defer: 6
+2022-07-04 22:52:34,332 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:34,332 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:34,333 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 6
+2022-07-04 22:52:34,333 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:34,333 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 6
+2022-07-04 22:52:34,333 INFO    SenderThread:2895 [dir_watcher.py:finish():279] shutting down directory watcher
+2022-07-04 22:52:34,433 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:34,782 INFO    Thread-12 :2895 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225226-39dyvroy/files/output.log
+2022-07-04 22:52:34,783 INFO    SenderThread:2895 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225226-39dyvroy/files/config.yaml
+2022-07-04 22:52:34,783 INFO    SenderThread:2895 [dir_watcher.py:finish():309] scan: Logs/wandb/run-20220704_225226-39dyvroy/files
+2022-07-04 22:52:34,783 INFO    SenderThread:2895 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225226-39dyvroy/files/output.log output.log
+2022-07-04 22:52:34,783 INFO    SenderThread:2895 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225226-39dyvroy/files/config.yaml config.yaml
+2022-07-04 22:52:34,783 INFO    SenderThread:2895 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225226-39dyvroy/files/diff.patch diff.patch
+2022-07-04 22:52:34,783 INFO    SenderThread:2895 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-metadata.json wandb-metadata.json
+2022-07-04 22:52:34,784 INFO    SenderThread:2895 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225226-39dyvroy/files/requirements.txt requirements.txt
+2022-07-04 22:52:34,784 INFO    SenderThread:2895 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-summary.json wandb-summary.json
+2022-07-04 22:52:34,784 INFO    SenderThread:2895 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225226-39dyvroy/files/code/run.py code/run.py
+2022-07-04 22:52:34,784 INFO    SenderThread:2895 [sender.py:transition_state():389] send defer: 7
+2022-07-04 22:52:34,784 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:34,795 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:34,795 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 7
+2022-07-04 22:52:34,796 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:34,796 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 7
+2022-07-04 22:52:34,796 INFO    SenderThread:2895 [file_pusher.py:finish():145] shutting down file pusher
+2022-07-04 22:52:34,887 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:34,887 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:34,988 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:34,989 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:35,092 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:35,092 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:35,194 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:35,194 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:35,295 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:35,295 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:35,396 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:35,397 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:35,498 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:35,498 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:35,576 INFO    Thread-20 :2895 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_225226-39dyvroy/files/config.yaml
+2022-07-04 22:52:35,584 INFO    Thread-21 :2895 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_225226-39dyvroy/files/requirements.txt
+2022-07-04 22:52:35,600 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:35,600 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:35,701 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:35,701 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:35,802 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:35,803 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:35,904 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:35,904 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:35,928 INFO    Thread-22 :2895 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_225226-39dyvroy/files/wandb-summary.json
+2022-07-04 22:52:35,990 INFO    Thread-19 :2895 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_225226-39dyvroy/files/output.log
+2022-07-04 22:52:36,005 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:36,005 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:36,107 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:36,107 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:36,191 INFO    Thread-11 :2895 [sender.py:transition_state():389] send defer: 8
+2022-07-04 22:52:36,192 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:36,192 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 8
+2022-07-04 22:52:36,192 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:36,192 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 8
+2022-07-04 22:52:36,208 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:36,533 INFO    SenderThread:2895 [sender.py:transition_state():389] send defer: 9
+2022-07-04 22:52:36,533 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:36,534 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:36,534 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 9
+2022-07-04 22:52:36,534 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:36,534 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 9
+2022-07-04 22:52:36,534 INFO    SenderThread:2895 [sender.py:transition_state():389] send defer: 10
+2022-07-04 22:52:36,535 DEBUG   SenderThread:2895 [sender.py:send():232] send: final
+2022-07-04 22:52:36,535 DEBUG   SenderThread:2895 [sender.py:send():232] send: footer
+2022-07-04 22:52:36,536 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:52:36,536 INFO    HandlerThread:2895 [handler.py:handle_request_defer():164] handle defer: 10
+2022-07-04 22:52:36,536 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:52:36,536 INFO    SenderThread:2895 [sender.py:send_request_defer():385] handle sender defer: 10
+2022-07-04 22:52:36,635 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:52:36,635 DEBUG   SenderThread:2895 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:52:36,635 INFO    SenderThread:2895 [file_pusher.py:join():150] waiting for file pusher
+2022-07-04 22:52:37,315 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: sampled_history
+2022-07-04 22:52:37,316 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: get_summary
+2022-07-04 22:52:37,317 INFO    MainThread:2895 [wandb_run.py:_footer_history_summary_info():3102] rendering history
+2022-07-04 22:52:37,317 INFO    MainThread:2895 [wandb_run.py:_footer_history_summary_info():3134] rendering summary
+2022-07-04 22:52:37,317 INFO    MainThread:2895 [wandb_run.py:_footer_sync_info():3057] logging synced files
+2022-07-04 22:52:37,317 DEBUG   HandlerThread:2895 [handler.py:handle_request():141] handle_request: shutdown
+2022-07-04 22:52:37,317 INFO    HandlerThread:2895 [handler.py:finish():806] shutting down handler
+2022-07-04 22:52:37,536 INFO    WriterThread:2895 [datastore.py:close():279] close: Logs/wandb/run-20220704_225226-39dyvroy/run-39dyvroy.wandb
+2022-07-04 22:52:38,215 INFO    SenderThread:2895 [sender.py:finish():1106] shutting down sender
+2022-07-04 22:52:38,215 INFO    SenderThread:2895 [file_pusher.py:finish():145] shutting down file pusher
+2022-07-04 22:52:38,215 INFO    SenderThread:2895 [file_pusher.py:join():150] waiting for file pusher
+2022-07-04 22:52:38,320 INFO    MainThread:2895 [internal.py:handle_exit():80] Internal process exited
diff --git a/Logs/wandb/run-20220704_225226-39dyvroy/logs/debug.log b/Logs/wandb/run-20220704_225226-39dyvroy/logs/debug.log
new file mode 100644
index 0000000..e3f8f65
--- /dev/null
+++ b/Logs/wandb/run-20220704_225226-39dyvroy/logs/debug.log
@@ -0,0 +1,26 @@
+2022-07-04 22:52:26,142 INFO    MainThread:2860 [wandb_setup.py:_flush():75] Loading settings from /home/sol/.config/wandb/settings
+2022-07-04 22:52:26,142 INFO    MainThread:2860 [wandb_setup.py:_flush():75] Loading settings from /home/sol/git/QML2/wandb/settings
+2022-07-04 22:52:26,142 INFO    MainThread:2860 [wandb_setup.py:_flush():75] Loading settings from environment variables: {'_require_service': 'True'}
+2022-07-04 22:52:26,142 INFO    MainThread:2860 [wandb_setup.py:_flush():75] Inferring run settings from compute environment: {'program_relpath': 'run.py', 'program': '/home/sol/git/QML2/run.py'}
+2022-07-04 22:52:26,142 INFO    MainThread:2860 [wandb_init.py:_log_setup():437] Logging user logs to Logs/wandb/run-20220704_225226-39dyvroy/logs/debug.log
+2022-07-04 22:52:26,142 INFO    MainThread:2860 [wandb_init.py:_log_setup():438] Logging internal logs to Logs/wandb/run-20220704_225226-39dyvroy/logs/debug-internal.log
+2022-07-04 22:52:26,142 INFO    MainThread:2860 [wandb_init.py:init():471] calling init triggers
+2022-07-04 22:52:26,143 INFO    MainThread:2860 [wandb_init.py:init():474] wandb.init called with sweep_config: {}
+config: {}
+2022-07-04 22:52:26,143 INFO    MainThread:2860 [wandb_init.py:init():524] starting backend
+2022-07-04 22:52:26,144 INFO    MainThread:2860 [backend.py:_multiprocessing_setup():97] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2022-07-04 22:52:26,146 INFO    MainThread:2860 [wandb_init.py:init():533] backend started and connected
+2022-07-04 22:52:26,149 INFO    MainThread:2860 [wandb_init.py:init():597] updated telemetry
+2022-07-04 22:52:26,161 INFO    MainThread:2860 [wandb_init.py:init():628] communicating run to backend with 30 second timeout
+2022-07-04 22:52:26,770 INFO    MainThread:2860 [wandb_run.py:_on_init():1923] communicating current version
+2022-07-04 22:52:26,888 INFO    MainThread:2860 [wandb_run.py:_on_init():1927] got version response upgrade_message: "wandb version 0.12.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2022-07-04 22:52:26,888 INFO    MainThread:2860 [wandb_init.py:init():659] starting run threads in backend
+2022-07-04 22:52:28,009 INFO    MainThread:2860 [wandb_run.py:_console_start():1897] atexit reg
+2022-07-04 22:52:28,010 INFO    MainThread:2860 [wandb_run.py:_redirect():1770] redirect: SettingsConsole.WRAP
+2022-07-04 22:52:28,010 INFO    MainThread:2860 [wandb_run.py:_redirect():1807] Wrapping output streams.
+2022-07-04 22:52:28,012 INFO    MainThread:2860 [wandb_run.py:_redirect():1831] Redirects installed.
+2022-07-04 22:52:28,012 INFO    MainThread:2860 [wandb_init.py:init():684] run started, returning control to user process
+2022-07-04 22:52:31,731 INFO    MainThread:2860 [wandb_run.py:_config_callback():1131] config_cb None None {'opt/optimizer': 'Adam', 'opt/lr': 1e-05, 'net/model': 'Simple_QHN', 'net/num_classes': 2, 'net/LSTM/input_size': 116, 'net/LSTM/hidden_size': 128, 'net/LSTM/num_layers': 1, 'net/LSTM/bidirectional': True, 'net/Simple_QHN/n_qubits': 2, 'net/Simple_QHN/shift': 0.6, 'net/Simple_QHN/lstm_hidden': 128, 'net/Simple_QHN/linear_out': 64, 'net/Simple_QHN/backend': 'aer_simulator', 'net/hybrid/n_qubits': 3, 'net/hybrid/simulator': 'aer_simulator', 'net/hybrid/shift': 0.5, 'net/hybrid/shots': 100, 'net/hybrid/dense_type': 2, 'net/roi_rank': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115], 'inputs': 'None'}
+2022-07-04 22:52:31,742 INFO    MainThread:2860 [wandb_watch.py:watch():47] Watching
+2022-07-04 22:52:38,320 WARNING MsgRouterThr:2860 [router.py:message_loop():76] message_loop has been closed
diff --git a/Logs/wandb/run-20220704_225226-39dyvroy/run-39dyvroy.wandb b/Logs/wandb/run-20220704_225226-39dyvroy/run-39dyvroy.wandb
new file mode 100644
index 0000000..479a851
Binary files /dev/null and b/Logs/wandb/run-20220704_225226-39dyvroy/run-39dyvroy.wandb differ
diff --git a/Logs/wandb/run-20220704_225452-38gsv9eg/files/code/run.py b/Logs/wandb/run-20220704_225452-38gsv9eg/files/code/run.py
new file mode 100644
index 0000000..69d3f58
--- /dev/null
+++ b/Logs/wandb/run-20220704_225452-38gsv9eg/files/code/run.py
@@ -0,0 +1,25 @@
+from omegaconf import OmegaConf
+from pathlib import Path
+
+from src.runners import S_Runner
+
+CONFIG_DIR = Path("Configs")
+
+
+def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+    model_params = OmegaConf.load(CONFIG_DIR / "models.yaml")
+    cfg = OmegaConf.merge(cfg, model_params)
+    cfg.merge_with_cli()
+
+    runner = S_Runner(
+        log=cfg.log,
+        optimizer=cfg.optimizer,
+        loader=cfg.loader,
+        network=cfg.network,
+        data=cfg.data,
+    )
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/Logs/wandb/run-20220704_225452-38gsv9eg/files/config.yaml b/Logs/wandb/run-20220704_225452-38gsv9eg/files/config.yaml
new file mode 100644
index 0000000..b2920bb
--- /dev/null
+++ b/Logs/wandb/run-20220704_225452-38gsv9eg/files/config.yaml
@@ -0,0 +1,208 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.12.15
+    code_path: code/run.py
+    framework: lightning
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    m:
+    - 1: trainer/global_step
+      6:
+      - 3
+    python_version: 3.8.10
+    start_time: 1656942892
+    t:
+      1:
+      - 1
+      - 5
+      - 9
+      - 41
+      - 53
+      - 55
+      3:
+      - 1
+      - 7
+      - 23
+      4: 3.8.10
+      5: 0.12.15
+      8:
+      - 5
+inputs:
+  desc: null
+  value: None
+net/LSTM/bidirectional:
+  desc: null
+  value: true
+net/LSTM/hidden_size:
+  desc: null
+  value: 128
+net/LSTM/input_size:
+  desc: null
+  value: 116
+net/LSTM/num_layers:
+  desc: null
+  value: 1
+net/Simple_QHN/backend:
+  desc: null
+  value: aer_simulator
+net/Simple_QHN/linear_out:
+  desc: null
+  value: 64
+net/Simple_QHN/lstm_hidden:
+  desc: null
+  value: 128
+net/Simple_QHN/n_qubits:
+  desc: null
+  value: 2
+net/Simple_QHN/shift:
+  desc: null
+  value: 0.6
+net/hybrid/dense_type:
+  desc: null
+  value: 2
+net/hybrid/n_qubits:
+  desc: null
+  value: 3
+net/hybrid/shift:
+  desc: null
+  value: 0.5
+net/hybrid/shots:
+  desc: null
+  value: 100
+net/hybrid/simulator:
+  desc: null
+  value: aer_simulator
+net/model:
+  desc: null
+  value: Simple_QHN
+net/num_classes:
+  desc: null
+  value: 2
+net/roi_rank:
+  desc: null
+  value:
+  - 0
+  - 1
+  - 2
+  - 3
+  - 4
+  - 5
+  - 6
+  - 7
+  - 8
+  - 9
+  - 10
+  - 11
+  - 12
+  - 13
+  - 14
+  - 15
+  - 16
+  - 17
+  - 18
+  - 19
+  - 20
+  - 21
+  - 22
+  - 23
+  - 24
+  - 25
+  - 26
+  - 27
+  - 28
+  - 29
+  - 30
+  - 31
+  - 32
+  - 33
+  - 34
+  - 35
+  - 36
+  - 37
+  - 38
+  - 39
+  - 40
+  - 41
+  - 42
+  - 43
+  - 44
+  - 45
+  - 46
+  - 47
+  - 48
+  - 49
+  - 50
+  - 51
+  - 52
+  - 53
+  - 54
+  - 55
+  - 56
+  - 57
+  - 58
+  - 59
+  - 60
+  - 61
+  - 62
+  - 63
+  - 64
+  - 65
+  - 66
+  - 67
+  - 68
+  - 69
+  - 70
+  - 71
+  - 72
+  - 73
+  - 74
+  - 75
+  - 76
+  - 77
+  - 78
+  - 79
+  - 80
+  - 81
+  - 82
+  - 83
+  - 84
+  - 85
+  - 86
+  - 87
+  - 88
+  - 89
+  - 90
+  - 91
+  - 92
+  - 93
+  - 94
+  - 95
+  - 96
+  - 97
+  - 98
+  - 99
+  - 100
+  - 101
+  - 102
+  - 103
+  - 104
+  - 105
+  - 106
+  - 107
+  - 108
+  - 109
+  - 110
+  - 111
+  - 112
+  - 113
+  - 114
+  - 115
+opt/lr:
+  desc: null
+  value: 1.0e-05
+opt/optimizer:
+  desc: null
+  value: Adam
diff --git a/Logs/wandb/run-20220704_225452-38gsv9eg/files/diff.patch b/Logs/wandb/run-20220704_225452-38gsv9eg/files/diff.patch
new file mode 100644
index 0000000..c121913
--- /dev/null
+++ b/Logs/wandb/run-20220704_225452-38gsv9eg/files/diff.patch
@@ -0,0 +1,446 @@
+diff --git a/Configs/config.yaml b/Configs/config.yaml
+index 541dd0b..aa00027 100644
+--- a/Configs/config.yaml
++++ b/Configs/config.yaml
+@@ -31,12 +31,7 @@ data:
+ network:
+   model: ${log.model_name}
+   num_classes: 2
+-  encoder: ${SCCNN_bn}
+-  attention: ${Attention}
+   LSTM: ${LSTM}
+-  decoder: ${Decoder}
+-  is_encoder_shared: True
+-  encoderType: SCCNN_bn
+   Simple_QHN: ${Simple_QHN}
+   hybrid: ${quantum_circuit}
+ 
+@@ -45,5 +40,4 @@ optimizer:
+   lr: 1e-5
+ 
+ 
+-runner: OneSiteHoldout_Runner
+ 
+diff --git a/Configs/models.yaml b/Configs/models.yaml
+index 2c5c2ba..e561fb7 100644
+--- a/Configs/models.yaml
++++ b/Configs/models.yaml
+@@ -1,6 +1,6 @@
+ 
+ LSTM:
+-  input_size: ${SCCNN.conv_block4.out_f}
++  input_size: 116
+   hidden_size: 128
+   num_layers: 1
+   bidirectional: True
+@@ -8,9 +8,9 @@ LSTM:
+ Simple_QHN:
+   n_qubits: 2
+   shift: 0.6
+-  is_cnot: True
+   lstm_hidden: 128
+   linear_out: 64
++  backend: aer_simulator
+ 
+ 
+ quantum_circuit:
+@@ -18,5 +18,4 @@ quantum_circuit:
+   simulator: aer_simulator
+   shift: 0.5
+   shots: 100
+-  is_cnot: True
+   dense_type: 2
+\ No newline at end of file
+diff --git a/Logs/QML-MNIST/version000/version_0/hparams.yaml b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+index 3851257..000d975 100644
+--- a/Logs/QML-MNIST/version000/version_0/hparams.yaml
++++ b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+@@ -15,6 +15,5 @@ net:
+     simulator: aer_simulator
+     shift: 0.5
+     shots: 100
+-    is_cnot: true
+     dense_type: 2
+ inputs: null
+diff --git a/run.py b/run.py
+index c8e1ccf..69d3f58 100644
+--- a/run.py
++++ b/run.py
+@@ -1,7 +1,7 @@
+ from omegaconf import OmegaConf
+ from pathlib import Path
+ 
+-from src.runners import S_Runner, MNIST_Runner
++from src.runners import S_Runner
+ 
+ CONFIG_DIR = Path("Configs")
+ 
+@@ -11,7 +11,7 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+     cfg = OmegaConf.merge(cfg, model_params)
+     cfg.merge_with_cli()
+ 
+-    runner = MNIST_Runner(
++    runner = S_Runner(
+         log=cfg.log,
+         optimizer=cfg.optimizer,
+         loader=cfg.loader,
+@@ -22,4 +22,4 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+ 
+ 
+ if __name__ == "__main__":
+-    main(cfg=OmegaConf.load(CONFIG_DIR / "config MNIST.yaml"))
++    main()
+diff --git a/src/data/__pycache__/data_etl.cpython-38.pyc b/src/data/__pycache__/data_etl.cpython-38.pyc
+index 288d14c..934cbd5 100644
+Binary files a/src/data/__pycache__/data_etl.cpython-38.pyc and b/src/data/__pycache__/data_etl.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/loader.cpython-38.pyc b/src/data/__pycache__/loader.cpython-38.pyc
+index eddf358..67504ef 100644
+Binary files a/src/data/__pycache__/loader.cpython-38.pyc and b/src/data/__pycache__/loader.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/sampling.cpython-38.pyc b/src/data/__pycache__/sampling.cpython-38.pyc
+index a2cbaf3..a748fd2 100644
+Binary files a/src/data/__pycache__/sampling.cpython-38.pyc and b/src/data/__pycache__/sampling.cpython-38.pyc differ
+diff --git a/src/data/data_etl.py b/src/data/data_etl.py
+index 08700be..dc691b0 100755
+--- a/src/data/data_etl.py
++++ b/src/data/data_etl.py
+@@ -137,24 +137,6 @@ def chunks(lst, n):
+     return [lst[i : i + div] for i in range(0, len(lst), div)]
+ 
+ 
+-@dataclass
+-class Load_MNIST:
+-    def __init__(self, is_train:Boolean):
+-        if is_train:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=True, download=True,
+-                         transform=transforms.Compose([transforms.ToTensor()]))
+-        else:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=False, download=True,
+-                        transform=transforms.Compose([transforms.ToTensor()]))
+-                        
+-    def get_samples(self, n_samples:int):
+-        idx = []
+-        for i in range(10):
+-            idx.append(np.where(self.dataset.targets == i)[0][:n_samples])#각 target에 해당되는 번호를 저장한다. 
+-        idx = np.array(idx).reshape(-1)
+-        self.dataset.data = self.dataset.data[idx]
+-        self.dataset.targets = self.dataset.targets[idx]
+-        return self.dataset
+ 
+ 
+ 
+diff --git a/src/data/loader.py b/src/data/loader.py
+index 288a3b3..158cbdc 100644
+--- a/src/data/loader.py
++++ b/src/data/loader.py
+@@ -5,25 +5,8 @@ from typing import Union, List
+ import torch
+ from torch.utils.data import Dataset
+ 
+-from src.data import Load, SITES_DICT, Load_MNIST
+-
+-class MNISTDataset(Dataset):
+-    def __init__(self, n_samples: int, is_train: Boolean) -> None:
+-        Load = Load_MNIST(is_train)
+-        if n_samples >0:
+-            self.data = Load.get_samples(n_samples=n_samples).data
+-            self.labels = Load.get_samples(n_samples=n_samples).targets  
+-        else:  
+-            self.data = Load.dataset.data
+-            self.labels = Load.dataset.targets
++from src.data import Load, SITES_DICT
+ 
+-    def __len__(self):
+-        return len(self.labels)
+-    
+-    def __getitem__(self, index: int):
+-        data = self.data[index]
+-        label = self.labels[index]
+-        return data, label 
+   
+ class ROIDataset(Dataset):
+     def __init__( self, site: Union[List, str]) -> None:
+diff --git a/src/datamodules/__pycache__/datamodule.cpython-38.pyc b/src/datamodules/__pycache__/datamodule.cpython-38.pyc
+index 6f06b1a..d4ba074 100644
+Binary files a/src/datamodules/__pycache__/datamodule.cpython-38.pyc and b/src/datamodules/__pycache__/datamodule.cpython-38.pyc differ
+diff --git a/src/datamodules/datamodule.py b/src/datamodules/datamodule.py
+index 4fcab3d..0bfab90 100644
+--- a/src/datamodules/datamodule.py
++++ b/src/datamodules/datamodule.py
+@@ -7,64 +7,23 @@ from torch.utils.data import DataLoader, Subset
+ from pytorch_lightning import LightningDataModule
+ from src.data import collate_fn, SamplerFactory
+ 
++
+ @dataclass
+-class MNISTDataModule(LightningDataModule):
+-    
+-    def __init__(self,data: Dict,loader: Dict,dataset: Dict):
++class DataModule(LightningDataModule):
++    def __init__(self, data: Dict, loader: Dict, dataset: Dict):
+         super().__init__()
+-        self.prepare_data_per_node = True
+         self.data = data
+         self.loader = loader
+         self.dataset = dataset
+-    def setup(self, stage: Optional[str] = None):
+-        if stage in ("fit", None):
+-            self.train_dataset = self.dataset(n_samples = self.loader.samples.train_sample, is_train=True)
+-            self.val_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-
+-        if stage in ("test", None):
+-            self.test_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-    
+-    def train_dataloader(self):
+-        conf = deepcopy(self.loader.train)
+-        batch_size = conf.pop("batch_size")
+-        conf.shuffle = False
+-        # return DataLoader(self.train_dataset, **conf, collate_fn=collate_fn)
+-
+-        return DataLoader(
+-            self.train_dataset,
+-            **conf,
+-            collate_fn=collate_fn,
+-            batch_sampler=SamplerFactory().get(
+-                class_idxs=[
+-                    np.where(self.train_dataset.labels == i)[0].tolist()
+-                    for i in range(10)
+-                ],
+-                batch_size=batch_size,
+-                n_batches=len(self.train_dataset)//batch_size + 5,
+-                alpha=1.0,
+-                kind="random",
+-            ),
+-        )
+-
+-    def val_dataloader(self):
+-        return DataLoader(self.val_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-    def test_dataloader(self):
+-        return DataLoader(self.test_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-@dataclass
+-class DataModule(LightningDataModule):
+-    data: Dict
+-    loader: Dict
+-    dataset: Dict
++        self.prepare_data_per_node = True
+ 
+     def setup(self, stage: Optional[str] = None):
+         if stage in ("fit", None):
+-            self.train_dataset = self.dataset(is_train = True, site = self.data.train_site)
+-            self.val_dataset = self.dataset(is_train = False, site = self.data.train_site)
++            self.train_dataset = self.dataset(site = self.data.train_site)
++            self.val_dataset = self.dataset(site = self.data.train_site)
+ 
+         if stage in ("test", None):
+-            self.test_dataset = self.dataset(is_train = False, site = self.data.train_site)
++            self.test_dataset = self.dataset(site = self.data.train_site)
+ 
+     def train_dataloader(self):
+         conf = deepcopy(self.loader.train)
+diff --git a/src/layers/__pycache__/__init__.cpython-38.pyc b/src/layers/__pycache__/__init__.cpython-38.pyc
+index 31453e2..a73c856 100644
+Binary files a/src/layers/__pycache__/__init__.cpython-38.pyc and b/src/layers/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/layers/__pycache__/quantum_layers.cpython-38.pyc b/src/layers/__pycache__/quantum_layers.cpython-38.pyc
+index 4f219f9..3e30b2e 100644
+Binary files a/src/layers/__pycache__/quantum_layers.cpython-38.pyc and b/src/layers/__pycache__/quantum_layers.cpython-38.pyc differ
+diff --git a/src/layers/quantum_layers.py b/src/layers/quantum_layers.py
+index 976aa6a..4dfccfa 100644
+--- a/src/layers/quantum_layers.py
++++ b/src/layers/quantum_layers.py
+@@ -35,7 +35,7 @@ class QuantumCircuit:
+         for theta, qubit in zip(self.theta, self.all_qubits):
+             self._circuit.ry(theta, qubit)
+ 
+-        for i in [2,3,4,5]:
++        for i in range(2, self.n_qubit):
+             self._circuit.cx(0,i)
+ 
+         self._circuit.measure_all()
+diff --git a/src/models/__pycache__/__init__.cpython-38.pyc b/src/models/__pycache__/__init__.cpython-38.pyc
+index f5a0d8c..9643058 100644
+Binary files a/src/models/__pycache__/__init__.cpython-38.pyc and b/src/models/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/models/__pycache__/qhn.cpython-38.pyc b/src/models/__pycache__/qhn.cpython-38.pyc
+index 56e2a1f..8b4a754 100644
+Binary files a/src/models/__pycache__/qhn.cpython-38.pyc and b/src/models/__pycache__/qhn.cpython-38.pyc differ
+diff --git a/src/models/qhn.py b/src/models/qhn.py
+index c74fec4..2e08fba 100644
+--- a/src/models/qhn.py
++++ b/src/models/qhn.py
+@@ -27,10 +27,10 @@ class Simple_QHN(nn.Module):
+         self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+         self.hybrid = Hybrid(
+             params.n_qubits,
+-            qiskit.Aer.get_backend("aer_simulator"),
++            params.backend,
+             100,
+             shift=params.shift,
+-            is_cnot=params.is_cnot,
++            
+         )
+         self.fc3 = nn.Linear(params.n_qubits * 2, 2)
+         self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+@@ -55,35 +55,3 @@ class Simple_QHN(nn.Module):
+         return x
+ 
+ 
+-class MNIST_QHN(nn.Module):
+-    def __init__(self, params: Optional[Dict] = None, *args, **kwargs) -> None:
+-        super(MNIST_QHN, self).__init__()
+-        params = params.MNIST_QHN
+-        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
+-        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
+-        self.dropout = nn.Dropout2d()
+-        self.fc1 = nn.Linear(256, params.linear_out)
+-        self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+-        self.hybrid = Hybrid(
+-            params.n_qubits,
+-            backend = "aer_simulator",
+-            shots = params.shots,
+-            shift=params.shift,
+-        )
+-        self.fc3 = nn.Linear(2**params.n_qubits, 10)
+-        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+-
+-    def forward(self, x):
+-        x = x.unsqueeze(1)
+-        x = F.relu(self.conv1(x))
+-        x = F.max_pool2d(x, 2)
+-        x = F.relu(self.conv2(x))
+-        x = F.max_pool2d(x, 2)
+-        
+-        x = x.view(x.shape[0],-1)
+-        x = F.relu(self.fc1(x))
+-        x = self.fc2(x)
+-        x = torch.tanh(x) * torch.ones_like(x) * torch.tensor(np.pi / 2)
+-        x = self.hybrid(x).to(self.device)
+-        x = F.softmax(self.fc3(x), dim=1)
+-        return x
+diff --git a/src/runners/__pycache__/base.cpython-38.pyc b/src/runners/__pycache__/base.cpython-38.pyc
+index 6fae27c..b7d08e9 100644
+Binary files a/src/runners/__pycache__/base.cpython-38.pyc and b/src/runners/__pycache__/base.cpython-38.pyc differ
+diff --git a/src/runners/__pycache__/runner.cpython-38.pyc b/src/runners/__pycache__/runner.cpython-38.pyc
+index 9813d76..3792f76 100644
+Binary files a/src/runners/__pycache__/runner.cpython-38.pyc and b/src/runners/__pycache__/runner.cpython-38.pyc differ
+diff --git a/src/runners/runner.py b/src/runners/runner.py
+index 86b61a5..5fb03f3 100644
+--- a/src/runners/runner.py
++++ b/src/runners/runner.py
+@@ -6,8 +6,8 @@ from typing import Optional
+ from copy import deepcopy
+ 
+ from src.runners import Base_Runner
+-from src.data import ROIDataset, SITES_DICT, MNISTDataset
+-from src.datamodules import DataModule, MNISTDataModule
++from src.data import ROIDataset, SITES_DICT
++from src.datamodules import DataModule
+ from src.tasks import ClassificationTask
+ from src.utils import plot_paper
+ from src.callbacks import wandb_callback as wbc
+@@ -177,106 +177,4 @@ class S_Runner(Base_Runner):
+ 
+         return
+ 
+-class MNIST_Runner(Base_Runner):
+-    def get_callbacks(self):
+-        """
+-        Write only callbacks that logger is not necessary
+-        """
+-        checkpoint_callback = ModelCheckpoint(
+-            dirpath=os.path.join(
+-                self.log.checkpoint_path,
+-                self.log.project_name,
+-                f"version{self.version:03d}",
+-            ),
+-            filename=os.path.join(f"model"),
+-            monitor=f"Accuracy/val",
+-            mode="max",
+-            verbose=False,
+-            save_top_k=1,
+-        )
+-
+-        callbacks = dict(
+-            filter(lambda item: item[0].endswith("callback"), vars().items())
+-        ).values()
+-        callbacks = list(callbacks)
+-        return callbacks if len(callbacks) > 0 else None
+-
+-    def run(self, profiler: Optional[str] = None):
+-        os.makedirs(
+-            os.path.join(self.log.checkpoint_path, self.log.project_name), exist_ok=True
+-        )
+-        self.version = len(
+-            os.listdir(os.path.join(self.log.checkpoint_path, self.log.project_name))
+-        )
+-
+-
+-        final_results = list()
+-
+-
+-
+-        dm = self.get_datamodule(dataset=MNISTDataset, datamodule=MNISTDataModule)
+-        model = self.get_network(Task=ClassificationTask)
+-        model.apply(initialize_weights)
+-
+-        trainer = Trainer(
+-            logger=[
+-                TensorBoardLogger(
+-                    save_dir=self.log.log_path,
+-                    name=os.path.join(
+-                        self.log.project_name,
+-                        f"version{self.version:03d}"
+-                    ),
+-                    default_hp_metric=False,
+-                    version=None,
+-                    # log_graph=True, # inavailable due to bug
+-                ),
+-                WandbLogger(
+-                    project=self.log.project_name,
+-                    save_dir=self.log.log_path,
+-                ),
+-            ],
+-            # ! use all gpu
+-            # gpus=-1,
+-            # auto_select_gpus=True,
+-            # ! use 2 gpu
+-            # devices=2,
+-            # accelerator="auto",
+-            # strategy="ddp",
+-            # ! use gpu 0
+-            devices=[0],
+-            accelerator="gpu",
+-            #devices=[self.log.device.gpu],
+-            #accelerator="cpu",
+-            check_val_every_n_epoch=self.log.val_log_freq_epoch,
+-            log_every_n_steps=1,
+-            num_sanity_val_steps=0,
+-            max_epochs=self.log.epoch,
+-            profiler=profiler,
+-            callbacks=[
+-                *self.get_callbacks(),
+-                wbc.WatchModel(),
+-                wbc.LogConfusionMatrix(),
+-                wbc.LogF1PrecRecHeatmap(),
+-                # tbc.WatchModel(),
+-                # tbc.LogConfusionMatrix(),
+-                # tbc.LogF1PrecRecHeatmap(),
+-            ],
+-            precision=self.log.precision,
+-            # gradient_clip_val=0.5,
+-        )
+-        trainer.test_site_prefix = model.prefix
+-        trainer.fit(model, datamodule=dm)
+-        trainer.test(model, datamodule=dm, ckpt_path="best")
+-        final_results.append(
+-            trainer.callback_metrics[f"Accuracy/test"]
+-        )
+-
+-    try:
+-        import wandb
+-
+-        wb_logger = wbc.get_wandb_logger(trainer=Trainer)
+-        
+-    except Exception as e:
+-        print(e)
+-
+     
diff --git a/Logs/wandb/run-20220704_225452-38gsv9eg/files/output.log b/Logs/wandb/run-20220704_225452-38gsv9eg/files/output.log
new file mode 100644
index 0000000..1905dcd
--- /dev/null
+++ b/Logs/wandb/run-20220704_225452-38gsv9eg/files/output.log
@@ -0,0 +1,267 @@
+GPU available: True, used: True
+TPU available: False, using: 0 TPU cores
+IPU available: False, using: 0 IPUs
+HPU available: False, using: 0 HPUs
+Epoch 0:   0%|                                                                                 | 0/14 [00:00<?, ?it/s]
+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
+  | Name        | Type             | Params
+-------------------------------------------------
+0 | model       | Simple_QHN       | 170 K
+1 | get_metrics | MetricCollection | 0
+-------------------------------------------------
+170 K     Trainable params
+0         Non-trainable params
+170 K     Total params
+0.680     Total estimated model params size (MB)
+Missing logger folder: Logs/QML-MNIST/version001/NYU
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
+    results = self._run_stage()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
+    return self._run_train()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
+    self.fit_loop.run()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
+    self._outputs = self.epoch_loop.run(self._data_fetcher)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
+    batch_output = self.batch_loop.run(batch, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
+    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
+    result = self._run_optimization(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
+    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
+    self.trainer._call_lightning_module_hook(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
+    optimizer.step(closure=optimizer_closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
+    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
+    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
+    return optimizer.step(closure=closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
+    return wrapped(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
+    loss = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
+    closure_result = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
+    self._result = self.closure(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
+    step_output = self._step_fn()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
+    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
+    return self.model.training_step(*args, **kwargs)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 61, in training_step
+    loss, preds, targets = self._shared_step(batch, batch_idx)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 40, in _shared_step
+    preds = self.model(x)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/git/QML2/src/models/qhn.py", line 54, in forward
+    x = F.softmax(self.fc3(x), dim=1)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
+    return F.linear(input, self.weight, self.bias)
+RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x8 and 4x2)
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
+    results = self._run_stage()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
+    return self._run_train()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
+    self.fit_loop.run()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
+    self._outputs = self.epoch_loop.run(self._data_fetcher)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
+    batch_output = self.batch_loop.run(batch, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
+    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
+    result = self._run_optimization(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
+    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
+    self.trainer._call_lightning_module_hook(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
+    optimizer.step(closure=optimizer_closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
+    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
+    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
+    return optimizer.step(closure=closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
+    return wrapped(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
+    loss = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
+    closure_result = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
+    self._result = self.closure(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
+    step_output = self._step_fn()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
+    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
+    return self.model.training_step(*args, **kwargs)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 61, in training_step
+    loss, preds, targets = self._shared_step(batch, batch_idx)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 40, in _shared_step
+    preds = self.model(x)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/git/QML2/src/models/qhn.py", line 54, in forward
+    x = F.softmax(self.fc3(x), dim=1)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
+    return F.linear(input, self.weight, self.bias)
+RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x8 and 4x2)
+Traceback (most recent call last):
+  File "/home/sol/git/QML2/run.py", line 25, in <module>
+    main()
+  File "/home/sol/git/QML2/run.py", line 21, in main
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
+    trainer.fit(model, datamodule=dm)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
+    self._call_and_handle_interrupt(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
+    return trainer_fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
+    results = self._run(model, ckpt_path=self.ckpt_path)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
+    results = self._run_stage()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
+    return self._run_train()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
+    self.fit_loop.run()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
+    self._outputs = self.epoch_loop.run(self._data_fetcher)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
+    batch_output = self.batch_loop.run(batch, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
+    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
+    self.advance(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
+    result = self._run_optimization(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
+    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
+    self.trainer._call_lightning_module_hook(
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
+    optimizer.step(closure=optimizer_closure)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
+    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
+    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
+    return optimizer.step(closure=closure, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
+    return wrapped(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
+    return func(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
+    loss = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
+    closure_result = closure()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
+    self._result = self.closure(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
+    step_output = self._step_fn()
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
+    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
+    output = fn(*args, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
+    return self.model.training_step(*args, **kwargs)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 61, in training_step
+    loss, preds, targets = self._shared_step(batch, batch_idx)
+  File "/home/sol/git/QML2/src/tasks/classsification.py", line 40, in _shared_step
+    preds = self.model(x)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/git/QML2/src/models/qhn.py", line 54, in forward
+    x = F.softmax(self.fc3(x), dim=1)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
+    return forward_call(*input, **kwargs)
+  File "/home/sol/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
+    return F.linear(input, self.weight, self.bias)
diff --git a/Logs/wandb/run-20220704_225452-38gsv9eg/files/requirements.txt b/Logs/wandb/run-20220704_225452-38gsv9eg/files/requirements.txt
new file mode 100644
index 0000000..f9f8632
--- /dev/null
+++ b/Logs/wandb/run-20220704_225452-38gsv9eg/files/requirements.txt
@@ -0,0 +1,166 @@
+absl-py==1.0.0
+aiohttp==3.8.1
+aiosignal==1.2.0
+antlr4-python3-runtime==4.8
+async-timeout==4.0.2
+attrs==19.3.0
+automat==0.8.0
+autopep8==1.6.0
+blinker==1.4
+cachetools==5.0.0
+certifi==2019.11.28
+chardet==3.0.4
+charset-normalizer==2.0.12
+click==7.0
+cloud-init==22.1
+colorama==0.4.3
+command-not-found==0.3
+configobj==5.0.6
+constantly==15.1.0
+cryptography==2.8
+cssselect==1.1.0
+cycler==0.11.0
+dbus-python==1.2.16
+dill==0.3.4
+distro-info==0.23ubuntu1
+distro==1.4.0
+docker-pycreds==0.4.0
+entrypoints==0.3
+filelock==3.6.0
+fonttools==4.33.3
+frozenlist==1.3.0
+fsspec==2022.3.0
+gistory==0.44
+gitdb==4.0.9
+gitpython==3.1.27
+google-auth-oauthlib==0.4.6
+google-auth==2.6.6
+grpcio==1.44.0
+httplib2==0.14.0
+hyperlink==19.0.0
+idna==2.8
+importlib-metadata==4.11.3
+incremental==16.10.1
+itemadapter==0.5.0
+itemloaders==1.0.4
+jinja2==2.10.1
+jmespath==1.0.0
+joblib==1.1.0
+jsonpatch==1.22
+jsonpointer==2.0
+jsonschema==3.2.0
+keyring==18.0.1
+kiwisolver==1.4.2
+language-selector==0.1
+launchpadlib==1.10.13
+lazr.restfulclient==0.14.2
+lazr.uri==1.0.3
+lxml==4.8.0
+markdown==3.3.6
+markupsafe==1.1.0
+matplotlib==3.5.1
+more-itertools==4.2.0
+mpmath==1.2.1
+multidict==6.0.2
+netifaces==0.10.4
+nibabel==3.2.2
+nilearn==0.9.1
+ntlm-auth==1.5.0
+numpy==1.22.3
+oauthlib==3.1.0
+omegaconf==2.1.2
+packaging==21.3
+pandas==1.4.2
+parsel==1.6.0
+pathtools==0.1.2
+pbr==5.8.1
+pexpect==4.6.0
+pillow==9.1.0
+pip==20.0.2
+ply==3.11
+promise==2.3
+protego==0.2.1
+protobuf==3.20.1
+psutil==5.9.0
+pyasn1-modules==0.2.1
+pyasn1==0.4.2
+pycodestyle==2.8.0
+pydeprecate==0.3.2
+pydispatcher==2.0.5
+pygame==2.1.2
+pygobject==3.36.0
+pyhamcrest==1.9.0
+pyjwt==1.7.1
+pymacaroons==0.13.0
+pynacl==1.3.0
+pyopenssl==19.0.0
+pyparsing==3.0.8
+pyrsistent==0.15.5
+pyserial==3.4
+python-apt==2.0.0+ubuntu0.20.4.7
+python-constraint==1.4.0
+python-dateutil==2.8.2
+python-debian==0.1.36ubuntu1
+pytorch-lightning==1.6.2
+pytz==2022.1
+pyyaml==6.0
+qiskit-aer==0.10.4
+qiskit-ibmq-provider==0.19.1
+qiskit-ignis==0.7.0
+qiskit-terra==0.20.1
+qiskit==0.36.1
+queuelib==1.6.2
+requests-file==1.5.1
+requests-ntlm==1.1.0
+requests-oauthlib==1.3.1
+requests-unixsocket==0.2.0
+requests==2.22.0
+retworkx==0.11.0
+rsa==4.8
+scikit-learn==1.0.2
+scipy==1.8.0
+seaborn==0.11.2
+secretstorage==2.3.1
+sentry-sdk==1.5.10
+service-identity==18.1.0
+setproctitle==1.2.3
+setuptools==45.2.0
+shortuuid==1.0.8
+simplejson==3.16.0
+six==1.14.0
+sklearn==0.0
+smmap==5.0.0
+sos==4.3
+ssh-import-id==5.10
+stevedore==3.5.0
+symengine==0.9.2
+sympy==1.10.1
+systemd-python==234
+tensorboard-data-server==0.6.1
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.8.0
+threadpoolctl==3.1.0
+tldextract==3.2.1
+toml==0.10.2
+torch-tb-profiler==0.4.0
+torch==1.12.0
+torchmetrics==0.8.1
+torchvision==0.13.0
+tqdm==4.64.0
+tweedledum==1.1.1
+twisted==18.9.0
+typing-extensions==4.2.0
+ubuntu-advantage-tools==27.7
+ufw==0.36
+unattended-upgrades==0.1
+urllib3==1.25.8
+w3lib==1.22.0
+wadllib==1.3.3
+wandb==0.12.15
+websocket-client==1.3.2
+websockets==10.3
+werkzeug==2.1.2
+wheel==0.34.2
+yarl==1.7.2
+zipp==1.0.0
+zope.interface==4.7.1
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-metadata.json b/Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-metadata.json
new file mode 100644
index 0000000..becf77a
--- /dev/null
+++ b/Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-metadata.json
@@ -0,0 +1,24 @@
+{
+    "os": "Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2022-07-04T13:54:55.093249",
+    "startedAt": "2022-07-04T13:54:52.595188",
+    "docker": null,
+    "gpu": "NVIDIA GeForce GTX 1650",
+    "gpu_count": 1,
+    "cpu_count": 8,
+    "cuda": null,
+    "args": [],
+    "state": "running",
+    "program": "/home/sol/git/QML2/run.py",
+    "codePath": "run.py",
+    "git": {
+        "remote": "https://github.com/PVNkT/QML2.git",
+        "commit": "1f3b04ad3f84cb17a3713da979f0adf1e2028c86"
+    },
+    "email": "3031902@gmail.com",
+    "root": "/home/sol/git/QML2",
+    "host": "DESKTOP-HFMS9NT",
+    "username": "sol",
+    "executable": "/bin/python3"
+}
diff --git a/Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-summary.json b/Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-summary.json
new file mode 100644
index 0000000..74ae82c
--- /dev/null
+++ b/Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 6}}
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_225452-38gsv9eg/logs/debug-internal.log b/Logs/wandb/run-20220704_225452-38gsv9eg/logs/debug-internal.log
new file mode 100644
index 0000000..2ce7e62
--- /dev/null
+++ b/Logs/wandb/run-20220704_225452-38gsv9eg/logs/debug-internal.log
@@ -0,0 +1,195 @@
+2022-07-04 22:54:52,605 INFO    StreamThr :3215 [internal.py:wandb_internal():90] W&B internal server running at pid: 3215, started at: 2022-07-04 22:54:52.604760
+2022-07-04 22:54:52,610 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: status
+2022-07-04 22:54:52,610 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: status
+2022-07-04 22:54:52,613 INFO    WriterThread:3215 [datastore.py:open_for_write():75] open: Logs/wandb/run-20220704_225452-38gsv9eg/run-38gsv9eg.wandb
+2022-07-04 22:54:52,613 DEBUG   SenderThread:3215 [sender.py:send():232] send: header
+2022-07-04 22:54:52,616 DEBUG   SenderThread:3215 [sender.py:send():232] send: run
+2022-07-04 22:54:52,618 INFO    SenderThread:3215 [sender.py:_maybe_setup_resume():489] checking resume status for None/QML-MNIST/38gsv9eg
+2022-07-04 22:54:53,679 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: check_version
+2022-07-04 22:54:53,681 INFO    SenderThread:3215 [dir_watcher.py:__init__():166] watching files in: Logs/wandb/run-20220704_225452-38gsv9eg/files
+2022-07-04 22:54:53,681 INFO    SenderThread:3215 [sender.py:_start_run_threads():811] run started: 38gsv9eg with start time 1656942892
+2022-07-04 22:54:53,681 DEBUG   SenderThread:3215 [sender.py:send():232] send: summary
+2022-07-04 22:54:53,682 INFO    SenderThread:3215 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:54:53,682 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: check_version
+2022-07-04 22:54:53,828 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: run_start
+2022-07-04 22:54:54,683 INFO    Thread-12 :3215 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-summary.json
+2022-07-04 22:54:55,093 DEBUG   HandlerThread:3215 [meta.py:__init__():35] meta init
+2022-07-04 22:54:55,093 DEBUG   HandlerThread:3215 [meta.py:__init__():49] meta init done
+2022-07-04 22:54:55,093 DEBUG   HandlerThread:3215 [meta.py:probe():209] probe
+2022-07-04 22:54:55,102 DEBUG   HandlerThread:3215 [meta.py:_setup_git():199] setup git
+2022-07-04 22:54:55,110 DEBUG   HandlerThread:3215 [meta.py:_setup_git():206] setup git done
+2022-07-04 22:54:55,110 DEBUG   HandlerThread:3215 [meta.py:_save_code():87] save code
+2022-07-04 22:54:55,115 DEBUG   HandlerThread:3215 [meta.py:_save_code():108] save code done
+2022-07-04 22:54:55,116 DEBUG   HandlerThread:3215 [meta.py:_save_patches():125] save patches
+2022-07-04 22:54:55,147 DEBUG   HandlerThread:3215 [meta.py:_save_patches():167] save patches done
+2022-07-04 22:54:55,147 DEBUG   HandlerThread:3215 [meta.py:_save_pip():53] save pip
+2022-07-04 22:54:55,147 DEBUG   HandlerThread:3215 [meta.py:_save_pip():67] save pip done
+2022-07-04 22:54:55,147 DEBUG   HandlerThread:3215 [meta.py:probe():247] probe done
+2022-07-04 22:54:55,199 DEBUG   SenderThread:3215 [sender.py:send():232] send: files
+2022-07-04 22:54:55,200 INFO    SenderThread:3215 [sender.py:_save_file():946] saving file wandb-metadata.json with policy now
+2022-07-04 22:54:55,200 INFO    SenderThread:3215 [sender.py:_save_file():946] saving file code/run.py with policy now
+2022-07-04 22:54:55,200 INFO    SenderThread:3215 [sender.py:_save_file():946] saving file diff.patch with policy now
+2022-07-04 22:54:55,207 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: stop_status
+2022-07-04 22:54:55,207 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: stop_status
+2022-07-04 22:54:55,584 DEBUG   SenderThread:3215 [sender.py:send():232] send: telemetry
+2022-07-04 22:54:55,585 DEBUG   SenderThread:3215 [sender.py:send():232] send: metric
+2022-07-04 22:54:55,585 DEBUG   SenderThread:3215 [sender.py:send():232] send: telemetry
+2022-07-04 22:54:55,585 DEBUG   SenderThread:3215 [sender.py:send():232] send: metric
+2022-07-04 22:54:55,585 WARNING SenderThread:3215 [sender.py:send_metric():904] Seen metric with glob (shouldn't happen)
+2022-07-04 22:54:55,683 INFO    Thread-12 :3215 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225452-38gsv9eg/files/output.log
+2022-07-04 22:54:55,684 INFO    Thread-12 :3215 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225452-38gsv9eg/files/requirements.txt
+2022-07-04 22:54:55,684 INFO    Thread-12 :3215 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-metadata.json
+2022-07-04 22:54:55,684 INFO    Thread-12 :3215 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225452-38gsv9eg/files/diff.patch
+2022-07-04 22:54:55,684 INFO    Thread-12 :3215 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225452-38gsv9eg/files/code/run.py
+2022-07-04 22:54:55,684 INFO    Thread-12 :3215 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225452-38gsv9eg/files/code
+2022-07-04 22:54:56,379 INFO    Thread-18 :3215 [upload_job.py:push():137] Uploaded file /tmp/tmpagtotekcwandb/ixjzzo5y-diff.patch
+2022-07-04 22:54:56,558 INFO    Thread-16 :3215 [upload_job.py:push():137] Uploaded file /tmp/tmpagtotekcwandb/3dh7gp7o-wandb-metadata.json
+2022-07-04 22:54:56,574 INFO    Thread-17 :3215 [upload_job.py:push():137] Uploaded file /tmp/tmpagtotekcwandb/7kgfdsin-code/run.py
+2022-07-04 22:54:57,684 INFO    Thread-12 :3215 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225452-38gsv9eg/files/output.log
+2022-07-04 22:54:58,971 DEBUG   SenderThread:3215 [sender.py:send():232] send: config
+2022-07-04 22:54:58,981 DEBUG   SenderThread:3215 [sender.py:send():232] send: telemetry
+2022-07-04 22:54:59,873 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:54:59,874 DEBUG   SenderThread:3215 [sender.py:send():232] send: exit
+2022-07-04 22:54:59,874 INFO    SenderThread:3215 [sender.py:send_exit():368] handling exit code: 1
+2022-07-04 22:54:59,874 INFO    SenderThread:3215 [sender.py:send_exit():370] handling runtime: 6
+2022-07-04 22:54:59,874 INFO    SenderThread:3215 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:54:59,875 INFO    SenderThread:3215 [sender.py:send_exit():376] send defer
+2022-07-04 22:54:59,875 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:54:59,877 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:54:59,877 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 0
+2022-07-04 22:54:59,878 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:54:59,878 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 0
+2022-07-04 22:54:59,878 INFO    SenderThread:3215 [sender.py:transition_state():389] send defer: 1
+2022-07-04 22:54:59,879 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:54:59,879 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 1
+2022-07-04 22:54:59,959 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:54:59,959 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 1
+2022-07-04 22:54:59,960 INFO    SenderThread:3215 [sender.py:transition_state():389] send defer: 2
+2022-07-04 22:54:59,960 DEBUG   SenderThread:3215 [sender.py:send():232] send: stats
+2022-07-04 22:54:59,960 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:54:59,960 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 2
+2022-07-04 22:54:59,960 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:54:59,960 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 2
+2022-07-04 22:54:59,960 INFO    SenderThread:3215 [sender.py:transition_state():389] send defer: 3
+2022-07-04 22:54:59,961 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:54:59,961 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 3
+2022-07-04 22:54:59,961 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:54:59,961 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 3
+2022-07-04 22:54:59,961 INFO    SenderThread:3215 [sender.py:transition_state():389] send defer: 4
+2022-07-04 22:54:59,961 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:54:59,961 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 4
+2022-07-04 22:54:59,961 DEBUG   SenderThread:3215 [sender.py:send():232] send: summary
+2022-07-04 22:54:59,962 INFO    SenderThread:3215 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:54:59,962 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:54:59,962 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 4
+2022-07-04 22:54:59,962 INFO    SenderThread:3215 [sender.py:transition_state():389] send defer: 5
+2022-07-04 22:54:59,962 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:54:59,962 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 5
+2022-07-04 22:54:59,962 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:54:59,962 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 5
+2022-07-04 22:54:59,978 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:00,566 INFO    SenderThread:3215 [sender.py:transition_state():389] send defer: 6
+2022-07-04 22:55:00,566 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:00,567 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:55:00,567 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 6
+2022-07-04 22:55:00,567 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:55:00,567 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 6
+2022-07-04 22:55:00,567 INFO    SenderThread:3215 [dir_watcher.py:finish():279] shutting down directory watcher
+2022-07-04 22:55:00,668 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:00,686 INFO    SenderThread:3215 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-summary.json
+2022-07-04 22:55:00,686 INFO    SenderThread:3215 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225452-38gsv9eg/files/config.yaml
+2022-07-04 22:55:00,686 INFO    SenderThread:3215 [dir_watcher.py:finish():309] scan: Logs/wandb/run-20220704_225452-38gsv9eg/files
+2022-07-04 22:55:00,686 INFO    SenderThread:3215 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225452-38gsv9eg/files/output.log output.log
+2022-07-04 22:55:00,687 INFO    SenderThread:3215 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225452-38gsv9eg/files/config.yaml config.yaml
+2022-07-04 22:55:00,687 INFO    SenderThread:3215 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225452-38gsv9eg/files/diff.patch diff.patch
+2022-07-04 22:55:00,687 INFO    SenderThread:3215 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-metadata.json wandb-metadata.json
+2022-07-04 22:55:00,687 INFO    SenderThread:3215 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225452-38gsv9eg/files/requirements.txt requirements.txt
+2022-07-04 22:55:00,691 INFO    SenderThread:3215 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-summary.json wandb-summary.json
+2022-07-04 22:55:00,693 INFO    SenderThread:3215 [dir_watcher.py:finish():323] scan save: Logs/wandb/run-20220704_225452-38gsv9eg/files/code/run.py code/run.py
+2022-07-04 22:55:00,693 INFO    SenderThread:3215 [sender.py:transition_state():389] send defer: 7
+2022-07-04 22:55:00,695 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:00,698 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:55:00,698 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 7
+2022-07-04 22:55:00,699 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:55:00,699 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 7
+2022-07-04 22:55:00,699 INFO    SenderThread:3215 [file_pusher.py:finish():145] shutting down file pusher
+2022-07-04 22:55:00,800 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:00,801 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:00,902 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:00,902 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:01,003 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:01,003 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:01,105 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:01,106 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:01,207 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:01,207 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:01,309 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:01,309 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:01,410 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:01,411 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:01,512 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:01,512 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:01,613 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:01,614 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:01,715 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:01,715 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:01,729 INFO    Thread-19 :3215 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_225452-38gsv9eg/files/output.log
+2022-07-04 22:55:01,792 INFO    Thread-21 :3215 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_225452-38gsv9eg/files/requirements.txt
+2022-07-04 22:55:01,816 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:01,816 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:01,898 INFO    Thread-22 :3215 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_225452-38gsv9eg/files/wandb-summary.json
+2022-07-04 22:55:01,918 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:01,918 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:02,020 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:02,020 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:02,121 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:02,121 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:02,222 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:02,222 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:02,323 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:02,324 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:02,425 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:02,425 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:02,527 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:02,527 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:02,628 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:02,628 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:02,637 INFO    Thread-20 :3215 [upload_job.py:push():137] Uploaded file Logs/wandb/run-20220704_225452-38gsv9eg/files/config.yaml
+2022-07-04 22:55:02,729 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:02,730 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:02,831 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:02,831 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:02,837 INFO    Thread-11 :3215 [sender.py:transition_state():389] send defer: 8
+2022-07-04 22:55:02,838 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:55:02,838 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 8
+2022-07-04 22:55:02,838 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:55:02,838 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 8
+2022-07-04 22:55:02,932 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:03,150 INFO    SenderThread:3215 [sender.py:transition_state():389] send defer: 9
+2022-07-04 22:55:03,150 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:03,152 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:55:03,152 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 9
+2022-07-04 22:55:03,152 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:55:03,152 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 9
+2022-07-04 22:55:03,152 INFO    SenderThread:3215 [sender.py:transition_state():389] send defer: 10
+2022-07-04 22:55:03,153 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: defer
+2022-07-04 22:55:03,153 DEBUG   SenderThread:3215 [sender.py:send():232] send: final
+2022-07-04 22:55:03,153 INFO    HandlerThread:3215 [handler.py:handle_request_defer():164] handle defer: 10
+2022-07-04 22:55:03,153 DEBUG   SenderThread:3215 [sender.py:send():232] send: footer
+2022-07-04 22:55:03,153 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: defer
+2022-07-04 22:55:03,153 INFO    SenderThread:3215 [sender.py:send_request_defer():385] handle sender defer: 10
+2022-07-04 22:55:03,252 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: poll_exit
+2022-07-04 22:55:03,252 DEBUG   SenderThread:3215 [sender.py:send_request():246] send_request: poll_exit
+2022-07-04 22:55:03,253 INFO    SenderThread:3215 [file_pusher.py:join():150] waiting for file pusher
+2022-07-04 22:55:04,545 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: sampled_history
+2022-07-04 22:55:04,546 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: get_summary
+2022-07-04 22:55:04,546 INFO    MainThread:3215 [wandb_run.py:_footer_history_summary_info():3102] rendering history
+2022-07-04 22:55:04,546 INFO    MainThread:3215 [wandb_run.py:_footer_history_summary_info():3134] rendering summary
+2022-07-04 22:55:04,546 INFO    MainThread:3215 [wandb_run.py:_footer_sync_info():3057] logging synced files
+2022-07-04 22:55:04,547 DEBUG   HandlerThread:3215 [handler.py:handle_request():141] handle_request: shutdown
+2022-07-04 22:55:04,547 INFO    HandlerThread:3215 [handler.py:finish():806] shutting down handler
+2022-07-04 22:55:05,153 INFO    WriterThread:3215 [datastore.py:close():279] close: Logs/wandb/run-20220704_225452-38gsv9eg/run-38gsv9eg.wandb
+2022-07-04 22:55:05,444 INFO    SenderThread:3215 [sender.py:finish():1106] shutting down sender
+2022-07-04 22:55:05,444 INFO    SenderThread:3215 [file_pusher.py:finish():145] shutting down file pusher
+2022-07-04 22:55:05,444 INFO    SenderThread:3215 [file_pusher.py:join():150] waiting for file pusher
+2022-07-04 22:55:05,549 INFO    MainThread:3215 [internal.py:handle_exit():80] Internal process exited
diff --git a/Logs/wandb/run-20220704_225452-38gsv9eg/logs/debug.log b/Logs/wandb/run-20220704_225452-38gsv9eg/logs/debug.log
new file mode 100644
index 0000000..e46dadc
--- /dev/null
+++ b/Logs/wandb/run-20220704_225452-38gsv9eg/logs/debug.log
@@ -0,0 +1,26 @@
+2022-07-04 22:54:52,597 INFO    MainThread:3180 [wandb_setup.py:_flush():75] Loading settings from /home/sol/.config/wandb/settings
+2022-07-04 22:54:52,597 INFO    MainThread:3180 [wandb_setup.py:_flush():75] Loading settings from /home/sol/git/QML2/wandb/settings
+2022-07-04 22:54:52,597 INFO    MainThread:3180 [wandb_setup.py:_flush():75] Loading settings from environment variables: {'_require_service': 'True'}
+2022-07-04 22:54:52,597 INFO    MainThread:3180 [wandb_setup.py:_flush():75] Inferring run settings from compute environment: {'program_relpath': 'run.py', 'program': '/home/sol/git/QML2/run.py'}
+2022-07-04 22:54:52,597 INFO    MainThread:3180 [wandb_init.py:_log_setup():437] Logging user logs to Logs/wandb/run-20220704_225452-38gsv9eg/logs/debug.log
+2022-07-04 22:54:52,597 INFO    MainThread:3180 [wandb_init.py:_log_setup():438] Logging internal logs to Logs/wandb/run-20220704_225452-38gsv9eg/logs/debug-internal.log
+2022-07-04 22:54:52,597 INFO    MainThread:3180 [wandb_init.py:init():471] calling init triggers
+2022-07-04 22:54:52,597 INFO    MainThread:3180 [wandb_init.py:init():474] wandb.init called with sweep_config: {}
+config: {}
+2022-07-04 22:54:52,597 INFO    MainThread:3180 [wandb_init.py:init():524] starting backend
+2022-07-04 22:54:52,599 INFO    MainThread:3180 [backend.py:_multiprocessing_setup():97] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2022-07-04 22:54:52,600 INFO    MainThread:3180 [wandb_init.py:init():533] backend started and connected
+2022-07-04 22:54:52,604 INFO    MainThread:3180 [wandb_init.py:init():597] updated telemetry
+2022-07-04 22:54:52,615 INFO    MainThread:3180 [wandb_init.py:init():628] communicating run to backend with 30 second timeout
+2022-07-04 22:54:53,676 INFO    MainThread:3180 [wandb_run.py:_on_init():1923] communicating current version
+2022-07-04 22:54:53,820 INFO    MainThread:3180 [wandb_run.py:_on_init():1927] got version response upgrade_message: "wandb version 0.12.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2022-07-04 22:54:53,820 INFO    MainThread:3180 [wandb_init.py:init():659] starting run threads in backend
+2022-07-04 22:54:55,204 INFO    MainThread:3180 [wandb_run.py:_console_start():1897] atexit reg
+2022-07-04 22:54:55,205 INFO    MainThread:3180 [wandb_run.py:_redirect():1770] redirect: SettingsConsole.WRAP
+2022-07-04 22:54:55,205 INFO    MainThread:3180 [wandb_run.py:_redirect():1807] Wrapping output streams.
+2022-07-04 22:54:55,206 INFO    MainThread:3180 [wandb_run.py:_redirect():1831] Redirects installed.
+2022-07-04 22:54:55,207 INFO    MainThread:3180 [wandb_init.py:init():684] run started, returning control to user process
+2022-07-04 22:54:58,970 INFO    MainThread:3180 [wandb_run.py:_config_callback():1131] config_cb None None {'opt/optimizer': 'Adam', 'opt/lr': 1e-05, 'net/model': 'Simple_QHN', 'net/num_classes': 2, 'net/LSTM/input_size': 116, 'net/LSTM/hidden_size': 128, 'net/LSTM/num_layers': 1, 'net/LSTM/bidirectional': True, 'net/Simple_QHN/n_qubits': 2, 'net/Simple_QHN/shift': 0.6, 'net/Simple_QHN/lstm_hidden': 128, 'net/Simple_QHN/linear_out': 64, 'net/Simple_QHN/backend': 'aer_simulator', 'net/hybrid/n_qubits': 3, 'net/hybrid/simulator': 'aer_simulator', 'net/hybrid/shift': 0.5, 'net/hybrid/shots': 100, 'net/hybrid/dense_type': 2, 'net/roi_rank': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115], 'inputs': 'None'}
+2022-07-04 22:54:58,980 INFO    MainThread:3180 [wandb_watch.py:watch():47] Watching
+2022-07-04 22:55:05,549 WARNING MsgRouterThr:3180 [router.py:message_loop():76] message_loop has been closed
diff --git a/Logs/wandb/run-20220704_225452-38gsv9eg/run-38gsv9eg.wandb b/Logs/wandb/run-20220704_225452-38gsv9eg/run-38gsv9eg.wandb
new file mode 100644
index 0000000..3827539
Binary files /dev/null and b/Logs/wandb/run-20220704_225452-38gsv9eg/run-38gsv9eg.wandb differ
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/code/run.py b/Logs/wandb/run-20220704_225542-2b98ixbg/files/code/run.py
new file mode 100644
index 0000000..69d3f58
--- /dev/null
+++ b/Logs/wandb/run-20220704_225542-2b98ixbg/files/code/run.py
@@ -0,0 +1,25 @@
+from omegaconf import OmegaConf
+from pathlib import Path
+
+from src.runners import S_Runner
+
+CONFIG_DIR = Path("Configs")
+
+
+def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+    model_params = OmegaConf.load(CONFIG_DIR / "models.yaml")
+    cfg = OmegaConf.merge(cfg, model_params)
+    cfg.merge_with_cli()
+
+    runner = S_Runner(
+        log=cfg.log,
+        optimizer=cfg.optimizer,
+        loader=cfg.loader,
+        network=cfg.network,
+        data=cfg.data,
+    )
+    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/config.yaml b/Logs/wandb/run-20220704_225542-2b98ixbg/files/config.yaml
new file mode 100644
index 0000000..5299c14
--- /dev/null
+++ b/Logs/wandb/run-20220704_225542-2b98ixbg/files/config.yaml
@@ -0,0 +1,496 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.12.15
+    code_path: code/run.py
+    framework: lightning
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    m:
+    - 1: trainer/global_step
+      6:
+      - 3
+    - 1: gradients/model\.fc3\.bias._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc3\.bias.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc3\.bias.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc3\.weight._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc3\.weight.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc3\.weight.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc2\.bias._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc2\.bias.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc2\.bias.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc2\.weight._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc2\.weight.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc2\.weight.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc1\.bias._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc1\.bias.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc1\.bias.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc1\.weight._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc1\.weight.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.fc1\.weight.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_ih_l0._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_ih_l0.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_ih_l0.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_hh_l0._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_hh_l0.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_hh_l0.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_hh_l0_reverse._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_hh_l0_reverse.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_hh_l0_reverse.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_hh_l0_reverse._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_hh_l0_reverse.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_hh_l0_reverse.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_ih_l0_reverse._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_ih_l0_reverse.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.weight_ih_l0_reverse.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_ih_l0._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_ih_l0.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_ih_l0.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_ih_l0_reverse._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_ih_l0_reverse.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_ih_l0_reverse.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_hh_l0._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_hh_l0.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.lstm\.bias_hh_l0.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv2\.weight._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv2\.weight.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv2\.weight.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv2\.bias._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv2\.bias.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv2\.bias.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv1\.weight._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv1\.weight.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv1\.weight.bins
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv1\.bias._type
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv1\.bias.values
+      5: 1
+      6:
+      - 1
+    - 1: gradients/model\.conv1\.bias.bins
+      5: 1
+      6:
+      - 1
+    - 1: NYU/confusion_matrix/0._type
+      5: 1
+      6:
+      - 1
+    - 1: NYU/confusion_matrix/0.sha256
+      5: 1
+      6:
+      - 1
+    - 1: NYU/confusion_matrix/0.size
+      5: 1
+      6:
+      - 1
+    - 1: NYU/confusion_matrix/0.path
+      5: 1
+      6:
+      - 1
+    - 1: NYU/confusion_matrix/0.format
+      5: 1
+      6:
+      - 1
+    - 1: NYU/confusion_matrix/0.width
+      5: 1
+      6:
+      - 1
+    - 1: NYU/confusion_matrix/0.height
+      5: 1
+      6:
+      - 1
+    - 1: NYU/f1_p_r_heatmap/0._type
+      5: 1
+      6:
+      - 1
+    - 1: NYU/f1_p_r_heatmap/0.sha256
+      5: 1
+      6:
+      - 1
+    - 1: NYU/f1_p_r_heatmap/0.size
+      5: 1
+      6:
+      - 1
+    - 1: NYU/f1_p_r_heatmap/0.path
+      5: 1
+      6:
+      - 1
+    - 1: NYU/f1_p_r_heatmap/0.format
+      5: 1
+      6:
+      - 1
+    - 1: NYU/f1_p_r_heatmap/0.width
+      5: 1
+      6:
+      - 1
+    - 1: NYU/f1_p_r_heatmap/0.height
+      5: 1
+      6:
+      - 1
+    - 1: NYU/Loss/val
+      5: 1
+      6:
+      - 1
+    - 1: NYU/Accuracy/val
+      5: 1
+      6:
+      - 1
+    - 1: NYU/Loss/train
+      5: 1
+      6:
+      - 1
+    - 1: NYU/Accuracy/train
+      5: 1
+      6:
+      - 1
+    python_version: 3.8.10
+    start_time: 1656942942
+    t:
+      1:
+      - 1
+      - 5
+      - 9
+      - 41
+      - 53
+      - 55
+      3:
+      - 1
+      - 7
+      - 23
+      4: 3.8.10
+      5: 0.12.15
+      8:
+      - 5
+inputs:
+  desc: null
+  value: None
+net/LSTM/bidirectional:
+  desc: null
+  value: true
+net/LSTM/hidden_size:
+  desc: null
+  value: 128
+net/LSTM/input_size:
+  desc: null
+  value: 116
+net/LSTM/num_layers:
+  desc: null
+  value: 1
+net/Simple_QHN/backend:
+  desc: null
+  value: aer_simulator
+net/Simple_QHN/linear_out:
+  desc: null
+  value: 64
+net/Simple_QHN/lstm_hidden:
+  desc: null
+  value: 128
+net/Simple_QHN/n_qubits:
+  desc: null
+  value: 2
+net/Simple_QHN/shift:
+  desc: null
+  value: 0.6
+net/hybrid/dense_type:
+  desc: null
+  value: 2
+net/hybrid/n_qubits:
+  desc: null
+  value: 3
+net/hybrid/shift:
+  desc: null
+  value: 0.5
+net/hybrid/shots:
+  desc: null
+  value: 100
+net/hybrid/simulator:
+  desc: null
+  value: aer_simulator
+net/model:
+  desc: null
+  value: Simple_QHN
+net/num_classes:
+  desc: null
+  value: 2
+net/roi_rank:
+  desc: null
+  value:
+  - 0
+  - 1
+  - 2
+  - 3
+  - 4
+  - 5
+  - 6
+  - 7
+  - 8
+  - 9
+  - 10
+  - 11
+  - 12
+  - 13
+  - 14
+  - 15
+  - 16
+  - 17
+  - 18
+  - 19
+  - 20
+  - 21
+  - 22
+  - 23
+  - 24
+  - 25
+  - 26
+  - 27
+  - 28
+  - 29
+  - 30
+  - 31
+  - 32
+  - 33
+  - 34
+  - 35
+  - 36
+  - 37
+  - 38
+  - 39
+  - 40
+  - 41
+  - 42
+  - 43
+  - 44
+  - 45
+  - 46
+  - 47
+  - 48
+  - 49
+  - 50
+  - 51
+  - 52
+  - 53
+  - 54
+  - 55
+  - 56
+  - 57
+  - 58
+  - 59
+  - 60
+  - 61
+  - 62
+  - 63
+  - 64
+  - 65
+  - 66
+  - 67
+  - 68
+  - 69
+  - 70
+  - 71
+  - 72
+  - 73
+  - 74
+  - 75
+  - 76
+  - 77
+  - 78
+  - 79
+  - 80
+  - 81
+  - 82
+  - 83
+  - 84
+  - 85
+  - 86
+  - 87
+  - 88
+  - 89
+  - 90
+  - 91
+  - 92
+  - 93
+  - 94
+  - 95
+  - 96
+  - 97
+  - 98
+  - 99
+  - 100
+  - 101
+  - 102
+  - 103
+  - 104
+  - 105
+  - 106
+  - 107
+  - 108
+  - 109
+  - 110
+  - 111
+  - 112
+  - 113
+  - 114
+  - 115
+opt/lr:
+  desc: null
+  value: 1.0e-05
+opt/optimizer:
+  desc: null
+  value: Adam
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/diff.patch b/Logs/wandb/run-20220704_225542-2b98ixbg/files/diff.patch
new file mode 100644
index 0000000..2c78aa2
--- /dev/null
+++ b/Logs/wandb/run-20220704_225542-2b98ixbg/files/diff.patch
@@ -0,0 +1,454 @@
+diff --git a/Configs/config.yaml b/Configs/config.yaml
+index 541dd0b..aa00027 100644
+--- a/Configs/config.yaml
++++ b/Configs/config.yaml
+@@ -31,12 +31,7 @@ data:
+ network:
+   model: ${log.model_name}
+   num_classes: 2
+-  encoder: ${SCCNN_bn}
+-  attention: ${Attention}
+   LSTM: ${LSTM}
+-  decoder: ${Decoder}
+-  is_encoder_shared: True
+-  encoderType: SCCNN_bn
+   Simple_QHN: ${Simple_QHN}
+   hybrid: ${quantum_circuit}
+ 
+@@ -45,5 +40,4 @@ optimizer:
+   lr: 1e-5
+ 
+ 
+-runner: OneSiteHoldout_Runner
+ 
+diff --git a/Configs/models.yaml b/Configs/models.yaml
+index 2c5c2ba..e561fb7 100644
+--- a/Configs/models.yaml
++++ b/Configs/models.yaml
+@@ -1,6 +1,6 @@
+ 
+ LSTM:
+-  input_size: ${SCCNN.conv_block4.out_f}
++  input_size: 116
+   hidden_size: 128
+   num_layers: 1
+   bidirectional: True
+@@ -8,9 +8,9 @@ LSTM:
+ Simple_QHN:
+   n_qubits: 2
+   shift: 0.6
+-  is_cnot: True
+   lstm_hidden: 128
+   linear_out: 64
++  backend: aer_simulator
+ 
+ 
+ quantum_circuit:
+@@ -18,5 +18,4 @@ quantum_circuit:
+   simulator: aer_simulator
+   shift: 0.5
+   shots: 100
+-  is_cnot: True
+   dense_type: 2
+\ No newline at end of file
+diff --git a/Logs/QML-MNIST/version000/version_0/hparams.yaml b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+index 3851257..000d975 100644
+--- a/Logs/QML-MNIST/version000/version_0/hparams.yaml
++++ b/Logs/QML-MNIST/version000/version_0/hparams.yaml
+@@ -15,6 +15,5 @@ net:
+     simulator: aer_simulator
+     shift: 0.5
+     shots: 100
+-    is_cnot: true
+     dense_type: 2
+ inputs: null
+diff --git a/run.py b/run.py
+index c8e1ccf..69d3f58 100644
+--- a/run.py
++++ b/run.py
+@@ -1,7 +1,7 @@
+ from omegaconf import OmegaConf
+ from pathlib import Path
+ 
+-from src.runners import S_Runner, MNIST_Runner
++from src.runners import S_Runner
+ 
+ CONFIG_DIR = Path("Configs")
+ 
+@@ -11,7 +11,7 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+     cfg = OmegaConf.merge(cfg, model_params)
+     cfg.merge_with_cli()
+ 
+-    runner = MNIST_Runner(
++    runner = S_Runner(
+         log=cfg.log,
+         optimizer=cfg.optimizer,
+         loader=cfg.loader,
+@@ -22,4 +22,4 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
+ 
+ 
+ if __name__ == "__main__":
+-    main(cfg=OmegaConf.load(CONFIG_DIR / "config MNIST.yaml"))
++    main()
+diff --git a/src/data/__pycache__/data_etl.cpython-38.pyc b/src/data/__pycache__/data_etl.cpython-38.pyc
+index 288d14c..934cbd5 100644
+Binary files a/src/data/__pycache__/data_etl.cpython-38.pyc and b/src/data/__pycache__/data_etl.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/loader.cpython-38.pyc b/src/data/__pycache__/loader.cpython-38.pyc
+index eddf358..67504ef 100644
+Binary files a/src/data/__pycache__/loader.cpython-38.pyc and b/src/data/__pycache__/loader.cpython-38.pyc differ
+diff --git a/src/data/__pycache__/sampling.cpython-38.pyc b/src/data/__pycache__/sampling.cpython-38.pyc
+index a2cbaf3..a748fd2 100644
+Binary files a/src/data/__pycache__/sampling.cpython-38.pyc and b/src/data/__pycache__/sampling.cpython-38.pyc differ
+diff --git a/src/data/data_etl.py b/src/data/data_etl.py
+index 08700be..dc691b0 100755
+--- a/src/data/data_etl.py
++++ b/src/data/data_etl.py
+@@ -137,24 +137,6 @@ def chunks(lst, n):
+     return [lst[i : i + div] for i in range(0, len(lst), div)]
+ 
+ 
+-@dataclass
+-class Load_MNIST:
+-    def __init__(self, is_train:Boolean):
+-        if is_train:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=True, download=True,
+-                         transform=transforms.Compose([transforms.ToTensor()]))
+-        else:
+-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=False, download=True,
+-                        transform=transforms.Compose([transforms.ToTensor()]))
+-                        
+-    def get_samples(self, n_samples:int):
+-        idx = []
+-        for i in range(10):
+-            idx.append(np.where(self.dataset.targets == i)[0][:n_samples])#각 target에 해당되는 번호를 저장한다. 
+-        idx = np.array(idx).reshape(-1)
+-        self.dataset.data = self.dataset.data[idx]
+-        self.dataset.targets = self.dataset.targets[idx]
+-        return self.dataset
+ 
+ 
+ 
+diff --git a/src/data/loader.py b/src/data/loader.py
+index 288a3b3..158cbdc 100644
+--- a/src/data/loader.py
++++ b/src/data/loader.py
+@@ -5,25 +5,8 @@ from typing import Union, List
+ import torch
+ from torch.utils.data import Dataset
+ 
+-from src.data import Load, SITES_DICT, Load_MNIST
+-
+-class MNISTDataset(Dataset):
+-    def __init__(self, n_samples: int, is_train: Boolean) -> None:
+-        Load = Load_MNIST(is_train)
+-        if n_samples >0:
+-            self.data = Load.get_samples(n_samples=n_samples).data
+-            self.labels = Load.get_samples(n_samples=n_samples).targets  
+-        else:  
+-            self.data = Load.dataset.data
+-            self.labels = Load.dataset.targets
++from src.data import Load, SITES_DICT
+ 
+-    def __len__(self):
+-        return len(self.labels)
+-    
+-    def __getitem__(self, index: int):
+-        data = self.data[index]
+-        label = self.labels[index]
+-        return data, label 
+   
+ class ROIDataset(Dataset):
+     def __init__( self, site: Union[List, str]) -> None:
+diff --git a/src/datamodules/__pycache__/datamodule.cpython-38.pyc b/src/datamodules/__pycache__/datamodule.cpython-38.pyc
+index 6f06b1a..d4ba074 100644
+Binary files a/src/datamodules/__pycache__/datamodule.cpython-38.pyc and b/src/datamodules/__pycache__/datamodule.cpython-38.pyc differ
+diff --git a/src/datamodules/datamodule.py b/src/datamodules/datamodule.py
+index 4fcab3d..0bfab90 100644
+--- a/src/datamodules/datamodule.py
++++ b/src/datamodules/datamodule.py
+@@ -7,64 +7,23 @@ from torch.utils.data import DataLoader, Subset
+ from pytorch_lightning import LightningDataModule
+ from src.data import collate_fn, SamplerFactory
+ 
++
+ @dataclass
+-class MNISTDataModule(LightningDataModule):
+-    
+-    def __init__(self,data: Dict,loader: Dict,dataset: Dict):
++class DataModule(LightningDataModule):
++    def __init__(self, data: Dict, loader: Dict, dataset: Dict):
+         super().__init__()
+-        self.prepare_data_per_node = True
+         self.data = data
+         self.loader = loader
+         self.dataset = dataset
+-    def setup(self, stage: Optional[str] = None):
+-        if stage in ("fit", None):
+-            self.train_dataset = self.dataset(n_samples = self.loader.samples.train_sample, is_train=True)
+-            self.val_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-
+-        if stage in ("test", None):
+-            self.test_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
+-    
+-    def train_dataloader(self):
+-        conf = deepcopy(self.loader.train)
+-        batch_size = conf.pop("batch_size")
+-        conf.shuffle = False
+-        # return DataLoader(self.train_dataset, **conf, collate_fn=collate_fn)
+-
+-        return DataLoader(
+-            self.train_dataset,
+-            **conf,
+-            collate_fn=collate_fn,
+-            batch_sampler=SamplerFactory().get(
+-                class_idxs=[
+-                    np.where(self.train_dataset.labels == i)[0].tolist()
+-                    for i in range(10)
+-                ],
+-                batch_size=batch_size,
+-                n_batches=len(self.train_dataset)//batch_size + 5,
+-                alpha=1.0,
+-                kind="random",
+-            ),
+-        )
+-
+-    def val_dataloader(self):
+-        return DataLoader(self.val_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-    def test_dataloader(self):
+-        return DataLoader(self.test_dataset, **self.loader.eval, collate_fn=collate_fn)
+-
+-@dataclass
+-class DataModule(LightningDataModule):
+-    data: Dict
+-    loader: Dict
+-    dataset: Dict
++        self.prepare_data_per_node = True
+ 
+     def setup(self, stage: Optional[str] = None):
+         if stage in ("fit", None):
+-            self.train_dataset = self.dataset(is_train = True, site = self.data.train_site)
+-            self.val_dataset = self.dataset(is_train = False, site = self.data.train_site)
++            self.train_dataset = self.dataset(site = self.data.train_site)
++            self.val_dataset = self.dataset(site = self.data.train_site)
+ 
+         if stage in ("test", None):
+-            self.test_dataset = self.dataset(is_train = False, site = self.data.train_site)
++            self.test_dataset = self.dataset(site = self.data.train_site)
+ 
+     def train_dataloader(self):
+         conf = deepcopy(self.loader.train)
+diff --git a/src/layers/__pycache__/__init__.cpython-38.pyc b/src/layers/__pycache__/__init__.cpython-38.pyc
+index 31453e2..a73c856 100644
+Binary files a/src/layers/__pycache__/__init__.cpython-38.pyc and b/src/layers/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/layers/__pycache__/quantum_layers.cpython-38.pyc b/src/layers/__pycache__/quantum_layers.cpython-38.pyc
+index 4f219f9..3e30b2e 100644
+Binary files a/src/layers/__pycache__/quantum_layers.cpython-38.pyc and b/src/layers/__pycache__/quantum_layers.cpython-38.pyc differ
+diff --git a/src/layers/quantum_layers.py b/src/layers/quantum_layers.py
+index 976aa6a..4dfccfa 100644
+--- a/src/layers/quantum_layers.py
++++ b/src/layers/quantum_layers.py
+@@ -35,7 +35,7 @@ class QuantumCircuit:
+         for theta, qubit in zip(self.theta, self.all_qubits):
+             self._circuit.ry(theta, qubit)
+ 
+-        for i in [2,3,4,5]:
++        for i in range(2, self.n_qubit):
+             self._circuit.cx(0,i)
+ 
+         self._circuit.measure_all()
+diff --git a/src/models/__pycache__/__init__.cpython-38.pyc b/src/models/__pycache__/__init__.cpython-38.pyc
+index f5a0d8c..9643058 100644
+Binary files a/src/models/__pycache__/__init__.cpython-38.pyc and b/src/models/__pycache__/__init__.cpython-38.pyc differ
+diff --git a/src/models/__pycache__/qhn.cpython-38.pyc b/src/models/__pycache__/qhn.cpython-38.pyc
+index 56e2a1f..578377b 100644
+Binary files a/src/models/__pycache__/qhn.cpython-38.pyc and b/src/models/__pycache__/qhn.cpython-38.pyc differ
+diff --git a/src/models/qhn.py b/src/models/qhn.py
+index c74fec4..f9da96f 100644
+--- a/src/models/qhn.py
++++ b/src/models/qhn.py
+@@ -27,12 +27,12 @@ class Simple_QHN(nn.Module):
+         self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+         self.hybrid = Hybrid(
+             params.n_qubits,
+-            qiskit.Aer.get_backend("aer_simulator"),
++            params.backend,
+             100,
+             shift=params.shift,
+-            is_cnot=params.is_cnot,
++            
+         )
+-        self.fc3 = nn.Linear(params.n_qubits * 2, 2)
++        self.fc3 = nn.Linear(2**params.n_qubits, 2)
+         self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+ 
+     def forward(self, x):
+@@ -50,40 +50,7 @@ class Simple_QHN(nn.Module):
+         x = self.fc2(x)
+         x = torch.tanh(x) * torch.ones_like(x) * torch.tensor(np.pi / 2)
+         x = self.hybrid(x).to(self.device)
+-        x = torch.cat((x, 1 - x), -1)
+         x = F.softmax(self.fc3(x), dim=1)
+         return x
+ 
+ 
+-class MNIST_QHN(nn.Module):
+-    def __init__(self, params: Optional[Dict] = None, *args, **kwargs) -> None:
+-        super(MNIST_QHN, self).__init__()
+-        params = params.MNIST_QHN
+-        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
+-        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
+-        self.dropout = nn.Dropout2d()
+-        self.fc1 = nn.Linear(256, params.linear_out)
+-        self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
+-        self.hybrid = Hybrid(
+-            params.n_qubits,
+-            backend = "aer_simulator",
+-            shots = params.shots,
+-            shift=params.shift,
+-        )
+-        self.fc3 = nn.Linear(2**params.n_qubits, 10)
+-        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+-
+-    def forward(self, x):
+-        x = x.unsqueeze(1)
+-        x = F.relu(self.conv1(x))
+-        x = F.max_pool2d(x, 2)
+-        x = F.relu(self.conv2(x))
+-        x = F.max_pool2d(x, 2)
+-        
+-        x = x.view(x.shape[0],-1)
+-        x = F.relu(self.fc1(x))
+-        x = self.fc2(x)
+-        x = torch.tanh(x) * torch.ones_like(x) * torch.tensor(np.pi / 2)
+-        x = self.hybrid(x).to(self.device)
+-        x = F.softmax(self.fc3(x), dim=1)
+-        return x
+diff --git a/src/runners/__pycache__/base.cpython-38.pyc b/src/runners/__pycache__/base.cpython-38.pyc
+index 6fae27c..b7d08e9 100644
+Binary files a/src/runners/__pycache__/base.cpython-38.pyc and b/src/runners/__pycache__/base.cpython-38.pyc differ
+diff --git a/src/runners/__pycache__/runner.cpython-38.pyc b/src/runners/__pycache__/runner.cpython-38.pyc
+index 9813d76..3792f76 100644
+Binary files a/src/runners/__pycache__/runner.cpython-38.pyc and b/src/runners/__pycache__/runner.cpython-38.pyc differ
+diff --git a/src/runners/runner.py b/src/runners/runner.py
+index 86b61a5..5fb03f3 100644
+--- a/src/runners/runner.py
++++ b/src/runners/runner.py
+@@ -6,8 +6,8 @@ from typing import Optional
+ from copy import deepcopy
+ 
+ from src.runners import Base_Runner
+-from src.data import ROIDataset, SITES_DICT, MNISTDataset
+-from src.datamodules import DataModule, MNISTDataModule
++from src.data import ROIDataset, SITES_DICT
++from src.datamodules import DataModule
+ from src.tasks import ClassificationTask
+ from src.utils import plot_paper
+ from src.callbacks import wandb_callback as wbc
+@@ -177,106 +177,4 @@ class S_Runner(Base_Runner):
+ 
+         return
+ 
+-class MNIST_Runner(Base_Runner):
+-    def get_callbacks(self):
+-        """
+-        Write only callbacks that logger is not necessary
+-        """
+-        checkpoint_callback = ModelCheckpoint(
+-            dirpath=os.path.join(
+-                self.log.checkpoint_path,
+-                self.log.project_name,
+-                f"version{self.version:03d}",
+-            ),
+-            filename=os.path.join(f"model"),
+-            monitor=f"Accuracy/val",
+-            mode="max",
+-            verbose=False,
+-            save_top_k=1,
+-        )
+-
+-        callbacks = dict(
+-            filter(lambda item: item[0].endswith("callback"), vars().items())
+-        ).values()
+-        callbacks = list(callbacks)
+-        return callbacks if len(callbacks) > 0 else None
+-
+-    def run(self, profiler: Optional[str] = None):
+-        os.makedirs(
+-            os.path.join(self.log.checkpoint_path, self.log.project_name), exist_ok=True
+-        )
+-        self.version = len(
+-            os.listdir(os.path.join(self.log.checkpoint_path, self.log.project_name))
+-        )
+-
+-
+-        final_results = list()
+-
+-
+-
+-        dm = self.get_datamodule(dataset=MNISTDataset, datamodule=MNISTDataModule)
+-        model = self.get_network(Task=ClassificationTask)
+-        model.apply(initialize_weights)
+-
+-        trainer = Trainer(
+-            logger=[
+-                TensorBoardLogger(
+-                    save_dir=self.log.log_path,
+-                    name=os.path.join(
+-                        self.log.project_name,
+-                        f"version{self.version:03d}"
+-                    ),
+-                    default_hp_metric=False,
+-                    version=None,
+-                    # log_graph=True, # inavailable due to bug
+-                ),
+-                WandbLogger(
+-                    project=self.log.project_name,
+-                    save_dir=self.log.log_path,
+-                ),
+-            ],
+-            # ! use all gpu
+-            # gpus=-1,
+-            # auto_select_gpus=True,
+-            # ! use 2 gpu
+-            # devices=2,
+-            # accelerator="auto",
+-            # strategy="ddp",
+-            # ! use gpu 0
+-            devices=[0],
+-            accelerator="gpu",
+-            #devices=[self.log.device.gpu],
+-            #accelerator="cpu",
+-            check_val_every_n_epoch=self.log.val_log_freq_epoch,
+-            log_every_n_steps=1,
+-            num_sanity_val_steps=0,
+-            max_epochs=self.log.epoch,
+-            profiler=profiler,
+-            callbacks=[
+-                *self.get_callbacks(),
+-                wbc.WatchModel(),
+-                wbc.LogConfusionMatrix(),
+-                wbc.LogF1PrecRecHeatmap(),
+-                # tbc.WatchModel(),
+-                # tbc.LogConfusionMatrix(),
+-                # tbc.LogF1PrecRecHeatmap(),
+-            ],
+-            precision=self.log.precision,
+-            # gradient_clip_val=0.5,
+-        )
+-        trainer.test_site_prefix = model.prefix
+-        trainer.fit(model, datamodule=dm)
+-        trainer.test(model, datamodule=dm, ckpt_path="best")
+-        final_results.append(
+-            trainer.callback_metrics[f"Accuracy/test"]
+-        )
+-
+-    try:
+-        import wandb
+-
+-        wb_logger = wbc.get_wandb_logger(trainer=Trainer)
+-        
+-    except Exception as e:
+-        print(e)
+-
+     
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/0_0_d8ed2534aa2e58956c20.png b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/0_0_d8ed2534aa2e58956c20.png
new file mode 100644
index 0000000..949eb3b
Binary files /dev/null and b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/0_0_d8ed2534aa2e58956c20.png differ
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/1_3_d8ed2534aa2e58956c20.png b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/1_3_d8ed2534aa2e58956c20.png
new file mode 100644
index 0000000..949eb3b
Binary files /dev/null and b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/1_3_d8ed2534aa2e58956c20.png differ
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/2_6_d8ed2534aa2e58956c20.png b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/2_6_d8ed2534aa2e58956c20.png
new file mode 100644
index 0000000..949eb3b
Binary files /dev/null and b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/2_6_d8ed2534aa2e58956c20.png differ
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/0_1_28b0d4d32bb09382fc1b.png b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/0_1_28b0d4d32bb09382fc1b.png
new file mode 100644
index 0000000..f5bf6f8
Binary files /dev/null and b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/0_1_28b0d4d32bb09382fc1b.png differ
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/1_4_28b0d4d32bb09382fc1b.png b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/1_4_28b0d4d32bb09382fc1b.png
new file mode 100644
index 0000000..f5bf6f8
Binary files /dev/null and b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/1_4_28b0d4d32bb09382fc1b.png differ
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/2_7_28b0d4d32bb09382fc1b.png b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/2_7_28b0d4d32bb09382fc1b.png
new file mode 100644
index 0000000..f5bf6f8
Binary files /dev/null and b/Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/2_7_28b0d4d32bb09382fc1b.png differ
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log b/Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
new file mode 100644
index 0000000..bec6cf1
--- /dev/null
+++ b/Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
@@ -0,0 +1,80 @@
+GPU available: True, used: True
+TPU available: False, using: 0 TPU cores
+IPU available: False, using: 0 IPUs
+HPU available: False, using: 0 HPUs
+Epoch 0:   0%|                                                                                 | 0/14 [00:00<?, ?it/s]
+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
+  | Name        | Type             | Params
+-------------------------------------------------
+0 | model       | Simple_QHN       | 170 K
+1 | get_metrics | MetricCollection | 0
+-------------------------------------------------
+170 K     Trainable params
+0         Non-trainable params
+170 K     Total params
+0.680     Total estimated model params size (MB)
+Missing logger folder: Logs/QML-MNIST/version001/NYU
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+
+
+
+Epoch 0:  93%|████████████████████████████████████████████▌   | 13/14 [00:08<00:00,  1.56it/s, loss=0.706, v_num=ixbg]
+Validation DataLoader 0:   0%|                                                                  | 0/1 [00:00<?, ?it/s]
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+
+Validation DataLoader 0: 100%|██████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.79s/it]
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
+
+
+
+
+
+Epoch 1:  93%|████████████████████████████████████████████▌   | 13/14 [00:21<00:01,  1.65s/it, loss=0.706, v_num=ixbg]
+
+Validation DataLoader 0:   0%|                                                                  | 0/1 [00:00<?, ?it/s]
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
+
+
+
+Epoch 2:  93%|████████████████████████████████████████████▌   | 13/14 [00:32<00:02,  2.48s/it, loss=0.706, v_num=ixbg]
+Validation DataLoader 0:   0%|                                                                  | 0/1 [00:00<?, ?it/s]
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
+  rank_zero_warn(
+/home/sol/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
+
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/requirements.txt b/Logs/wandb/run-20220704_225542-2b98ixbg/files/requirements.txt
new file mode 100644
index 0000000..f9f8632
--- /dev/null
+++ b/Logs/wandb/run-20220704_225542-2b98ixbg/files/requirements.txt
@@ -0,0 +1,166 @@
+absl-py==1.0.0
+aiohttp==3.8.1
+aiosignal==1.2.0
+antlr4-python3-runtime==4.8
+async-timeout==4.0.2
+attrs==19.3.0
+automat==0.8.0
+autopep8==1.6.0
+blinker==1.4
+cachetools==5.0.0
+certifi==2019.11.28
+chardet==3.0.4
+charset-normalizer==2.0.12
+click==7.0
+cloud-init==22.1
+colorama==0.4.3
+command-not-found==0.3
+configobj==5.0.6
+constantly==15.1.0
+cryptography==2.8
+cssselect==1.1.0
+cycler==0.11.0
+dbus-python==1.2.16
+dill==0.3.4
+distro-info==0.23ubuntu1
+distro==1.4.0
+docker-pycreds==0.4.0
+entrypoints==0.3
+filelock==3.6.0
+fonttools==4.33.3
+frozenlist==1.3.0
+fsspec==2022.3.0
+gistory==0.44
+gitdb==4.0.9
+gitpython==3.1.27
+google-auth-oauthlib==0.4.6
+google-auth==2.6.6
+grpcio==1.44.0
+httplib2==0.14.0
+hyperlink==19.0.0
+idna==2.8
+importlib-metadata==4.11.3
+incremental==16.10.1
+itemadapter==0.5.0
+itemloaders==1.0.4
+jinja2==2.10.1
+jmespath==1.0.0
+joblib==1.1.0
+jsonpatch==1.22
+jsonpointer==2.0
+jsonschema==3.2.0
+keyring==18.0.1
+kiwisolver==1.4.2
+language-selector==0.1
+launchpadlib==1.10.13
+lazr.restfulclient==0.14.2
+lazr.uri==1.0.3
+lxml==4.8.0
+markdown==3.3.6
+markupsafe==1.1.0
+matplotlib==3.5.1
+more-itertools==4.2.0
+mpmath==1.2.1
+multidict==6.0.2
+netifaces==0.10.4
+nibabel==3.2.2
+nilearn==0.9.1
+ntlm-auth==1.5.0
+numpy==1.22.3
+oauthlib==3.1.0
+omegaconf==2.1.2
+packaging==21.3
+pandas==1.4.2
+parsel==1.6.0
+pathtools==0.1.2
+pbr==5.8.1
+pexpect==4.6.0
+pillow==9.1.0
+pip==20.0.2
+ply==3.11
+promise==2.3
+protego==0.2.1
+protobuf==3.20.1
+psutil==5.9.0
+pyasn1-modules==0.2.1
+pyasn1==0.4.2
+pycodestyle==2.8.0
+pydeprecate==0.3.2
+pydispatcher==2.0.5
+pygame==2.1.2
+pygobject==3.36.0
+pyhamcrest==1.9.0
+pyjwt==1.7.1
+pymacaroons==0.13.0
+pynacl==1.3.0
+pyopenssl==19.0.0
+pyparsing==3.0.8
+pyrsistent==0.15.5
+pyserial==3.4
+python-apt==2.0.0+ubuntu0.20.4.7
+python-constraint==1.4.0
+python-dateutil==2.8.2
+python-debian==0.1.36ubuntu1
+pytorch-lightning==1.6.2
+pytz==2022.1
+pyyaml==6.0
+qiskit-aer==0.10.4
+qiskit-ibmq-provider==0.19.1
+qiskit-ignis==0.7.0
+qiskit-terra==0.20.1
+qiskit==0.36.1
+queuelib==1.6.2
+requests-file==1.5.1
+requests-ntlm==1.1.0
+requests-oauthlib==1.3.1
+requests-unixsocket==0.2.0
+requests==2.22.0
+retworkx==0.11.0
+rsa==4.8
+scikit-learn==1.0.2
+scipy==1.8.0
+seaborn==0.11.2
+secretstorage==2.3.1
+sentry-sdk==1.5.10
+service-identity==18.1.0
+setproctitle==1.2.3
+setuptools==45.2.0
+shortuuid==1.0.8
+simplejson==3.16.0
+six==1.14.0
+sklearn==0.0
+smmap==5.0.0
+sos==4.3
+ssh-import-id==5.10
+stevedore==3.5.0
+symengine==0.9.2
+sympy==1.10.1
+systemd-python==234
+tensorboard-data-server==0.6.1
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.8.0
+threadpoolctl==3.1.0
+tldextract==3.2.1
+toml==0.10.2
+torch-tb-profiler==0.4.0
+torch==1.12.0
+torchmetrics==0.8.1
+torchvision==0.13.0
+tqdm==4.64.0
+tweedledum==1.1.1
+twisted==18.9.0
+typing-extensions==4.2.0
+ubuntu-advantage-tools==27.7
+ufw==0.36
+unattended-upgrades==0.1
+urllib3==1.25.8
+w3lib==1.22.0
+wadllib==1.3.3
+wandb==0.12.15
+websocket-client==1.3.2
+websockets==10.3
+werkzeug==2.1.2
+wheel==0.34.2
+yarl==1.7.2
+zipp==1.0.0
+zope.interface==4.7.1
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-metadata.json b/Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-metadata.json
new file mode 100644
index 0000000..48a2122
--- /dev/null
+++ b/Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-metadata.json
@@ -0,0 +1,24 @@
+{
+    "os": "Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.29",
+    "python": "3.8.10",
+    "heartbeatAt": "2022-07-04T13:55:44.976081",
+    "startedAt": "2022-07-04T13:55:42.835877",
+    "docker": null,
+    "gpu": "NVIDIA GeForce GTX 1650",
+    "gpu_count": 1,
+    "cpu_count": 8,
+    "cuda": null,
+    "args": [],
+    "state": "running",
+    "program": "/home/sol/git/QML2/run.py",
+    "codePath": "run.py",
+    "git": {
+        "remote": "https://github.com/PVNkT/QML2.git",
+        "commit": "1f3b04ad3f84cb17a3713da979f0adf1e2028c86"
+    },
+    "email": "3031902@gmail.com",
+    "root": "/home/sol/git/QML2",
+    "host": "DESKTOP-HFMS9NT",
+    "username": "sol",
+    "executable": "/bin/python3"
+}
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-summary.json b/Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-summary.json
new file mode 100644
index 0000000..3d97e86
--- /dev/null
+++ b/Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-summary.json
@@ -0,0 +1 @@
+{"gradients/model.fc3.bias": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-0.035740889608860016, -0.03462398797273636, -0.0335070826113224, -0.032390180975198746, -0.03127327933907509, -0.030156375840306282, -0.029039472341537476, -0.02792257070541382, -0.026805667206645012, -0.025688763707876205, -0.024571862071752548, -0.023454958572983742, -0.022338055074214935, -0.021221153438091278, -0.02010424993932247, -0.018987346440553665, -0.017870444804430008, -0.0167535413056612, -0.015636639669537544, -0.014519736170768738, -0.013402833603322506, -0.012285931035876274, -0.011169027537107468, -0.010052124969661236, -0.008935222402215004, -0.007818319834768772, -0.006701416801661253, -0.005584513768553734, -0.004467611201107502, -0.00335070863366127, -0.002233805600553751, -0.0011169025674462318, 0.0, 0.0011169028002768755, 0.002233805600553751, 0.0033507084008306265, 0.004467611201107502, 0.005584513768553734, 0.006701416801661253, 0.007818319834768772, 0.008935222402215004, 0.010052124969661236, 0.011169027537107468, 0.012285931035876274, 0.013402833603322506, 0.014519736170768738, 0.015636639669537544, 0.01675354316830635, 0.017870444804430008, 0.018987348303198814, 0.02010424993932247, 0.021221153438091278, 0.022338055074214935, 0.023454958572983742, 0.024571862071752548, 0.025688763707876205, 0.026805667206645012, 0.02792257070541382, 0.029039472341537476, 0.030156375840306282, 0.03127327933907509, 0.032390180975198746, 0.0335070826113224, 0.03462398797273636, 0.035740889608860016]}, "_timestamp": 1656942984, "_runtime": 42, "gradients/model.fc3.weight": {"_type": "histogram", "values": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0], "bins": [-0.018957311287522316, -0.01836489513516426, -0.017772478982806206, -0.01718006283044815, -0.016587646678090096, -0.01599523052573204, -0.01540281530469656, -0.014810399152338505, -0.01421798299998045, -0.013625566847622395, -0.01303315069526434, -0.012440734542906284, -0.011848319321870804, -0.011255903169512749, -0.010663487017154694, -0.010071070864796638, -0.009478654712438583, -0.008886238560080528, -0.008293822407722473, -0.007701406721025705, -0.00710899056866765, -0.006516574416309595, -0.005924158729612827, -0.005331742577254772, -0.004739326424896717, -0.004146910272538662, -0.0035544943530112505, -0.002962078433483839, -0.002369662281125784, -0.0017772461287677288, -0.0011848302092403173, -0.0005924142897129059, 0.0, 0.0005924160359427333, 0.0011848320718854666, 0.0017772481078281999, 0.002369664143770933, 0.0029620802961289883, 0.0035544962156563997, 0.004146912135183811, 0.004739328287541866, 0.005331744439899921, 0.0059241605922579765, 0.006516576278954744, 0.0071089924313127995, 0.007701408583670855, 0.008293824270367622, 0.008886240422725677, 0.009478656575083733, 0.010071072727441788, 0.010663488879799843, 0.011255905032157898, 0.011848321184515953, 0.012440737336874008, 0.013033152557909489, 0.013625568710267544, 0.014217984862625599, 0.014810401014983654, 0.01540281716734171, 0.01599523238837719, 0.016587648540735245, 0.0171800646930933, 0.017772480845451355, 0.01836489699780941, 0.018957313150167465]}, "gradients/model.fc2.bias": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-0.004163419362157583, -0.0041307108476758, -0.004098002798855305, -0.004065294284373522, -0.004032585769891739, -0.003999877255409956, -0.003967168740928173, -0.0039344606921076775, -0.0039017521776258945, -0.0038690436631441116, -0.0038363353814929724, -0.003803627099841833, -0.00377091858536005, -0.0037382100708782673, -0.003705501789227128, -0.0036727935075759888, -0.003640084993094206, -0.003607376478612423, -0.0035746681969612837, -0.0035419599153101444, -0.0035092514008283615, -0.0034765428863465786, -0.0034438346046954393, -0.0034111263230443, -0.003378417808562517, -0.0033457092940807343, -0.003313001012429595, -0.0032802927307784557, -0.003247584216296673, -0.00321487570181489, -0.0031821674201637506, -0.0031494591385126114, -0.0031167506240308285, -0.0030840421095490456, -0.0030513338278979063, -0.003018625546246767, -0.002985917031764984, -0.0029532085172832012, -0.002920500235632062, -0.0028877919539809227, -0.00285508343949914, -0.002822374925017357, -0.0027896666433662176, -0.0027569583617150784, -0.0027242498472332954, -0.0026915413327515125, -0.0026588330511003733, -0.002626124769449234, -0.002593416254967451, -0.002560707740485668, -0.002527999458834529, -0.0024952911771833897, -0.0024625826627016068, -0.002429874148219824, -0.0023971658665686846, -0.0023644575849175453, -0.0023317490704357624, -0.0022990405559539795, -0.0022663322743028402, -0.002233623992651701, -0.002200915478169918, -0.002168206963688135, -0.002135498682036996, -0.0021027904003858566, -0.0020700818859040737]}, "gradients/model.fc2.weight": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 5.0, 2.0, 2.0, 3.0, 2.0, 0.0, 1.0, 2.0, 5.0, 2.0, 2.0, 2.0, 4.0, 4.0, 4.0, 52.0, 6.0, 4.0, 3.0, 1.0, 2.0, 1.0, 1.0, 1.0], "bins": [-0.005045817233622074, -0.004954938776791096, -0.00486405985429883, -0.004773181397467852, -0.004682302940636873, -0.0045914240181446075, -0.004500545561313629, -0.004409667104482651, -0.004318788647651672, -0.004227910190820694, -0.004137031268328428, -0.00404615281149745, -0.0039552743546664715, -0.0038643956650048494, -0.0037735169753432274, -0.003682638518512249, -0.0035917595960199833, -0.0035008809063583612, -0.003410002449527383, -0.003319123759865761, -0.0032282453030347824, -0.0031373666133731604, -0.0030464879237115383, -0.00295560946688056, -0.0028647310100495815, -0.0027738523203879595, -0.002682973863556981, -0.002592095173895359, -0.0025012167170643806, -0.0024103380274027586, -0.0023194593377411366, -0.002228580880910158, -0.0021377019584178925, -0.0020468232687562704, -0.001955944811925292, -0.00186506612226367, -0.0017741875490173697, -0.0016833089757710695, -0.0015924304025247693, -0.001501551829278469, -0.0014106733724474907, -0.0013197947992011905, -0.0012289162259548903, -0.0011380375362932682, -0.001047158963046968, -0.0009562803898006678, -0.0008654018165543675, -0.0007745231851004064, -0.0006836445536464453, -0.000592765980400145, -0.0005018873489461839, -0.0004110087756998837, -0.000320130173349753, -0.00022925157099962234, -0.00013837299775332212, -4.749436629936099e-05, 4.338420694693923e-05, 0.0001342628092970699, 0.00022514139709528536, 0.0003160199848935008, 0.0004068985872436315, 0.0004977771895937622, 0.0005886557628400624, 0.0006795343942940235, 0.0007704129675403237]}, "gradients/model.fc1.bias": {"_type": "histogram", "values": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 3.0, 0.0, 0.0, 1.0, 2.0, 19.0, 4.0, 1.0, 3.0, 0.0, 1.0, 2.0, 1.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], "bins": [-0.0019507082179188728, -0.0018951799720525742, -0.0018396518426015973, -0.0017841235967352986, -0.0017285954672843218, -0.0016730672214180231, -0.0016175389755517244, -0.0015620108461007476, -0.0015064827166497707, -0.001450954470783472, -0.0013954263413324952, -0.0013398980954661965, -0.0012843699660152197, -0.001228841720148921, -0.0011733134742826223, -0.0011177853448316455, -0.0010622570989653468, -0.0010067288530990481, -0.0009512007236480713, -0.0008956724777817726, -0.0008401443483307958, -0.0007846161024644971, -0.0007290879148058593, -0.0006735597271472216, -0.0006180315394885838, -0.000562503351829946, -0.0005069751641713083, -0.00045144694740884006, -0.0003959187597502023, -0.00034039057209156454, -0.0002848623553290963, -0.00022933416767045856, -0.0001738059800118208, -0.00011827778507722542, -6.274959014263004e-05, -7.22138793207705e-06, 4.830679972656071e-05, 0.00010383498738519847, 0.0001593632041476667, 0.00021489139180630445, 0.0002704195794649422, 0.00032594776712358, 0.00038147595478221774, 0.00043700417154468596, 0.0004925323883071542, 0.000548060517758131, 0.0006035887636244297, 0.0006591169512830675, 0.0007146451389417052, 0.000770173326600343, 0.0008257015142589808, 0.0008812297601252794, 0.0009367578895762563, 0.000992286135442555, 0.0010478142648935318, 0.0011033425107598305, 0.0011588707566261292, 0.0012143990024924278, 0.0012699271319434047, 0.0013254553778097034, 0.0013809835072606802, 0.0014365117531269789, 0.0014920399989932775, 0.0015475681284442544, 0.0016030962578952312]}, "gradients/model.fc1.weight": {"_type": "histogram", "values": [2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 6.0, 5.0, 9.0, 13.0, 13.0, 6.0, 8.0, 20.0, 22.0, 31.0, 37.0, 53.0, 54.0, 61.0, 81.0, 96.0, 137.0, 167.0, 209.0, 303.0, 403.0, 577.0, 917.0, 1810.0, 7317.0, 1277.0, 690.0, 504.0, 357.0, 263.0, 209.0, 126.0, 116.0, 94.0, 65.0, 48.0, 48.0, 38.0, 38.0, 26.0, 22.0, 18.0, 15.0, 12.0, 6.0, 12.0, 7.0, 8.0, 7.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 2.0], "bins": [-0.0016453929711133242, -0.0015943427570164204, -0.0015432925429195166, -0.0014922423288226128, -0.001441192114725709, -0.0013901419006288052, -0.0013390916865319014, -0.0012880414724349976, -0.0012369912583380938, -0.00118594104424119, -0.0011348908301442862, -0.0010838406160473824, -0.0010327904019504786, -0.0009817401878535748, -0.000930689973756671, -0.0008796397596597672, -0.0008285895455628633, -0.0007775393314659595, -0.0007264891173690557, -0.000675438903272152, -0.0006243886891752481, -0.0005733384750783443, -0.0005222882609814405, -0.00047123804688453674, -0.00042018783278763294, -0.00036913761869072914, -0.00031808740459382534, -0.00026703719049692154, -0.00021598697640001774, -0.00016493676230311394, -0.00011388654820621014, -6.283633410930634e-05, -1.1786236427724361e-05, 3.926397766917944e-05, 9.031419176608324e-05, 0.00014136440586298704, 0.00019241461995989084, 0.00024346483405679464, 0.00029451504815369844, 0.00034556526225060225, 0.00039661547634750605, 0.00044766569044440985, 0.0004987159045413136, 0.0005497661186382174, 0.0006008163327351213, 0.000651866546832025, 0.0007029167609289289, 0.0007539669750258327, 0.0008050171891227365, 0.0008560674032196403, 0.0009071176173165441, 0.0009581678314134479, 0.0010092180455103517, 0.0010602682596072555, 0.0011113184737041593, 0.001162368687801063, 0.0012134189018979669, 0.0012644691159948707, 0.0013155193300917745, 0.0013665695441886783, 0.001417619758285582, 0.0014686699723824859, 0.0015197201864793897, 0.0015707704005762935, 0.0016218206146731973]}, "gradients/model.lstm.weight_ih_l0": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 3.0, 1.0, 4.0, 2.0, 2.0, 3.0, 18.0, 17.0, 19.0, 24.0, 50.0, 94.0, 216.0, 745.0, 4840.0, 1343.0, 382.0, 176.0, 83.0, 45.0, 42.0, 26.0, 17.0, 7.0, 5.0, 3.0, 3.0, 2.0, 3.0, 4.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-0.0023923025000840425, -0.0023024629335850477, -0.0022126235999166965, -0.0021227840334177017, -0.0020329446997493505, -0.0019431051332503557, -0.0018532656831666827, -0.0017634262330830097, -0.0016735867829993367, -0.0015837473329156637, -0.0014939078828319907, -0.0014040684327483177, -0.0013142288662493229, -0.0012243895325809717, -0.0011345499660819769, -0.0010447105159983039, -0.0009548710659146309, -0.0008650316158309579, -0.0007751921657472849, -0.000685352657455951, -0.000595513207372278, -0.000505673757288605, -0.00041583424899727106, -0.00032599479891359806, -0.00023615534882992506, -0.00014631588419433683, -5.64764195587486e-05, 3.3363059628754854e-05, 0.00012320250971242785, 0.00021304195979610085, 0.00030288146808743477, 0.00039272091817110777, 0.0004825606010854244, 0.0005724000511690974, 0.0006622395012527704, 0.0007520790095441043, 0.0008419184596277773, 0.0009317579097114503, 0.0010215974180027843, 0.0011114368680864573, 0.0012012763181701303, 0.0012911157682538033, 0.0013809552183374763, 0.0014707946684211493, 0.001560634234920144, 0.0016504735685884953, 0.00174031313508749, 0.001830152585171163, 0.001919992035254836, 0.002009831601753831, 0.002099670935422182, 0.002189510501921177, 0.002279349835589528, 0.002369189402088523, 0.002459028735756874, 0.002548868302255869, 0.0026387078687548637, 0.0027285474352538586, 0.0028183867689222097, 0.0029082263354212046, 0.0029980656690895557, 0.0030879052355885506, 0.0031777448020875454, 0.0032675841357558966, 0.0033574234694242477]}, "gradients/model.lstm.weight_hh_l0": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3.0, 3.0, 4.0, 2.0, 3.0, 7.0, 5.0, 7.0, 9.0, 14.0, 15.0, 19.0, 41.0, 56.0, 69.0, 123.0, 191.0, 405.0, 837.0, 1915.0, 7556.0, 44953.0, 5942.0, 1664.0, 719.0, 356.0, 235.0, 123.0, 65.0, 46.0, 36.0, 36.0, 27.0, 17.0, 9.0, 4.0, 4.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 1.0], "bins": [-0.0007023100042715669, -0.0006812945939600468, -0.0006602791836485267, -0.0006392637733370066, -0.0006182483630254865, -0.0005972330109216273, -0.0005762176006101072, -0.0005552021902985871, -0.000534186779987067, -0.0005131713696755469, -0.0004921559593640268, -0.00047114057815633714, -0.00045012516784481704, -0.00042910975753329694, -0.0004080943763256073, -0.0003870789660140872, -0.0003660635557025671, -0.000345048145391047, -0.0003240327350795269, -0.00030301735387183726, -0.00028200194356031716, -0.00026098653324879706, -0.0002399711374891922, -0.00021895574172958732, -0.00019794033141806722, -0.00017692492110654712, -0.00015590952534694225, -0.00013489412958733737, -0.00011387871927581728, -9.286331624025479e-05, -7.18479132046923e-05, -5.083251744508743e-05, -2.9817165341228247e-05, -8.801762305665761e-06, 1.2213640729896724e-05, 3.322904376545921e-05, 5.4244446801021695e-05, 7.525984983658418e-05, 9.627525287214667e-05, 0.00011729064863175154, 0.00013830605894327164, 0.00015932146925479174, 0.0001803368650143966, 0.00020135226077400148, 0.00022236767108552158, 0.00024338308139704168, 0.0002643984626047313, 0.0002854138729162514, 0.0003064292832277715, 0.0003274446935392916, 0.0003484601038508117, 0.00036947548505850136, 0.00039049089537002146, 0.00041150630568154156, 0.0004325216868892312, 0.0004535370972007513, 0.0004745525075122714, 0.0004955679178237915, 0.0005165833281353116, 0.0005375987384468317, 0.0005586141487583518, 0.000579629500862211, 0.0006006449111737311, 0.0006216603214852512, 0.0006426757317967713]}, "gradients/model.lstm.weight_hh_l0_reverse": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 2.0, 1.0, 4.0, 2.0, 3.0, 5.0, 8.0, 3.0, 9.0, 12.0, 17.0, 21.0, 19.0, 29.0, 41.0, 65.0, 69.0, 97.0, 149.0, 257.0, 352.0, 610.0, 1236.0, 3128.0, 12909.0, 36501.0, 5795.0, 1929.0, 822.0, 479.0, 279.0, 181.0, 132.0, 100.0, 62.0, 55.0, 30.0, 19.0, 21.0, 14.0, 19.0, 10.0, 10.0, 5.0, 7.0, 3.0, 2.0, 1.0, 1.0, 1.0, 0.0, 3.0, 1.0, 1.0, 0.0, 0.0, 1.0], "bins": [-0.00045079391566105187, -0.0004368166555650532, -0.0004228393663652241, -0.0004088621062692255, -0.0003948848170693964, -0.00038090755697339773, -0.00036693026777356863, -0.00035295300767757, -0.00033897574758157134, -0.0003249984874855727, -0.0003110211982857436, -0.00029704393818974495, -0.00028306664898991585, -0.0002690893888939172, -0.00025511212879791856, -0.00024113483959808946, -0.00022715755039826035, -0.00021318027575034648, -0.0001992030011024326, -0.00018522574100643396, -0.00017124845180660486, -0.00015727119171060622, -0.00014329391706269234, -0.00012931664241477847, -0.0001153393677668646, -0.00010136209311895072, -8.738481847103685e-05, -7.340755109908059e-05, -5.943027645116672e-05, -4.5453001803252846e-05, -3.147573443129659e-05, -1.7498459783382714e-05, -3.5212142392992973e-06, 1.0456058589625172e-05, 2.4433331418549642e-05, 3.841060242848471e-05, 5.238787707639858e-05, 6.636515172431245e-05, 8.034241909626871e-05, 9.431969374418259e-05, 0.00010829696839209646, 0.00012227424304001033, 0.0001362515176879242, 0.00015022879233583808, 0.00016420605243183672, 0.00017818334163166583, 0.00019216060172766447, 0.00020613787637557834, 0.00022011515102349222, 0.0002340924256714061, 0.00024806970031931996, 0.0002620469604153186, 0.0002760242496151477, 0.00029000150971114635, 0.00030397879891097546, 0.0003179560590069741, 0.00033193331910297275, 0.0003459105791989714, 0.0003598878683988005, 0.00037386512849479914, 0.00038784241769462824, 0.0004018196777906269, 0.00041579693788662553, 0.00042977422708645463, 0.00044375151628628373]}, "gradients/model.lstm.bias_hh_l0_reverse": {"_type": "histogram", "values": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0, 3.0, 1.0, 3.0, 3.0, 4.0, 7.0, 7.0, 8.0, 15.0, 25.0, 42.0, 49.0, 100.0, 93.0, 40.0, 31.0, 25.0, 8.0, 8.0, 5.0, 7.0, 3.0, 2.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0], "bins": [-0.0006770301260985434, -0.000658657168969512, -0.0006402841536328197, -0.0006219111965037882, -0.0006035382393747568, -0.0005851652822457254, -0.000566792325116694, -0.0005484193097800016, -0.0005300463526509702, -0.0005116733955219388, -0.0004933003801852465, -0.00047492742305621505, -0.00045655446592718363, -0.0004381815087981522, -0.00041980852256529033, -0.00040143553633242846, -0.00038306257920339704, -0.0003646896220743656, -0.00034631663584150374, -0.00032794364960864186, -0.00030957069247961044, -0.000291197735350579, -0.00027282474911771715, -0.00025445176288485527, -0.00023607880575582385, -0.0002177058340748772, -0.00019933286239393055, -0.0001809598907129839, -0.00016258691903203726, -0.0001442139473510906, -0.00012584097567014396, -0.00010746800398919731, -8.909503230825067e-05, -7.072206062730402e-05, -5.234908894635737e-05, -3.397611726541072e-05, -1.5603145584464073e-05, 2.769826096482575e-06, 2.1142797777429223e-05, 3.951576945837587e-05, 5.788874113932252e-05, 7.626171282026917e-05, 9.463468450121582e-05, 0.00011300765618216246, 0.0001313806278631091, 0.00014975359954405576, 0.0001681265712250024, 0.00018649954290594906, 0.0002048725145868957, 0.00022324548626784235, 0.000241618457948789, 0.0002599914441816509, 0.0002783644013106823, 0.0002967373584397137, 0.0003151103446725756, 0.00033348333090543747, 0.0003518562880344689, 0.0003702292451635003, 0.0003886022313963622, 0.00040697521762922406, 0.0004253481747582555, 0.0004437211318872869, 0.0004620941181201488, 0.00048046710435301065, 0.0004988400614820421]}, "gradients/model.lstm.weight_ih_l0_reverse": {"_type": "histogram", "values": [2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 5.0, 6.0, 1.0, 2.0, 2.0, 6.0, 7.0, 7.0, 13.0, 9.0, 11.0, 16.0, 18.0, 36.0, 39.0, 63.0, 82.0, 125.0, 278.0, 530.0, 1926.0, 3621.0, 672.0, 286.0, 144.0, 73.0, 62.0, 41.0, 19.0, 15.0, 15.0, 9.0, 10.0, 6.0, 4.0, 3.0, 4.0, 6.0, 2.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0], "bins": [-0.002403516788035631, -0.002342097694054246, -0.0022806788329035044, -0.002219259738922119, -0.002157840644940734, -0.0020964215509593487, -0.002035002689808607, -0.001973583595827222, -0.0019121646182611585, -0.001850745640695095, -0.0017893265467137098, -0.0017279075691476464, -0.001666488591581583, -0.0016050694976001978, -0.0015436505200341344, -0.001482231542468071, -0.0014208124484866858, -0.0013593934709206223, -0.0012979743769392371, -0.0012365553993731737, -0.0011751363053917885, -0.001113717327825725, -0.0010522983502596617, -0.0009908792562782764, -0.000929460278712213, -0.0008680412429384887, -0.0008066222071647644, -0.000745203229598701, -0.0006837841938249767, -0.0006223651580512524, -0.000560946180485189, -0.0004995271447114646, -0.0004381081089377403, -0.000376689073164016, -0.00031527006649412215, -0.0002538510598242283, -0.00019243202405050397, -0.00013101298827677965, -6.959398160688579e-05, -8.17497493699193e-06, 5.324406083673239e-05, 0.00011466308205854148, 0.00017608210328035057, 0.00023750112450215966, 0.00029892014572396874, 0.00036033918149769306, 0.0004217581881675869, 0.0004831771948374808, 0.0005445962306112051, 0.0006060152663849294, 0.0006674343021586537, 0.0007288532797247171, 0.0007902723154984415, 0.0008516913512721658, 0.0009131103288382292, 0.0009745293646119535, 0.0010359484003856778, 0.0010973673779517412, 0.0011587864719331264, 0.0012202054494991899, 0.0012816244270652533, 0.0013430435210466385, 0.001404462498612702, 0.0014658814761787653, 0.0015273005701601505]}, "gradients/model.lstm.bias_ih_l0": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 5.0, 2.0, 3.0, 3.0, 3.0, 13.0, 14.0, 30.0, 45.0, 112.0, 116.0, 54.0, 31.0, 24.0, 13.0, 16.0, 7.0, 3.0, 2.0, 4.0, 3.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-0.0006164732039906085, -0.00059268600307405, -0.0005688987439498305, -0.000545111543033272, -0.0005213243421167135, -0.000497537141200155, -0.00047374991117976606, -0.0004499626811593771, -0.0004261754802428186, -0.0004023882793262601, -0.00037860104930587113, -0.00035481381928548217, -0.00033102661836892366, -0.00030723941745236516, -0.0002834521874319762, -0.00025966495741158724, -0.00023587775649502873, -0.000212090541026555, -0.00018830332555808127, -0.00016451611008960754, -0.0001407288946211338, -0.00011694167915266007, -9.315446368418634e-05, -6.936724821571261e-05, -4.5580032747238874e-05, -2.1792817278765142e-05, 1.9943981897085905e-06, 2.5781613658182323e-05, 4.9568829126656055e-05, 7.335604459512979e-05, 9.714326006360352e-05, 0.00012093047553207725, 0.0001447177492082119, 0.00016850496467668563, 0.00019229218014515936, 0.0002160793956136331, 0.00023986661108210683, 0.00026365381199866533, 0.0002874410420190543, 0.00031122827203944325, 0.00033501547295600176, 0.00035880267387256026, 0.0003825899038929492, 0.0004063771339133382, 0.0004301643348298967, 0.0004539515357464552, 0.00047773876576684415, 0.0005015259957872331, 0.0005253131967037916, 0.0005491003976203501, 0.0005728875985369086, 0.000596674857661128, 0.0006204620585776865, 0.000644249259494245, 0.0006680365186184645, 0.000691823719535023, 0.0007156109204515815, 0.00073939812136814, 0.0007631853222846985, 0.0007869725814089179, 0.0008107597823254764, 0.0008345469832420349, 0.0008583342423662543, 0.0008821214432828128, 0.0009059086441993713]}, "gradients/model.lstm.bias_ih_l0_reverse": {"_type": "histogram", "values": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0, 3.0, 1.0, 3.0, 3.0, 4.0, 7.0, 7.0, 8.0, 15.0, 25.0, 42.0, 49.0, 100.0, 93.0, 40.0, 31.0, 25.0, 8.0, 8.0, 5.0, 7.0, 3.0, 2.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0], "bins": [-0.0006770301260985434, -0.000658657168969512, -0.0006402841536328197, -0.0006219111965037882, -0.0006035382393747568, -0.0005851652822457254, -0.000566792325116694, -0.0005484193097800016, -0.0005300463526509702, -0.0005116733955219388, -0.0004933003801852465, -0.00047492742305621505, -0.00045655446592718363, -0.0004381815087981522, -0.00041980852256529033, -0.00040143553633242846, -0.00038306257920339704, -0.0003646896220743656, -0.00034631663584150374, -0.00032794364960864186, -0.00030957069247961044, -0.000291197735350579, -0.00027282474911771715, -0.00025445176288485527, -0.00023607880575582385, -0.0002177058340748772, -0.00019933286239393055, -0.0001809598907129839, -0.00016258691903203726, -0.0001442139473510906, -0.00012584097567014396, -0.00010746800398919731, -8.909503230825067e-05, -7.072206062730402e-05, -5.234908894635737e-05, -3.397611726541072e-05, -1.5603145584464073e-05, 2.769826096482575e-06, 2.1142797777429223e-05, 3.951576945837587e-05, 5.788874113932252e-05, 7.626171282026917e-05, 9.463468450121582e-05, 0.00011300765618216246, 0.0001313806278631091, 0.00014975359954405576, 0.0001681265712250024, 0.00018649954290594906, 0.0002048725145868957, 0.00022324548626784235, 0.000241618457948789, 0.0002599914441816509, 0.0002783644013106823, 0.0002967373584397137, 0.0003151103446725756, 0.00033348333090543747, 0.0003518562880344689, 0.0003702292451635003, 0.0003886022313963622, 0.00040697521762922406, 0.0004253481747582555, 0.0004437211318872869, 0.0004620941181201488, 0.00048046710435301065, 0.0004988400614820421]}, "gradients/model.lstm.bias_hh_l0": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 5.0, 2.0, 3.0, 3.0, 3.0, 13.0, 14.0, 30.0, 45.0, 112.0, 116.0, 54.0, 31.0, 24.0, 13.0, 16.0, 7.0, 3.0, 2.0, 4.0, 3.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-0.0006164732039906085, -0.00059268600307405, -0.0005688987439498305, -0.000545111543033272, -0.0005213243421167135, -0.000497537141200155, -0.00047374991117976606, -0.0004499626811593771, -0.0004261754802428186, -0.0004023882793262601, -0.00037860104930587113, -0.00035481381928548217, -0.00033102661836892366, -0.00030723941745236516, -0.0002834521874319762, -0.00025966495741158724, -0.00023587775649502873, -0.000212090541026555, -0.00018830332555808127, -0.00016451611008960754, -0.0001407288946211338, -0.00011694167915266007, -9.315446368418634e-05, -6.936724821571261e-05, -4.5580032747238874e-05, -2.1792817278765142e-05, 1.9943981897085905e-06, 2.5781613658182323e-05, 4.9568829126656055e-05, 7.335604459512979e-05, 9.714326006360352e-05, 0.00012093047553207725, 0.0001447177492082119, 0.00016850496467668563, 0.00019229218014515936, 0.0002160793956136331, 0.00023986661108210683, 0.00026365381199866533, 0.0002874410420190543, 0.00031122827203944325, 0.00033501547295600176, 0.00035880267387256026, 0.0003825899038929492, 0.0004063771339133382, 0.0004301643348298967, 0.0004539515357464552, 0.00047773876576684415, 0.0005015259957872331, 0.0005253131967037916, 0.0005491003976203501, 0.0005728875985369086, 0.000596674857661128, 0.0006204620585776865, 0.000644249259494245, 0.0006680365186184645, 0.000691823719535023, 0.0007156109204515815, 0.00073939812136814, 0.0007631853222846985, 0.0007869725814089179, 0.0008107597823254764, 0.0008345469832420349, 0.0008583342423662543, 0.0008821214432828128, 0.0009059086441993713]}, "gradients/model.conv2.weight": {"_type": "histogram", "values": [2.0, 0.0, 0.0, 0.0, 1.0, 2.0, 2.0, 1.0, 0.0, 2.0, 5.0, 4.0, 3.0, 4.0, 2.0, 10.0, 5.0, 6.0, 18.0, 6.0, 8.0, 17.0, 15.0, 12.0, 15.0, 22.0, 19.0, 20.0, 22.0, 12.0, 9.0, 16.0, 15.0, 15.0, 18.0, 13.0, 16.0, 11.0, 18.0, 19.0, 9.0, 11.0, 12.0, 6.0, 6.0, 6.0, 10.0, 6.0, 5.0, 5.0, 2.0, 4.0, 2.0, 3.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0], "bins": [-0.0009430152713321149, -0.000909825146663934, -0.000876635021995753, -0.0008434448973275721, -0.0008102547726593912, -0.0007770646479912102, -0.0007438745815306902, -0.0007106844568625093, -0.0006774943321943283, -0.0006443042075261474, -0.0006111140828579664, -0.0005779239581897855, -0.0005447338335216045, -0.0005115437088534236, -0.0004783536132890731, -0.00044516348862089217, -0.00041197333484888077, -0.0003787832101806998, -0.0003455930855125189, -0.00031240296084433794, -0.000279212836176157, -0.00024602271150797606, -0.00021283261594362557, -0.00017964249127544463, -0.00014645236660726368, -0.00011326224193908274, -8.007212454685941e-05, -4.6882007154636085e-05, -1.3691882486455142e-05, 1.94982421817258e-05, 5.2688352297991514e-05, 8.587847696617246e-05, 0.00011906854342669249, 0.00015225866809487343, 0.00018544879276305437, 0.00021863890287932009, 0.00025182904209941626, 0.0002850191667675972, 0.0003182092623319477, 0.0003513993870001286, 0.00038458951166830957, 0.0004177796363364905, 0.00045096976100467145, 0.00048415985656902194, 0.0005173499812372029, 0.0005505401059053838, 0.0005837302305735648, 0.0006169203552417457, 0.0006501104799099267, 0.0006833006045781076, 0.0007164907292462885, 0.0007496808539144695, 0.0007828709785826504, 0.0008160611032508314, 0.0008492511697113514, 0.0008824412943795323, 0.0009156314190477133, 0.0009488215437158942, 0.0009820116683840752, 0.001015201793052256, 0.001048391917720437, 0.001081582042388618, 0.001114772167056799, 0.0011479622917249799, 0.0011811524163931608]}, "gradients/model.conv2.bias": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0], "bins": [-0.0004998873919248581, -0.00048273292486555874, -0.00046557842870242894, -0.0004484239616431296, -0.0004312694654799998, -0.00041411499842070043, -0.0003969605313614011, -0.0003798060351982713, -0.0003626515681389719, -0.0003454971010796726, -0.00032834260491654277, -0.0003111881378572434, -0.00029403367079794407, -0.00027687917463481426, -0.0002597247075755149, -0.00024257022596430033, -0.00022541574435308576, -0.00020826126274187118, -0.0001911067811306566, -0.00017395231407135725, -0.00015679783246014267, -0.0001396433508489281, -0.00012248888378962874, -0.00010533440217841417, -8.817992056719959e-05, -7.102543895598501e-05, -5.3870964620728046e-05, -3.6716486647492275e-05, -1.9562008674256504e-05, -2.4075270630419254e-06, 1.4746947272215039e-05, 3.1901421607472e-05, 4.905584501102567e-05, 6.621032662224025e-05, 8.336480095749721e-05, 0.00010051927529275417, 0.00011767375690396875, 0.00013482823851518333, 0.00015198270557448268, 0.00016913718718569726, 0.00018629166879691184, 0.00020344615040812641, 0.000220600632019341, 0.00023775509907864034, 0.0002549095661379397, 0.0002720640623010695, 0.00028921852936036885, 0.0003063729964196682, 0.000323527492582798, 0.00034068195964209735, 0.00035783645580522716, 0.0003749909228645265, 0.0003921454190276563, 0.00040929988608695567, 0.000426454353146255, 0.0004436088493093848, 0.00046076331636868417, 0.0004779177834279835, 0.0004950722795911133, 0.0005122267757542431, 0.000529381213709712, 0.0005465357098728418, 0.0005636902060359716, 0.0005808446439914405, 0.0005979991401545703]}, "gradients/model.conv1.weight": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 3.0, 2.0, 7.0, 9.0, 9.0, 15.0, 10.0, 23.0, 22.0, 24.0, 43.0, 51.0, 50.0, 73.0, 90.0, 111.0, 119.0, 111.0, 125.0, 159.0, 161.0, 185.0, 183.0, 174.0, 138.0, 179.0, 175.0, 189.0, 148.0, 155.0, 120.0, 108.0, 106.0, 72.0, 50.0, 39.0, 48.0, 38.0, 42.0, 15.0, 24.0, 18.0, 10.0, 8.0, 9.0, 4.0, 7.0, 4.0, 3.0, 1.0, 1.0, 1.0, 1.0, 0.0, 3.0], "bins": [-0.0015863567823544145, -0.001537778414785862, -0.0014892000472173095, -0.001440621679648757, -0.0013920431956648827, -0.0013434648280963302, -0.0012948864605277777, -0.0012463080929592252, -0.0011977297253906727, -0.0011491513578221202, -0.0011005729902535677, -0.0010519946226850152, -0.0010034162551164627, -0.0009548378293402493, -0.0009062594035640359, -0.0008576810359954834, -0.0008091026684269309, -0.0007605243008583784, -0.0007119459332898259, -0.0006633675075136125, -0.00061478913994506, -0.0005662107723765075, -0.0005176323466002941, -0.0004690539790317416, -0.0004204756114631891, -0.00037189724389463663, -0.0003233188472222537, -0.00027474045054987073, -0.00022616208298131824, -0.0001775837008608505, -0.0001290053187403828, -8.042692206799984e-05, -3.184867091476917e-05, 1.672971120569855e-05, 6.530809332616627e-05, 0.000113886475446634, 0.00016246485756710172, 0.00021104323968756944, 0.00025962162180803716, 0.0003082000184804201, 0.0003567783860489726, 0.0004053567536175251, 0.00045393515028990805, 0.000502513546962291, 0.0005510919145308435, 0.000599670282099396, 0.0006482487078756094, 0.0006968270754441619, 0.0007454054430127144, 0.0007939838105812669, 0.0008425621781498194, 0.0008911406039260328, 0.0009397189714945853, 0.0009882973972707987, 0.0010368757648393512, 0.0010854541324079037, 0.0011340324999764562, 0.0011826108675450087, 0.0012311892351135612, 0.0012797676026821136, 0.001328346086665988, 0.0013769244542345405, 0.001425502821803093, 0.0014740811893716455, 0.001522659556940198]}, "gradients/model.conv1.bias": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], "bins": [-0.0007094737375155091, -0.0006776030058972538, -0.0006457322742789984, -0.0006138614844530821, -0.0005819907528348267, -0.0005501200212165713, -0.000518249289598316, -0.0004863785579800606, -0.00045450779725797474, -0.00042263706563971937, -0.00039076630491763353, -0.00035889557329937816, -0.0003270248416811228, -0.00029515408095903695, -0.00026328334934078157, -0.00023141260317061096, -0.00019954185700044036, -0.00016767111083026975, -0.00013580036466009915, -0.00010392963304184377, -7.205888687167317e-05, -4.018814070150256e-05, -8.317409083247185e-06, 2.355333708692342e-05, 5.5424083257094026e-05, 8.729482942726463e-05, 0.00011916556832147762, 0.0001510363072156906, 0.00018290705338586122, 0.00021477779955603182, 0.0002466485311742872, 0.00027851929189637303, 0.0003103900235146284, 0.0003422607551328838, 0.0003741315158549696, 0.000406002247473225, 0.00043787300819531083, 0.0004697437398135662, 0.0005016144714318216, 0.000533485203050077, 0.0005653559928759933, 0.0005972267244942486, 0.000629097456112504, 0.0006609682459384203, 0.0006928389775566757, 0.000724709709174931, 0.0007565804407931864, 0.0007884511724114418, 0.0008203219040296972, 0.0008521926356479526, 0.0008840633672662079, 0.0009159340988844633, 0.0009478048887103796, 0.0009796756785362959, 0.0010115463519468904, 0.0010434171417728066, 0.001075287815183401, 0.0011071586050093174, 0.0011390292784199119, 0.0011709000682458282, 0.0012027707416564226, 0.001234641531482339, 0.0012665123213082552, 0.0012983829947188497, 0.001330253784544766]}, "NYU/confusion_matrix/0": {"_type": "image-file", "sha256": "d8ed2534aa2e58956c20f8f3ff4a9d1d893298b62bd34a2ad7231239f2d68f43", "size": 38450, "path": "media/images/NYU/confusion_matrix/0_0_d8ed2534aa2e58956c20.png", "format": "png", "width": 800, "height": 800}, "_step": 8, "NYU/f1_p_r_heatmap/0": {"_type": "image-file", "sha256": "28b0d4d32bb09382fc1b7ad67285b25ac5c5624b887bedf232041cd7015f7424", "size": 18308, "path": "media/images/NYU/f1_p_r_heatmap/0_1_28b0d4d32bb09382fc1b.png", "format": "png", "width": 1400, "height": 300}, "NYU/Loss/val": 0.7309685945510864, "NYU/Accuracy/val": 0.4280155599117279, "trainer/global_step": 2.0, "NYU/Loss/train": 0.7064915895462036, "NYU/Accuracy/train": 0.5, "NYU/confusion_matrix/1": {"_type": "image-file", "sha256": "d8ed2534aa2e58956c20f8f3ff4a9d1d893298b62bd34a2ad7231239f2d68f43", "size": 38450, "path": "media/images/NYU/confusion_matrix/1_3_d8ed2534aa2e58956c20.png", "format": "png", "width": 800, "height": 800}, "NYU/f1_p_r_heatmap/1": {"_type": "image-file", "sha256": "28b0d4d32bb09382fc1b7ad67285b25ac5c5624b887bedf232041cd7015f7424", "size": 18308, "path": "media/images/NYU/f1_p_r_heatmap/1_4_28b0d4d32bb09382fc1b.png", "format": "png", "width": 1400, "height": 300}, "NYU/confusion_matrix/2": {"_type": "image-file", "sha256": "d8ed2534aa2e58956c20f8f3ff4a9d1d893298b62bd34a2ad7231239f2d68f43", "size": 38450, "path": "media/images/NYU/confusion_matrix/2_6_d8ed2534aa2e58956c20.png", "format": "png", "width": 800, "height": 800}, "NYU/f1_p_r_heatmap/2": {"_type": "image-file", "sha256": "28b0d4d32bb09382fc1b7ad67285b25ac5c5624b887bedf232041cd7015f7424", "size": 18308, "path": "media/images/NYU/f1_p_r_heatmap/2_7_28b0d4d32bb09382fc1b.png", "format": "png", "width": 1400, "height": 300}}
\ No newline at end of file
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/logs/debug-internal.log b/Logs/wandb/run-20220704_225542-2b98ixbg/logs/debug-internal.log
new file mode 100644
index 0000000..c25ceac
--- /dev/null
+++ b/Logs/wandb/run-20220704_225542-2b98ixbg/logs/debug-internal.log
@@ -0,0 +1,974 @@
+2022-07-04 22:55:42,848 INFO    StreamThr :3530 [internal.py:wandb_internal():90] W&B internal server running at pid: 3530, started at: 2022-07-04 22:55:42.847610
+2022-07-04 22:55:42,851 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: status
+2022-07-04 22:55:42,853 DEBUG   SenderThread:3530 [sender.py:send_request():246] send_request: status
+2022-07-04 22:55:42,854 DEBUG   SenderThread:3530 [sender.py:send():232] send: header
+2022-07-04 22:55:42,855 INFO    WriterThread:3530 [datastore.py:open_for_write():75] open: Logs/wandb/run-20220704_225542-2b98ixbg/run-2b98ixbg.wandb
+2022-07-04 22:55:42,860 DEBUG   SenderThread:3530 [sender.py:send():232] send: run
+2022-07-04 22:55:42,862 INFO    SenderThread:3530 [sender.py:_maybe_setup_resume():489] checking resume status for None/QML-MNIST/2b98ixbg
+2022-07-04 22:55:43,782 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: check_version
+2022-07-04 22:55:43,786 INFO    SenderThread:3530 [dir_watcher.py:__init__():166] watching files in: Logs/wandb/run-20220704_225542-2b98ixbg/files
+2022-07-04 22:55:43,786 INFO    SenderThread:3530 [sender.py:_start_run_threads():811] run started: 2b98ixbg with start time 1656942942
+2022-07-04 22:55:43,787 DEBUG   SenderThread:3530 [sender.py:send():232] send: summary
+2022-07-04 22:55:43,787 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:55:43,787 DEBUG   SenderThread:3530 [sender.py:send_request():246] send_request: check_version
+2022-07-04 22:55:43,952 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: run_start
+2022-07-04 22:55:44,788 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-summary.json
+2022-07-04 22:55:44,975 DEBUG   HandlerThread:3530 [meta.py:__init__():35] meta init
+2022-07-04 22:55:44,975 DEBUG   HandlerThread:3530 [meta.py:__init__():49] meta init done
+2022-07-04 22:55:44,976 DEBUG   HandlerThread:3530 [meta.py:probe():209] probe
+2022-07-04 22:55:44,987 DEBUG   HandlerThread:3530 [meta.py:_setup_git():199] setup git
+2022-07-04 22:55:45,001 DEBUG   HandlerThread:3530 [meta.py:_setup_git():206] setup git done
+2022-07-04 22:55:45,002 DEBUG   HandlerThread:3530 [meta.py:_save_code():87] save code
+2022-07-04 22:55:45,008 DEBUG   HandlerThread:3530 [meta.py:_save_code():108] save code done
+2022-07-04 22:55:45,009 DEBUG   HandlerThread:3530 [meta.py:_save_patches():125] save patches
+2022-07-04 22:55:45,053 DEBUG   HandlerThread:3530 [meta.py:_save_patches():167] save patches done
+2022-07-04 22:55:45,054 DEBUG   HandlerThread:3530 [meta.py:_save_pip():53] save pip
+2022-07-04 22:55:45,055 DEBUG   HandlerThread:3530 [meta.py:_save_pip():67] save pip done
+2022-07-04 22:55:45,055 DEBUG   HandlerThread:3530 [meta.py:probe():247] probe done
+2022-07-04 22:55:45,196 DEBUG   SenderThread:3530 [sender.py:send():232] send: files
+2022-07-04 22:55:45,197 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-metadata.json with policy now
+2022-07-04 22:55:45,198 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file code/run.py with policy now
+2022-07-04 22:55:45,199 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file diff.patch with policy now
+2022-07-04 22:55:45,207 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: stop_status
+2022-07-04 22:55:45,213 DEBUG   SenderThread:3530 [sender.py:send_request():246] send_request: stop_status
+2022-07-04 22:55:45,599 DEBUG   SenderThread:3530 [sender.py:send():232] send: telemetry
+2022-07-04 22:55:45,600 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:55:45,600 DEBUG   SenderThread:3530 [sender.py:send():232] send: telemetry
+2022-07-04 22:55:45,600 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:55:45,601 WARNING SenderThread:3530 [sender.py:send_metric():904] Seen metric with glob (shouldn't happen)
+2022-07-04 22:55:45,787 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/diff.patch
+2022-07-04 22:55:45,788 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/code/run.py
+2022-07-04 22:55:45,789 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-metadata.json
+2022-07-04 22:55:45,789 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:55:45,789 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/requirements.txt
+2022-07-04 22:55:45,789 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/code
+2022-07-04 22:55:46,160 INFO    Thread-17 :3530 [upload_job.py:push():137] Uploaded file /tmp/tmp11fbbmfhwandb/22ofnz6v-code/run.py
+2022-07-04 22:55:46,361 INFO    Thread-18 :3530 [upload_job.py:push():137] Uploaded file /tmp/tmp11fbbmfhwandb/2abovatg-diff.patch
+2022-07-04 22:55:46,915 INFO    Thread-16 :3530 [upload_job.py:push():137] Uploaded file /tmp/tmp11fbbmfhwandb/1he5qgh8-wandb-metadata.json
+2022-07-04 22:55:47,788 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:55:49,787 DEBUG   SenderThread:3530 [sender.py:send():232] send: config
+2022-07-04 22:55:49,795 DEBUG   SenderThread:3530 [sender.py:send():232] send: telemetry
+2022-07-04 22:55:50,455 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,457 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,793 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,795 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,797 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,799 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,826 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,828 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,830 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,831 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,833 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,834 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,838 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,839 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,852 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,853 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,931 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:50,932 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,124 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,126 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,462 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,463 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,465 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,467 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,478 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,480 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,481 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,483 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,485 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,487 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,488 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,489 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,491 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,493 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,495 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,497 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,690 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,692 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:51,789 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:55:52,009 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,011 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,013 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,015 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,023 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,025 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,026 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,028 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,029 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,031 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,032 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,076 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,077 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,077 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,077 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,077 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,222 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,224 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,553 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,555 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,558 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,559 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,569 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,572 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,574 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,576 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,578 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,580 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,582 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,584 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,587 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,588 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,591 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,592 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,783 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:52,784 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,119 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,121 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,123 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,125 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,134 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,137 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,139 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,141 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,143 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,145 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,147 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,148 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,150 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,152 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,155 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,157 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,499 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,501 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,790 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:55:53,823 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,825 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,828 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,831 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,843 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,846 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,848 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,851 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,853 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,854 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,856 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,858 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,861 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,863 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,866 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:53,869 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,073 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,075 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,406 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,408 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,410 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,413 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,427 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,430 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,432 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,434 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,437 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,439 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,441 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,442 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,445 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,447 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,450 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,452 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,660 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,662 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,987 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,990 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,992 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:54,994 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,009 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,011 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,014 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,016 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,020 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,022 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,024 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,026 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,029 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,031 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,034 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,036 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,246 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,248 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,574 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,576 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,578 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,580 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,594 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,597 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,599 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,601 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,603 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,605 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,607 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,610 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,612 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,614 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,619 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,621 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,790 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:55:55,824 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:55,827 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,149 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,152 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,155 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,157 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,176 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,180 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,184 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,187 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,188 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,190 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,192 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,195 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,199 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,202 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,207 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,209 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,456 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,460 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,791 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,797 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,801 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,804 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,826 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,832 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,838 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,844 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,849 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,854 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,858 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,863 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,869 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,873 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,881 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:56,887 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,100 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,104 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,447 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,450 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,453 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,456 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,474 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,477 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,480 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,484 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,487 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,489 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,491 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,494 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,497 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,499 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,504 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,506 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,709 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,712 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:57,791 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:55:58,043 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,045 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,047 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,050 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,070 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,073 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,076 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,078 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,081 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,084 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,086 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,088 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,091 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,093 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,099 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:58,100 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:55:59,792 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:00,602 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: stop_status
+2022-07-04 22:56:00,602 DEBUG   SenderThread:3530 [sender.py:send_request():246] send_request: stop_status
+2022-07-04 22:56:01,793 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:02,288 DEBUG   SenderThread:3530 [sender.py:send():232] send: files
+2022-07-04 22:56:02,288 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file media/images/NYU/confusion_matrix/0_0_d8ed2534aa2e58956c20.png with policy now
+2022-07-04 22:56:02,289 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:02,295 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,295 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,295 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,295 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,295 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,296 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,297 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,297 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,297 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,297 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,297 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,297 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,297 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,297 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,297 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,298 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,298 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,298 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,298 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,298 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,298 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,298 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,298 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,299 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,299 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,299 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,299 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,299 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,299 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,299 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,300 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,300 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,300 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,300 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,300 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,300 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,300 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,301 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,301 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,301 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,301 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,301 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,301 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,301 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,301 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,301 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,301 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,302 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,302 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,302 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,302 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,302 DEBUG   SenderThread:3530 [sender.py:send():232] send: history
+2022-07-04 22:56:02,304 DEBUG   SenderThread:3530 [sender.py:send():232] send: summary
+2022-07-04 22:56:02,306 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:56:02,502 DEBUG   SenderThread:3530 [sender.py:send():232] send: files
+2022-07-04 22:56:02,502 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file media/images/NYU/f1_p_r_heatmap/0_1_28b0d4d32bb09382fc1b.png with policy now
+2022-07-04 22:56:02,503 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:02,567 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:02,569 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,569 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,569 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,570 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,570 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,570 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,570 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,570 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,570 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,570 DEBUG   SenderThread:3530 [sender.py:send():232] send: history
+2022-07-04 22:56:02,571 DEBUG   SenderThread:3530 [sender.py:send():232] send: summary
+2022-07-04 22:56:02,574 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:56:02,632 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:02,634 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,634 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:02,634 DEBUG   SenderThread:3530 [sender.py:send():232] send: history
+2022-07-04 22:56:02,634 DEBUG   SenderThread:3530 [sender.py:send():232] send: summary
+2022-07-04 22:56:02,638 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:56:02,793 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-summary.json
+2022-07-04 22:56:02,793 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/0_0_d8ed2534aa2e58956c20.png
+2022-07-04 22:56:02,793 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/0_1_28b0d4d32bb09382fc1b.png
+2022-07-04 22:56:02,793 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media
+2022-07-04 22:56:02,793 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap
+2022-07-04 22:56:02,794 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images
+2022-07-04 22:56:02,794 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix
+2022-07-04 22:56:02,794 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU
+2022-07-04 22:56:03,115 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,120 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,205 INFO    Thread-19 :3530 [upload_job.py:push():137] Uploaded file /tmp/tmp11fbbmfhwandb/qzn6s6dt-media/images/NYU/confusion_matrix/0_0_d8ed2534aa2e58956c20.png
+2022-07-04 22:56:03,517 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,522 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,529 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,535 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,556 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,563 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,569 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,576 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,582 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,588 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,594 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,600 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,607 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,614 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,622 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,627 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,698 INFO    Thread-20 :3530 [upload_job.py:push():137] Uploaded file /tmp/tmp11fbbmfhwandb/1md3jee8-media/images/NYU/f1_p_r_heatmap/0_1_28b0d4d32bb09382fc1b.png
+2022-07-04 22:56:03,793 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:03,865 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:03,869 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,251 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,254 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,257 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,260 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,281 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,285 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,289 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,292 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,295 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,297 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,299 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,302 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,305 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,307 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,312 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,316 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,545 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,549 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,933 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,935 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,938 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,940 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,960 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,963 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,967 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,969 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,971 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,974 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,976 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,979 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,983 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,986 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,993 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:04,997 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,243 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,245 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,653 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,655 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,660 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,662 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,682 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,687 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,690 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,693 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,697 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,700 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,704 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,707 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,711 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,716 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,722 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,726 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,794 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:05,932 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:05,935 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,258 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,307 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,307 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,307 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,308 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,308 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,308 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,308 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,308 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,308 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,308 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,309 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,309 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,309 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,309 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,311 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,524 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,527 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,864 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,866 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,868 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,870 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,889 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,892 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,894 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,896 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,898 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,900 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,902 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,904 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,907 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,908 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,914 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:06,916 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,129 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,131 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,468 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,470 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,474 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,476 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,495 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,498 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,501 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,503 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,505 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,507 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,509 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,511 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,516 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,518 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,523 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,527 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,781 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,783 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:07,795 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:08,103 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,105 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,108 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,110 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,129 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,132 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,134 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,136 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,138 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,140 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,142 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,144 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,148 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,150 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,155 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,157 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,360 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,362 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,683 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,685 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,688 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,691 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,709 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,712 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,717 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,719 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,721 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,724 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,726 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,728 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,731 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,732 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,738 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,740 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,945 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:08,947 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,275 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,277 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,281 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,283 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,302 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,305 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,308 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,310 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,314 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,316 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,318 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,321 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,326 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,328 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,333 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,335 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,549 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,552 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,796 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:09,896 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,897 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,899 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,902 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,920 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,924 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,927 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,930 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,932 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,934 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,937 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,939 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,942 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,945 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,951 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:09,954 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,159 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,161 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,488 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,491 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,493 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,495 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,514 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,517 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,521 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,523 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,525 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,527 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,529 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,531 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,533 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,536 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,540 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,542 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,751 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:10,754 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,132 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,133 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,135 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,138 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,156 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,159 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,161 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,163 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,165 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,168 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,170 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,172 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,175 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,177 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,184 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,188 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:11,797 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:13,188 DEBUG   SenderThread:3530 [sender.py:send():232] send: stats
+2022-07-04 22:56:13,262 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:13,445 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:13,490 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:13,525 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:13,574 DEBUG   SenderThread:3530 [sender.py:send():232] send: files
+2022-07-04 22:56:13,574 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file media/images/NYU/confusion_matrix/1_3_d8ed2534aa2e58956c20.png with policy now
+2022-07-04 22:56:13,574 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,574 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,575 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,575 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,575 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,575 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,576 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,577 DEBUG   SenderThread:3530 [sender.py:send():232] send: history
+2022-07-04 22:56:13,579 DEBUG   SenderThread:3530 [sender.py:send():232] send: summary
+2022-07-04 22:56:13,582 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:56:13,582 DEBUG   SenderThread:3530 [sender.py:send():232] send: files
+2022-07-04 22:56:13,582 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file media/images/NYU/f1_p_r_heatmap/1_4_28b0d4d32bb09382fc1b.png with policy now
+2022-07-04 22:56:13,583 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,583 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,583 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,583 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,583 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,584 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,584 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:13,584 DEBUG   SenderThread:3530 [sender.py:send():232] send: history
+2022-07-04 22:56:13,587 DEBUG   SenderThread:3530 [sender.py:send():232] send: summary
+2022-07-04 22:56:13,589 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:56:13,590 DEBUG   SenderThread:3530 [sender.py:send():232] send: history
+2022-07-04 22:56:13,590 DEBUG   SenderThread:3530 [sender.py:send():232] send: summary
+2022-07-04 22:56:13,592 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:56:13,798 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:13,798 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/config.yaml
+2022-07-04 22:56:13,798 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-summary.json
+2022-07-04 22:56:13,799 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/1_4_28b0d4d32bb09382fc1b.png
+2022-07-04 22:56:13,799 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/1_3_d8ed2534aa2e58956c20.png
+2022-07-04 22:56:13,799 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap
+2022-07-04 22:56:13,799 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix
+2022-07-04 22:56:13,958 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:13,962 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,296 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,300 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,303 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,307 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,327 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,330 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,333 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,335 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,337 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,339 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,341 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,344 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,348 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,350 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,356 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,357 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,578 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,580 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,736 INFO    Thread-21 :3530 [upload_job.py:push():137] Uploaded file /tmp/tmp11fbbmfhwandb/3kfrb5m9-media/images/NYU/confusion_matrix/1_3_d8ed2534aa2e58956c20.png
+2022-07-04 22:56:14,906 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,909 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,912 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,914 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,934 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,937 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,940 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,943 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,945 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,947 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,949 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,951 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,954 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,956 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,960 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:14,962 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,073 INFO    Thread-22 :3530 [upload_job.py:push():137] Uploaded file /tmp/tmp11fbbmfhwandb/jnrkbnuo-media/images/NYU/f1_p_r_heatmap/1_4_28b0d4d32bb09382fc1b.png
+2022-07-04 22:56:15,169 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,173 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,569 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,572 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,577 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,581 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,599 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,603 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,607 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,612 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,615 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,618 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,622 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,624 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,628 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,630 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,635 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,638 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,799 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:15,862 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:15,864 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,259 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: stop_status
+2022-07-04 22:56:16,259 DEBUG   SenderThread:3530 [sender.py:send_request():246] send_request: stop_status
+2022-07-04 22:56:16,264 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,266 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,270 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,273 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,292 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,295 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,298 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,300 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,303 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,306 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,310 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,313 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,317 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,320 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,327 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,387 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,551 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,554 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,886 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,889 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,891 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,892 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,911 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,913 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,916 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,919 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,923 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,925 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,927 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,929 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,931 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,933 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,937 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:16,939 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,147 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,149 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,484 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,486 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,490 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,492 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,511 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,513 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,516 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,518 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,521 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,523 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,525 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,527 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,529 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,531 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,536 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,538 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,748 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,751 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:17,800 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:18,128 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,130 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,132 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,134 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,153 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,156 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,159 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,161 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,163 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,165 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,167 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,169 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,171 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,173 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,180 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,182 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,396 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,400 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,769 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,771 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,774 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,777 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,795 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,798 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,801 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,803 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,805 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,808 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,810 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,813 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,816 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,818 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,825 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:18,828 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,049 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,052 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,421 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,423 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,427 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,429 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,448 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,452 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,454 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,456 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,458 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,460 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,461 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,463 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,466 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,469 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,475 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,479 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,692 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,695 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:19,801 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:20,035 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,038 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,042 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,044 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,063 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,066 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,068 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,070 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,074 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,077 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,079 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,081 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,084 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,087 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,091 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,094 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,309 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,312 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,632 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,634 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,636 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,639 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,657 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,660 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,662 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,664 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,666 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,668 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,670 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,672 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,675 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,677 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,682 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,684 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,902 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:20,904 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,228 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,230 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,233 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,237 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,258 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,263 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,266 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,269 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,271 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,274 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,277 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,279 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,282 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,286 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,292 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,294 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,680 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,726 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:21,802 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:22,021 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,023 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,025 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,027 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,046 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,049 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,052 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,054 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,056 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,058 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,060 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,062 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,065 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,067 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,072 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:22,077 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:23,802 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:24,207 DEBUG   SenderThread:3530 [sender.py:send():232] send: files
+2022-07-04 22:56:24,207 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file media/images/NYU/confusion_matrix/2_6_d8ed2534aa2e58956c20.png with policy now
+2022-07-04 22:56:24,207 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:24,212 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,212 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,212 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,212 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,212 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,212 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,212 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,212 DEBUG   SenderThread:3530 [sender.py:send():232] send: history
+2022-07-04 22:56:24,214 DEBUG   SenderThread:3530 [sender.py:send():232] send: summary
+2022-07-04 22:56:24,216 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:56:24,388 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:24,389 DEBUG   SenderThread:3530 [sender.py:send():232] send: files
+2022-07-04 22:56:24,389 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file media/images/NYU/f1_p_r_heatmap/2_7_28b0d4d32bb09382fc1b.png with policy now
+2022-07-04 22:56:24,427 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:24,428 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,430 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,430 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,430 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,430 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,430 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,430 DEBUG   SenderThread:3530 [sender.py:send():232] send: metric
+2022-07-04 22:56:24,430 DEBUG   SenderThread:3530 [sender.py:send():232] send: history
+2022-07-04 22:56:24,431 DEBUG   SenderThread:3530 [sender.py:send():232] send: summary
+2022-07-04 22:56:24,434 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:56:24,487 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:24,489 DEBUG   SenderThread:3530 [sender.py:send():232] send: history
+2022-07-04 22:56:24,489 DEBUG   SenderThread:3530 [sender.py:send():232] send: summary
+2022-07-04 22:56:24,494 INFO    SenderThread:3530 [sender.py:_save_file():946] saving file wandb-summary.json with policy end
+2022-07-04 22:56:24,803 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/wandb-summary.json
+2022-07-04 22:56:24,804 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix/2_6_d8ed2534aa2e58956c20.png
+2022-07-04 22:56:24,804 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_created():213] file/dir created: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap/2_7_28b0d4d32bb09382fc1b.png
+2022-07-04 22:56:24,805 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/confusion_matrix
+2022-07-04 22:56:24,805 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/media/images/NYU/f1_p_r_heatmap
+2022-07-04 22:56:24,938 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:24,944 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,290 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,295 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,301 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,305 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,327 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,334 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,340 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,344 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,346 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,348 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,350 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,352 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,358 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,361 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,366 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,369 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,422 INFO    Thread-23 :3530 [upload_job.py:push():137] Uploaded file /tmp/tmp11fbbmfhwandb/2um9hgxx-media/images/NYU/confusion_matrix/2_6_d8ed2534aa2e58956c20.png
+2022-07-04 22:56:25,509 INFO    Thread-24 :3530 [upload_job.py:push():137] Uploaded file /tmp/tmp11fbbmfhwandb/sqspcwqw-media/images/NYU/f1_p_r_heatmap/2_7_28b0d4d32bb09382fc1b.png
+2022-07-04 22:56:25,592 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,595 DEBUG   HandlerThread:3530 [handler.py:handle_request():141] handle_request: partial_history
+2022-07-04 22:56:25,804 INFO    Thread-12 :3530 [dir_watcher.py:_on_file_modified():226] file/dir modified: Logs/wandb/run-20220704_225542-2b98ixbg/files/output.log
+2022-07-04 22:56:25,872 DEBUG   SenderThread:3530 [sender.py:send():232] send: config
+2022-07-04 22:56:26,027 INFO    MainThread:3530 [internal.py:handle_exit():80] Internal process exited
+2022-07-04 22:56:26,028 WARNING MsgRouterThr:3530 [router.py:message_loop():73] EOFError seen in message_loop
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/logs/debug.log b/Logs/wandb/run-20220704_225542-2b98ixbg/logs/debug.log
new file mode 100644
index 0000000..b0f7dea
--- /dev/null
+++ b/Logs/wandb/run-20220704_225542-2b98ixbg/logs/debug.log
@@ -0,0 +1,27 @@
+2022-07-04 22:55:42,837 INFO    MainThread:3487 [wandb_setup.py:_flush():75] Loading settings from /home/sol/.config/wandb/settings
+2022-07-04 22:55:42,837 INFO    MainThread:3487 [wandb_setup.py:_flush():75] Loading settings from /home/sol/git/QML2/wandb/settings
+2022-07-04 22:55:42,837 INFO    MainThread:3487 [wandb_setup.py:_flush():75] Loading settings from environment variables: {'_require_service': 'True'}
+2022-07-04 22:55:42,837 INFO    MainThread:3487 [wandb_setup.py:_flush():75] Inferring run settings from compute environment: {'program_relpath': 'run.py', 'program': '/home/sol/git/QML2/run.py'}
+2022-07-04 22:55:42,837 INFO    MainThread:3487 [wandb_init.py:_log_setup():437] Logging user logs to Logs/wandb/run-20220704_225542-2b98ixbg/logs/debug.log
+2022-07-04 22:55:42,838 INFO    MainThread:3487 [wandb_init.py:_log_setup():438] Logging internal logs to Logs/wandb/run-20220704_225542-2b98ixbg/logs/debug-internal.log
+2022-07-04 22:55:42,838 INFO    MainThread:3487 [wandb_init.py:init():471] calling init triggers
+2022-07-04 22:55:42,838 INFO    MainThread:3487 [wandb_init.py:init():474] wandb.init called with sweep_config: {}
+config: {}
+2022-07-04 22:55:42,838 INFO    MainThread:3487 [wandb_init.py:init():524] starting backend
+2022-07-04 22:55:42,841 INFO    MainThread:3487 [backend.py:_multiprocessing_setup():97] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
+2022-07-04 22:55:42,843 INFO    MainThread:3487 [wandb_init.py:init():533] backend started and connected
+2022-07-04 22:55:42,847 INFO    MainThread:3487 [wandb_init.py:init():597] updated telemetry
+2022-07-04 22:55:42,859 INFO    MainThread:3487 [wandb_init.py:init():628] communicating run to backend with 30 second timeout
+2022-07-04 22:55:43,778 INFO    MainThread:3487 [wandb_run.py:_on_init():1923] communicating current version
+2022-07-04 22:55:43,946 INFO    MainThread:3487 [wandb_run.py:_on_init():1927] got version response upgrade_message: "wandb version 0.12.20 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2022-07-04 22:55:43,946 INFO    MainThread:3487 [wandb_init.py:init():659] starting run threads in backend
+2022-07-04 22:55:45,206 INFO    MainThread:3487 [wandb_run.py:_console_start():1897] atexit reg
+2022-07-04 22:55:45,207 INFO    MainThread:3487 [wandb_run.py:_redirect():1770] redirect: SettingsConsole.WRAP
+2022-07-04 22:55:45,208 INFO    MainThread:3487 [wandb_run.py:_redirect():1807] Wrapping output streams.
+2022-07-04 22:55:45,210 INFO    MainThread:3487 [wandb_run.py:_redirect():1831] Redirects installed.
+2022-07-04 22:55:45,210 INFO    MainThread:3487 [wandb_init.py:init():684] run started, returning control to user process
+2022-07-04 22:55:49,786 INFO    MainThread:3487 [wandb_run.py:_config_callback():1131] config_cb None None {'opt/optimizer': 'Adam', 'opt/lr': 1e-05, 'net/model': 'Simple_QHN', 'net/num_classes': 2, 'net/LSTM/input_size': 116, 'net/LSTM/hidden_size': 128, 'net/LSTM/num_layers': 1, 'net/LSTM/bidirectional': True, 'net/Simple_QHN/n_qubits': 2, 'net/Simple_QHN/shift': 0.6, 'net/Simple_QHN/lstm_hidden': 128, 'net/Simple_QHN/linear_out': 64, 'net/Simple_QHN/backend': 'aer_simulator', 'net/hybrid/n_qubits': 3, 'net/hybrid/simulator': 'aer_simulator', 'net/hybrid/shift': 0.5, 'net/hybrid/shots': 100, 'net/hybrid/dense_type': 2, 'net/roi_rank': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115], 'inputs': 'None'}
+2022-07-04 22:55:49,794 INFO    MainThread:3487 [wandb_watch.py:watch():47] Watching
+2022-07-04 22:56:25,871 INFO    MainThread:3487 [wandb_run.py:_config_callback():1131] config_cb None None {'opt/optimizer': 'Adam', 'opt/lr': 1e-05, 'net/model': 'Simple_QHN', 'net/num_classes': 2, 'net/LSTM/input_size': 116, 'net/LSTM/hidden_size': 128, 'net/LSTM/num_layers': 1, 'net/LSTM/bidirectional': True, 'net/Simple_QHN/n_qubits': 2, 'net/Simple_QHN/shift': 0.6, 'net/Simple_QHN/lstm_hidden': 128, 'net/Simple_QHN/linear_out': 64, 'net/Simple_QHN/backend': 'aer_simulator', 'net/hybrid/n_qubits': 3, 'net/hybrid/simulator': 'aer_simulator', 'net/hybrid/shift': 0.5, 'net/hybrid/shots': 100, 'net/hybrid/dense_type': 2, 'net/roi_rank': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115], 'inputs': 'None'}
+2022-07-04 22:56:26,162 WARNING MsgRouterThr:3487 [router.py:message_loop():76] message_loop has been closed
diff --git a/Logs/wandb/run-20220704_225542-2b98ixbg/run-2b98ixbg.wandb b/Logs/wandb/run-20220704_225542-2b98ixbg/run-2b98ixbg.wandb
new file mode 100644
index 0000000..3bc50fe
Binary files /dev/null and b/Logs/wandb/run-20220704_225542-2b98ixbg/run-2b98ixbg.wandb differ
diff --git a/run.py b/run.py
index c8e1ccf..eb6b8ab 100644
--- a/run.py
+++ b/run.py
@@ -1,7 +1,7 @@
 from omegaconf import OmegaConf
 from pathlib import Path
-
-from src.runners import S_Runner, MNIST_Runner
+from qiskit import IBMQ
+from src.runners import S_Runner
 
 CONFIG_DIR = Path("Configs")
 
@@ -11,7 +11,7 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
     cfg = OmegaConf.merge(cfg, model_params)
     cfg.merge_with_cli()
 
-    runner = MNIST_Runner(
+    runner = S_Runner(
         log=cfg.log,
         optimizer=cfg.optimizer,
         loader=cfg.loader,
@@ -22,4 +22,6 @@ def main(cfg=OmegaConf.load(CONFIG_DIR / "config.yaml")) -> None:
 
 
 if __name__ == "__main__":
-    main(cfg=OmegaConf.load(CONFIG_DIR / "config MNIST.yaml"))
+    IBMQ.save_account("39e7cec1ae590142d86caa525b2bf85bc5fbc8893ba6b8a8bd7de88582afdff10c0ec41104c54851efe419e0cf125528184d9c01091878a8617ca41cb2754176")
+    IBMQ.providers()
+    main()
diff --git a/src/data/__pycache__/data_etl.cpython-38.pyc b/src/data/__pycache__/data_etl.cpython-38.pyc
index 288d14c..b2c51c9 100644
Binary files a/src/data/__pycache__/data_etl.cpython-38.pyc and b/src/data/__pycache__/data_etl.cpython-38.pyc differ
diff --git a/src/data/__pycache__/loader.cpython-38.pyc b/src/data/__pycache__/loader.cpython-38.pyc
index eddf358..7e47391 100644
Binary files a/src/data/__pycache__/loader.cpython-38.pyc and b/src/data/__pycache__/loader.cpython-38.pyc differ
diff --git a/src/data/__pycache__/sampling.cpython-38.pyc b/src/data/__pycache__/sampling.cpython-38.pyc
index a2cbaf3..206bfbf 100644
Binary files a/src/data/__pycache__/sampling.cpython-38.pyc and b/src/data/__pycache__/sampling.cpython-38.pyc differ
diff --git a/src/data/data_etl.py b/src/data/data_etl.py
index 08700be..dc691b0 100755
--- a/src/data/data_etl.py
+++ b/src/data/data_etl.py
@@ -137,24 +137,6 @@ def chunks(lst, n):
     return [lst[i : i + div] for i in range(0, len(lst), div)]
 
 
-@dataclass
-class Load_MNIST:
-    def __init__(self, is_train:Boolean):
-        if is_train:
-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=True, download=True,
-                         transform=transforms.Compose([transforms.ToTensor()]))
-        else:
-            self.dataset = dataset.MNIST(root='mnist_dataset/raw', train=False, download=True,
-                        transform=transforms.Compose([transforms.ToTensor()]))
-                        
-    def get_samples(self, n_samples:int):
-        idx = []
-        for i in range(10):
-            idx.append(np.where(self.dataset.targets == i)[0][:n_samples])#각 target에 해당되는 번호를 저장한다. 
-        idx = np.array(idx).reshape(-1)
-        self.dataset.data = self.dataset.data[idx]
-        self.dataset.targets = self.dataset.targets[idx]
-        return self.dataset
 
 
 
diff --git a/src/data/loader.py b/src/data/loader.py
index 288a3b3..158cbdc 100644
--- a/src/data/loader.py
+++ b/src/data/loader.py
@@ -5,25 +5,8 @@ from typing import Union, List
 import torch
 from torch.utils.data import Dataset
 
-from src.data import Load, SITES_DICT, Load_MNIST
-
-class MNISTDataset(Dataset):
-    def __init__(self, n_samples: int, is_train: Boolean) -> None:
-        Load = Load_MNIST(is_train)
-        if n_samples >0:
-            self.data = Load.get_samples(n_samples=n_samples).data
-            self.labels = Load.get_samples(n_samples=n_samples).targets  
-        else:  
-            self.data = Load.dataset.data
-            self.labels = Load.dataset.targets
+from src.data import Load, SITES_DICT
 
-    def __len__(self):
-        return len(self.labels)
-    
-    def __getitem__(self, index: int):
-        data = self.data[index]
-        label = self.labels[index]
-        return data, label 
   
 class ROIDataset(Dataset):
     def __init__( self, site: Union[List, str]) -> None:
diff --git a/src/datamodules/__pycache__/datamodule.cpython-38.pyc b/src/datamodules/__pycache__/datamodule.cpython-38.pyc
index 6f06b1a..6ad4afb 100644
Binary files a/src/datamodules/__pycache__/datamodule.cpython-38.pyc and b/src/datamodules/__pycache__/datamodule.cpython-38.pyc differ
diff --git a/src/datamodules/datamodule.py b/src/datamodules/datamodule.py
index 4fcab3d..0bfab90 100644
--- a/src/datamodules/datamodule.py
+++ b/src/datamodules/datamodule.py
@@ -7,64 +7,23 @@ from torch.utils.data import DataLoader, Subset
 from pytorch_lightning import LightningDataModule
 from src.data import collate_fn, SamplerFactory
 
+
 @dataclass
-class MNISTDataModule(LightningDataModule):
-    
-    def __init__(self,data: Dict,loader: Dict,dataset: Dict):
+class DataModule(LightningDataModule):
+    def __init__(self, data: Dict, loader: Dict, dataset: Dict):
         super().__init__()
-        self.prepare_data_per_node = True
         self.data = data
         self.loader = loader
         self.dataset = dataset
-    def setup(self, stage: Optional[str] = None):
-        if stage in ("fit", None):
-            self.train_dataset = self.dataset(n_samples = self.loader.samples.train_sample, is_train=True)
-            self.val_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
-
-        if stage in ("test", None):
-            self.test_dataset = self.dataset(n_samples = self.loader.samples.eval_sample, is_train=False)
-    
-    def train_dataloader(self):
-        conf = deepcopy(self.loader.train)
-        batch_size = conf.pop("batch_size")
-        conf.shuffle = False
-        # return DataLoader(self.train_dataset, **conf, collate_fn=collate_fn)
-
-        return DataLoader(
-            self.train_dataset,
-            **conf,
-            collate_fn=collate_fn,
-            batch_sampler=SamplerFactory().get(
-                class_idxs=[
-                    np.where(self.train_dataset.labels == i)[0].tolist()
-                    for i in range(10)
-                ],
-                batch_size=batch_size,
-                n_batches=len(self.train_dataset)//batch_size + 5,
-                alpha=1.0,
-                kind="random",
-            ),
-        )
-
-    def val_dataloader(self):
-        return DataLoader(self.val_dataset, **self.loader.eval, collate_fn=collate_fn)
-
-    def test_dataloader(self):
-        return DataLoader(self.test_dataset, **self.loader.eval, collate_fn=collate_fn)
-
-@dataclass
-class DataModule(LightningDataModule):
-    data: Dict
-    loader: Dict
-    dataset: Dict
+        self.prepare_data_per_node = True
 
     def setup(self, stage: Optional[str] = None):
         if stage in ("fit", None):
-            self.train_dataset = self.dataset(is_train = True, site = self.data.train_site)
-            self.val_dataset = self.dataset(is_train = False, site = self.data.train_site)
+            self.train_dataset = self.dataset(site = self.data.train_site)
+            self.val_dataset = self.dataset(site = self.data.train_site)
 
         if stage in ("test", None):
-            self.test_dataset = self.dataset(is_train = False, site = self.data.train_site)
+            self.test_dataset = self.dataset(site = self.data.train_site)
 
     def train_dataloader(self):
         conf = deepcopy(self.loader.train)
diff --git a/src/layers/__pycache__/__init__.cpython-38.pyc b/src/layers/__pycache__/__init__.cpython-38.pyc
index 31453e2..a73c856 100644
Binary files a/src/layers/__pycache__/__init__.cpython-38.pyc and b/src/layers/__pycache__/__init__.cpython-38.pyc differ
diff --git a/src/layers/__pycache__/quantum_layers.cpython-38.pyc b/src/layers/__pycache__/quantum_layers.cpython-38.pyc
index 4f219f9..90226a0 100644
Binary files a/src/layers/__pycache__/quantum_layers.cpython-38.pyc and b/src/layers/__pycache__/quantum_layers.cpython-38.pyc differ
diff --git a/src/layers/quantum_layers.py b/src/layers/quantum_layers.py
index 976aa6a..f2507f2 100644
--- a/src/layers/quantum_layers.py
+++ b/src/layers/quantum_layers.py
@@ -4,7 +4,7 @@ import torch
 from torch import nn
 from torch.autograd import Function
 import numpy as np
-
+from qiskit import IBMQ
 
 class QuantumCircuit:
     """
@@ -35,7 +35,7 @@ class QuantumCircuit:
         for theta, qubit in zip(self.theta, self.all_qubits):
             self._circuit.ry(theta, qubit)
 
-        for i in [2,3,4,5]:
+        for i in range(2, self.n_qubit):
             self._circuit.cx(0,i)
 
         self._circuit.measure_all()
@@ -45,6 +45,7 @@ class QuantumCircuit:
         self.shots = shots
 
     def run(self, thetas):
+        
         t_qc = transpile(self._circuit, self.backend)
         # qobj = assemble(
         #     t_qc,
@@ -52,29 +53,39 @@ class QuantumCircuit:
         #     parameter_binds=[{self.theta: theta} for theta in thetas],
         # )
         # --- multi qubit -- #
-        qobj = assemble(
-            t_qc,
-            shots=self.shots,
-            parameter_binds=[{self.theta[i]: thetas[i] for i in range(len(thetas))}],
-        )
-        job = self.backend.run(qobj)
-        result = job.result().get_counts()
+        circuit_list = []
+        header = {}
+        for i in range(len(thetas)):
+            header[thetas[i]] = {i:i}
+        for theta in thetas:
+            qobj = assemble(
+                t_qc,
+                shots=self.shots,
+                parameter_binds=[{self.theta[i]: float(theta[i].cpu()) for i in range(len(theta))}],
+                qobj_header = header
+            )
+            circuit_list.append(qobj)
+
+        job = qiskit.execute(circuit_list, self.backend)
+        results = job.result().get_counts()
 
-        counts = np.array(list(result.values()))
+        #counts = np.array(list(result.values()))
         # states = np.array(list(result.keys())).astype(float)
         # --- multi qubit -- #
         possible_states = []
         for s in range(2**(self.n_qubit)):
             possible_states.append(format(s, "b").zfill(self.n_qubit))
-       
         states = []
-        for i in possible_states:
-            try:
-                states.append(result[i])
-            except:
-                states.append(0)   
-        states = np.array(states, dtype=np.float64)
-         
+        for result in results:
+            state = []
+            for i in possible_states:
+                try:
+                    state.append(result[i])
+                except:
+                    state.append(0)   
+            state = np.array(state, dtype=np.float64)
+            states.append(state)
+        states = np.array(states) 
         return states/self.shots#기댓값을 출력
 
 
@@ -90,11 +101,9 @@ class HybridFunction(Function):
         # expectation_z = ctx.quantum_circuit.run(input[0].tolist())
         # result = torch.tensor([expectation_z])
         # -- multi qubit -- #
-        expectation_z = [
-            torch.Tensor(ctx.quantum_circuit.run(input[i].tolist()))
-            for i in range(input.size(0))
-        ]
-        result = torch.stack(expectation_z, axis=0)
+        result = torch.Tensor(ctx.quantum_circuit.run(input))
+        
+        #result = torch.stack(expectation_z, axis=0)
         ctx.save_for_backward(input, result)
         
         return result
@@ -138,8 +147,10 @@ class Hybrid(nn.Module):
         self, n_qubits=2, backend="aer_simulator", shots=100, shift=0.6
     ):
         super(Hybrid, self).__init__()
+        provider = IBMQ.load_account()
+        backend = provider.get_backend(backend)
         self.quantum_circuit = QuantumCircuit(
-            n_qubits, qiskit.Aer.get_backend(backend), shots
+            n_qubits, backend, shots
         )
         self.shift = shift
 
diff --git a/src/models/__pycache__/__init__.cpython-38.pyc b/src/models/__pycache__/__init__.cpython-38.pyc
index f5a0d8c..9643058 100644
Binary files a/src/models/__pycache__/__init__.cpython-38.pyc and b/src/models/__pycache__/__init__.cpython-38.pyc differ
diff --git a/src/models/__pycache__/qhn.cpython-38.pyc b/src/models/__pycache__/qhn.cpython-38.pyc
index 56e2a1f..0930137 100644
Binary files a/src/models/__pycache__/qhn.cpython-38.pyc and b/src/models/__pycache__/qhn.cpython-38.pyc differ
diff --git a/src/models/qhn.py b/src/models/qhn.py
index c74fec4..2bc3845 100644
--- a/src/models/qhn.py
+++ b/src/models/qhn.py
@@ -26,13 +26,13 @@ class Simple_QHN(nn.Module):
         self.fc1 = nn.Linear(256, params.linear_out)
         self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
         self.hybrid = Hybrid(
-            params.n_qubits,
-            qiskit.Aer.get_backend("aer_simulator"),
-            100,
-            shift=params.shift,
-            is_cnot=params.is_cnot,
+            n_qubits = params.n_qubits,
+            backend = params.backend,
+            shots = params.shots,
+            shift = params.shift,
+            
         )
-        self.fc3 = nn.Linear(params.n_qubits * 2, 2)
+        self.fc3 = nn.Linear(2**params.n_qubits, 2)
         self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
 
     def forward(self, x):
@@ -50,40 +50,7 @@ class Simple_QHN(nn.Module):
         x = self.fc2(x)
         x = torch.tanh(x) * torch.ones_like(x) * torch.tensor(np.pi / 2)
         x = self.hybrid(x).to(self.device)
-        x = torch.cat((x, 1 - x), -1)
         x = F.softmax(self.fc3(x), dim=1)
         return x
 
 
-class MNIST_QHN(nn.Module):
-    def __init__(self, params: Optional[Dict] = None, *args, **kwargs) -> None:
-        super(MNIST_QHN, self).__init__()
-        params = params.MNIST_QHN
-        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
-        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
-        self.dropout = nn.Dropout2d()
-        self.fc1 = nn.Linear(256, params.linear_out)
-        self.fc2 = nn.Linear(params.linear_out, params.n_qubits)
-        self.hybrid = Hybrid(
-            params.n_qubits,
-            backend = "aer_simulator",
-            shots = params.shots,
-            shift=params.shift,
-        )
-        self.fc3 = nn.Linear(2**params.n_qubits, 10)
-        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
-
-    def forward(self, x):
-        x = x.unsqueeze(1)
-        x = F.relu(self.conv1(x))
-        x = F.max_pool2d(x, 2)
-        x = F.relu(self.conv2(x))
-        x = F.max_pool2d(x, 2)
-        
-        x = x.view(x.shape[0],-1)
-        x = F.relu(self.fc1(x))
-        x = self.fc2(x)
-        x = torch.tanh(x) * torch.ones_like(x) * torch.tensor(np.pi / 2)
-        x = self.hybrid(x).to(self.device)
-        x = F.softmax(self.fc3(x), dim=1)
-        return x
diff --git a/src/runners/__pycache__/base.cpython-38.pyc b/src/runners/__pycache__/base.cpython-38.pyc
index 6fae27c..1159418 100644
Binary files a/src/runners/__pycache__/base.cpython-38.pyc and b/src/runners/__pycache__/base.cpython-38.pyc differ
diff --git a/src/runners/__pycache__/runner.cpython-38.pyc b/src/runners/__pycache__/runner.cpython-38.pyc
index 9813d76..a0a9e69 100644
Binary files a/src/runners/__pycache__/runner.cpython-38.pyc and b/src/runners/__pycache__/runner.cpython-38.pyc differ
diff --git a/src/runners/runner.py b/src/runners/runner.py
index 86b61a5..5fb03f3 100644
--- a/src/runners/runner.py
+++ b/src/runners/runner.py
@@ -6,8 +6,8 @@ from typing import Optional
 from copy import deepcopy
 
 from src.runners import Base_Runner
-from src.data import ROIDataset, SITES_DICT, MNISTDataset
-from src.datamodules import DataModule, MNISTDataModule
+from src.data import ROIDataset, SITES_DICT
+from src.datamodules import DataModule
 from src.tasks import ClassificationTask
 from src.utils import plot_paper
 from src.callbacks import wandb_callback as wbc
@@ -177,106 +177,4 @@ class S_Runner(Base_Runner):
 
         return
 
-class MNIST_Runner(Base_Runner):
-    def get_callbacks(self):
-        """
-        Write only callbacks that logger is not necessary
-        """
-        checkpoint_callback = ModelCheckpoint(
-            dirpath=os.path.join(
-                self.log.checkpoint_path,
-                self.log.project_name,
-                f"version{self.version:03d}",
-            ),
-            filename=os.path.join(f"model"),
-            monitor=f"Accuracy/val",
-            mode="max",
-            verbose=False,
-            save_top_k=1,
-        )
-
-        callbacks = dict(
-            filter(lambda item: item[0].endswith("callback"), vars().items())
-        ).values()
-        callbacks = list(callbacks)
-        return callbacks if len(callbacks) > 0 else None
-
-    def run(self, profiler: Optional[str] = None):
-        os.makedirs(
-            os.path.join(self.log.checkpoint_path, self.log.project_name), exist_ok=True
-        )
-        self.version = len(
-            os.listdir(os.path.join(self.log.checkpoint_path, self.log.project_name))
-        )
-
-
-        final_results = list()
-
-
-
-        dm = self.get_datamodule(dataset=MNISTDataset, datamodule=MNISTDataModule)
-        model = self.get_network(Task=ClassificationTask)
-        model.apply(initialize_weights)
-
-        trainer = Trainer(
-            logger=[
-                TensorBoardLogger(
-                    save_dir=self.log.log_path,
-                    name=os.path.join(
-                        self.log.project_name,
-                        f"version{self.version:03d}"
-                    ),
-                    default_hp_metric=False,
-                    version=None,
-                    # log_graph=True, # inavailable due to bug
-                ),
-                WandbLogger(
-                    project=self.log.project_name,
-                    save_dir=self.log.log_path,
-                ),
-            ],
-            # ! use all gpu
-            # gpus=-1,
-            # auto_select_gpus=True,
-            # ! use 2 gpu
-            # devices=2,
-            # accelerator="auto",
-            # strategy="ddp",
-            # ! use gpu 0
-            devices=[0],
-            accelerator="gpu",
-            #devices=[self.log.device.gpu],
-            #accelerator="cpu",
-            check_val_every_n_epoch=self.log.val_log_freq_epoch,
-            log_every_n_steps=1,
-            num_sanity_val_steps=0,
-            max_epochs=self.log.epoch,
-            profiler=profiler,
-            callbacks=[
-                *self.get_callbacks(),
-                wbc.WatchModel(),
-                wbc.LogConfusionMatrix(),
-                wbc.LogF1PrecRecHeatmap(),
-                # tbc.WatchModel(),
-                # tbc.LogConfusionMatrix(),
-                # tbc.LogF1PrecRecHeatmap(),
-            ],
-            precision=self.log.precision,
-            # gradient_clip_val=0.5,
-        )
-        trainer.test_site_prefix = model.prefix
-        trainer.fit(model, datamodule=dm)
-        trainer.test(model, datamodule=dm, ckpt_path="best")
-        final_results.append(
-            trainer.callback_metrics[f"Accuracy/test"]
-        )
-
-    try:
-        import wandb
-
-        wb_logger = wbc.get_wandb_logger(trainer=Trainer)
-        
-    except Exception as e:
-        print(e)
-
     
