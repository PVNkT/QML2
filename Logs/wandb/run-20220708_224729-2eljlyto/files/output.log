GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Epoch 0:   0%|                                                                                                                                | 0/14 [00:00<?, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name        | Type             | Params
-------------------------------------------------
0 | model       | Simple_QHN       | 170 K
1 | get_metrics | MetricCollection | 0
-------------------------------------------------
170 K     Trainable params
0         Non-trainable params
170 K     Total params
0.681     Total estimated model params size (MB)
Missing logger folder: Logs/QML-Real_Machine/version000/NYU
/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
  rank_zero_warn(
/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
  rank_zero_warn(
/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
Epoch 0:   0%|                                                                                                                                | 0/14 [00:00<?, ?it/s]tensor([[[-1.5046e-04, -4.0920e-05,  1.3767e-04, -6.1910e-04, -1.1795e-04,
           2.2414e-05, -2.1220e-05, -4.8052e-05, -6.8952e-05, -1.6634e-05,
          -3.7599e-05,  4.4183e-06, -9.4840e-04,  7.6428e-05, -1.4681e-04,
          -3.7937e-05],
         [-1.5980e-04, -4.6446e-05,  2.5464e-04, -4.3724e-04, -2.1818e-04,
           2.2614e-05, -1.4273e-05,  6.2334e-05, -4.1741e-04, -1.1188e-05,
          -2.5290e-05,  1.1144e-05, -9.1781e-04,  1.6707e-04, -2.2218e-04,
          -4.1605e-05],
         [ 1.1535e-04,  3.2709e-05, -2.2009e-04,  5.1468e-04,  0.0000e+00,
          -9.5557e-05, -7.5387e-06,  7.3164e-06, -7.3490e-05,  5.9094e-06,
           2.6716e-05, -0.0000e+00,  1.1965e-03, -4.0729e-05,  3.5206e-04,
           4.0434e-05],
         [ 1.1561e-04,  1.0928e-05, -2.4510e-05,  7.2747e-04,  0.0000e+00,
           0.0000e+00,  7.5557e-06, -2.1999e-05, -2.5780e-04,  0.0000e+00,
          -4.0165e-05, -0.0000e+00,  8.2702e-04, -4.0821e-05,  7.8413e-05,
           4.7574e-05],
         [ 1.6340e-04,  0.0000e+00,  0.0000e+00,  7.4005e-04,  6.2946e-05,
           0.0000e+00,  0.0000e+00,  7.3265e-06,  1.8398e-04,  0.0000e+00,
           1.3376e-05, -4.7156e-06,  1.0329e-03,  0.0000e+00,  0.0000e+00,
           4.9292e-05],
         [-1.4878e-04, -0.0000e+00, -0.0000e+00, -6.7338e-04,  1.1663e-04,
          -0.0000e+00, -0.0000e+00, -5.4302e-05, -3.4090e-05, -0.0000e+00,
           0.0000e+00,  8.7376e-06, -1.3014e-03, -0.0000e+00, -0.0000e+00,
          -4.6483e-05],
         [-9.3832e-05, -5.0506e-06,  9.0622e-05, -7.2134e-04,  5.8235e-05,
          -0.0000e+00,  6.9841e-06, -8.8117e-05,  4.4255e-04, -0.0000e+00,
           0.0000e+00, -1.0907e-05, -1.3569e-03, -0.0000e+00, -7.2481e-05,
          -4.7232e-05],
         [ 1.3606e-04,  0.0000e+00,  0.0000e+00,  6.7813e-04, -4.2223e-05,
           0.0000e+00,  0.0000e+00, -2.9487e-05, -1.8511e-04,  0.0000e+00,
          -0.0000e+00, -7.1170e-06,  1.1224e-03, -1.3679e-05,  3.9414e-05,
           5.0482e-05],
         [ 1.7382e-04,  1.0865e-05, -2.4369e-05,  5.7864e-04,  8.3520e-05,
           0.0000e+00,  0.0000e+00,  5.8328e-05, -7.3235e-05,  5.8889e-06,
          -0.0000e+00, -7.0391e-06,  1.3568e-03, -5.4116e-05,  1.1695e-04,
           4.0293e-05],
         [-1.4617e-04, -0.0000e+00,  1.1550e-04, -5.3606e-04, -3.9586e-05,
          -1.1284e-05, -0.0000e+00,  5.5292e-05, -2.4298e-04, -0.0000e+00,
           3.7856e-05,  6.6727e-06, -8.9641e-04,  2.5650e-05, -3.6953e-05,
          -4.3178e-05],
         [-1.2195e-04, -7.1920e-05,  2.7653e-04, -6.0935e-04, -7.8978e-05,
           3.3767e-05, -2.8416e-05, -6.2050e-05,  3.1163e-04, -0.0000e+00,
           2.5175e-05,  6.6563e-06, -8.9420e-04,  1.6631e-04, -3.3176e-04,
          -3.2304e-05],
         [-1.2931e-04, -8.8744e-05,  3.7467e-04, -4.4229e-04,  4.0128e-05,
           1.1438e-05, -2.8875e-05,  6.3054e-05, -3.1667e-04, -1.1317e-05,
          -6.3956e-05,  2.2546e-06, -5.5310e-04,  9.1002e-05, -4.4950e-04,
          -3.1985e-05],
         [ 1.4698e-04,  0.0000e+00,  0.0000e+00,  7.8224e-04, -6.3151e-05,
           0.0000e+00, -7.5738e-06,  3.6752e-05, -2.2150e-04,  0.0000e+00,
          -1.3420e-05,  2.6021e-05,  1.2020e-03,  0.0000e+00,  0.0000e+00,
           3.5324e-05],
         [ 1.4140e-04,  5.4797e-06, -2.4581e-05,  6.3672e-04,  2.1061e-05,
           0.0000e+00,  0.0000e+00, -7.3542e-06,  3.6935e-04, -5.9400e-06,
          -0.0000e+00, -7.1001e-06,  9.7456e-04, -4.0939e-05,  0.0000e+00,
           4.9478e-05],
         [ 1.9280e-04,  0.0000e+00,  0.0000e+00,  5.5050e-04,  2.9135e-04,
           0.0000e+00,  0.0000e+00, -5.8133e-05,  6.2042e-04,  0.0000e+00,
          -0.0000e+00, -3.2740e-05,  7.9906e-04,  0.0000e+00,  0.0000e+00,
           5.6747e-05],
         [-1.4772e-04, -4.6001e-05,  1.3757e-04, -6.4340e-04, -1.1787e-04,
          -1.1199e-05, -2.1204e-05, -6.8597e-06,  1.0335e-04, -5.5405e-06,
           0.0000e+00, -6.6227e-06, -9.0903e-04,  8.9102e-05, -1.4670e-04,
          -3.8734e-05],
         [-8.4369e-05, -7.6633e-05,  3.4376e-04, -5.6889e-04,  2.1599e-04,
           4.4775e-05,  1.4130e-05, -6.8565e-05,  4.1322e-04,  5.5379e-06,
          -1.2518e-05, -1.5446e-05, -1.0246e-03,  7.6337e-05, -6.5986e-04,
          -2.5536e-05],
         [-1.1426e-04, -7.0445e-05,  9.0285e-05, -5.6031e-04,  9.6697e-05,
          -1.1025e-05,  4.1749e-05, -6.0777e-05,  2.0349e-04,  2.1817e-05,
          -1.2329e-05, -1.0866e-05, -1.1234e-03,  1.0025e-04, -2.1663e-04,
          -2.7585e-05],
         [-9.2663e-05, -4.6171e-05,  6.9038e-05, -8.4448e-04,  1.7746e-04,
           1.1240e-05,  2.1283e-05, -8.9506e-05,  5.5326e-04,  5.5610e-06,
           2.5141e-05, -2.2157e-05, -1.0677e-03,  1.2776e-05, -2.2087e-04,
          -3.9704e-05],
         [ 1.1937e-04,  1.6522e-05,  0.0000e+00,  8.9324e-04, -2.1167e-04,
           0.0000e+00, -7.6159e-06, -0.0000e+00, -8.5379e-04, -5.9699e-06,
          -0.0000e+00,  3.3301e-05,  1.3546e-03, -4.1146e-05,  1.1856e-04,
           3.9072e-05],
         [-1.3385e-04,  5.0856e-06,  2.2813e-05, -6.2786e-04, -1.1728e-04,
          -0.0000e+00, -0.0000e+00, -6.1428e-05,  4.1134e-04, -0.0000e+00,
           0.0000e+00, -2.6358e-05, -1.3471e-03,  2.5330e-05, -3.6492e-05,
          -3.2800e-05],
         [ 1.5461e-04,  0.0000e+00,  0.0000e+00,  7.2523e-04,  2.0936e-05,
           0.0000e+00,  0.0000e+00, -4.3862e-05,  7.3430e-05,  0.0000e+00,
          -0.0000e+00,  4.7052e-06,  9.4814e-04,  0.0000e+00,  0.0000e+00,
           4.6549e-05],
         [ 1.1569e-04,  2.7339e-05, -7.3583e-05,  8.2065e-04, -1.4711e-04,
          -2.3961e-05,  0.0000e+00,  8.8060e-05, -4.7912e-04, -5.9271e-06,
           2.6796e-05,  3.0701e-05,  1.4690e-03, -2.7234e-05,  7.8470e-05,
           3.8792e-05],
         [-7.6343e-05, -5.6112e-05,  1.6018e-04, -8.8909e-04,  2.7448e-04,
           1.1177e-05,  1.4108e-05, -8.2153e-05,  2.4068e-04,  1.6589e-05,
          -1.2499e-05, -1.9829e-05, -7.1420e-04,  7.6221e-05, -6.5885e-04,
          -3.8657e-05],
         [ 5.1355e-05,  1.3268e-04, -2.7279e-04,  7.7620e-04, -4.0372e-04,
          -2.4226e-05, -6.8804e-05,  1.6323e-04, -5.2168e-04, -1.7978e-05,
           1.3546e-05,  3.5816e-05,  1.1506e-03, -1.5144e-04,  3.9669e-04,
           1.9611e-05],
         [ 9.1657e-05,  1.6650e-05, -4.9793e-05,  8.0613e-04, -4.2664e-04,
           0.0000e+00, -1.5350e-05,  1.5642e-04, -7.4819e-04, -6.0163e-06,
          -0.0000e+00,  3.3560e-05,  1.7222e-03, -1.3822e-05,  1.1948e-04,
           3.1321e-05],
         [ 6.5357e-05,  5.5063e-06,  0.0000e+00,  7.3311e-04, -3.3861e-04,
          -1.2064e-05,  0.0000e+00,  8.8678e-05, -4.4537e-04,  0.0000e+00,
           2.6984e-05,  3.0916e-05,  1.3127e-03, -4.1138e-05,  7.9021e-05,
           3.4625e-05],
         [-5.2253e-05, -5.0626e-06,  6.8128e-05, -7.5982e-04,  3.8915e-04,
           2.2185e-05,  1.4001e-05, -1.0871e-04,  4.4360e-04,  2.1951e-05,
          -2.4810e-05, -4.5917e-05, -1.3410e-03,  3.7823e-05, -1.8163e-04,
          -2.6121e-05],
         [-8.0440e-05, -1.0056e-05,  1.5788e-04, -7.5464e-04,  4.2515e-04,
           2.2033e-05,  3.4765e-05, -7.4228e-05,  2.3723e-04,  1.0901e-05,
          -3.6961e-05, -3.2574e-05, -1.2938e-03,  3.7565e-05, -1.8039e-04,
          -2.3511e-05],
         [ 1.1044e-04,  8.7795e-05, -3.4460e-04,  4.7819e-04, -4.2180e-05,
           1.2023e-05, -7.5879e-06,  2.2093e-05, -7.3970e-05, -2.9740e-05,
          -4.0336e-05, -9.4797e-06,  1.0797e-03, -1.6398e-04,  3.9373e-04,
           3.8929e-05],
         [-1.4630e-04, -1.5463e-05, -0.0000e+00, -7.3618e-04,  2.9717e-04,
          -1.1294e-05, -0.0000e+00,  1.3835e-05, -1.0423e-04, -0.0000e+00,
           0.0000e+00, -8.9050e-06, -1.1703e-03, -0.0000e+00, -0.0000e+00,
          -4.4879e-05],
         [ 1.9149e-04,  0.0000e+00,  0.0000e+00,  6.3786e-04,  6.2005e-05,
           0.0000e+00,  0.0000e+00, -5.7736e-05,  3.6246e-04,  0.0000e+00,
          -0.0000e+00, -9.2903e-06,  1.0988e-03,  0.0000e+00,  0.0000e+00,
           5.1157e-05]]], dtype=torch.float64) [[0. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 1. 0.]
 [0. 0. 1. 1.]
 [0. 1. 0. 0.]
 [0. 1. 0. 1.]
 [0. 1. 1. 0.]
 [0. 1. 1. 1.]
 [1. 0. 0. 0.]
 [1. 0. 0. 1.]
 [1. 0. 1. 0.]
 [1. 0. 1. 1.]
 [1. 1. 0. 0.]
 [1. 1. 0. 1.]
 [1. 1. 1. 0.]
 [1. 1. 1. 1.]]
tensor([[-0.0012, -0.0012, -0.0008, -0.0007],
        [-0.0015, -0.0012, -0.0004, -0.0003],
        [ 0.0015,  0.0015,  0.0007,  0.0005],
        [ 0.0006,  0.0009,  0.0008,  0.0007],
        [ 0.0013,  0.0012,  0.0008,  0.0008],
        [-0.0014, -0.0013, -0.0008, -0.0008],
        [-0.0010, -0.0015, -0.0008, -0.0009],
        [ 0.0010,  0.0011,  0.0007,  0.0007],
        [ 0.0014,  0.0016,  0.0008,  0.0006],
        [-0.0011, -0.0009, -0.0004, -0.0005],
        [-0.0007, -0.0012, -0.0008, -0.0006],
        [-0.0013, -0.0009, -0.0006, -0.0004],
        [ 0.0010,  0.0012,  0.0009,  0.0009],
        [ 0.0013,  0.0010,  0.0006,  0.0006],
        [ 0.0014,  0.0011,  0.0005,  0.0005],
        [-0.0009, -0.0012, -0.0007, -0.0007],
        [-0.0012, -0.0014, -0.0010, -0.0006],
        [-0.0011, -0.0012, -0.0008, -0.0006],
        [-0.0008, -0.0012, -0.0011, -0.0010],
        [ 0.0006,  0.0013,  0.0011,  0.0009],
        [-0.0010, -0.0016, -0.0008, -0.0007],
        [ 0.0011,  0.0010,  0.0007,  0.0007],
        [ 0.0011,  0.0015,  0.0010,  0.0009],
        [-0.0011, -0.0011, -0.0015, -0.0010],
        [ 0.0009,  0.0011,  0.0011,  0.0009],
        [ 0.0011,  0.0016,  0.0011,  0.0010],
        [ 0.0010,  0.0011,  0.0010,  0.0008],
        [-0.0011, -0.0012, -0.0011, -0.0009],
        [-0.0013, -0.0011, -0.0009, -0.0008],
        [ 0.0012,  0.0013,  0.0005,  0.0004],
        [-0.0013, -0.0009, -0.0008, -0.0008],
        [ 0.0015,  0.0012,  0.0006,  0.0006]])
Traceback (most recent call last):
  File "run.py", line 26, in <module>
    main()
  File "run.py", line 21, in main
    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
    trainer.fit(model, datamodule=dm)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
    self._call_and_handle_interrupt(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
    results = self._run_stage()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
    return self._run_train()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
    self.fit_loop.run()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
    loss = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 143, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 311, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 168, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 80, in backward
    model.backward(closure_loss, optimizer, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1370, in backward
    loss.backward(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: function HybridFunctionBackward returned an incorrect number of gradients (expected 6, got 3)
Traceback (most recent call last):
  File "run.py", line 26, in <module>
    main()
  File "run.py", line 21, in main
    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
    trainer.fit(model, datamodule=dm)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
    self._call_and_handle_interrupt(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
    results = self._run_stage()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
    return self._run_train()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
    self.fit_loop.run()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
    loss = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 143, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 311, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 168, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 80, in backward
    model.backward(closure_loss, optimizer, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1370, in backward
    loss.backward(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: function HybridFunctionBackward returned an incorrect number of gradients (expected 6, got 3)
Traceback (most recent call last):
  File "run.py", line 26, in <module>
    main()
  File "run.py", line 21, in main
    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
    trainer.fit(model, datamodule=dm)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
    self._call_and_handle_interrupt(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
    results = self._run_stage()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
    return self._run_train()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
    self.fit_loop.run()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
    loss = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 143, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 311, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 168, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 80, in backward
    model.backward(closure_loss, optimizer, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1370, in backward
    loss.backward(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
