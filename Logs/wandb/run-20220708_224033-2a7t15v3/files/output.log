GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Epoch 0:   0%|                                                                                                                                | 0/14 [00:00<?, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name        | Type             | Params
-------------------------------------------------
0 | model       | Simple_QHN       | 170 K
1 | get_metrics | MetricCollection | 0
-------------------------------------------------
170 K     Trainable params
0         Non-trainable params
170 K     Total params
0.681     Total estimated model params size (MB)
Missing logger folder: Logs/QML-Real_Machine/version000/NYU
/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
  rank_zero_warn(
/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
  rank_zero_warn(
/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2708: UserWarning: Using trainer.logger when Trainer is configured to use multiple loggers. This behavior will change in v1.8 when LoggerCollection is removed, and trainer.logger will return the first logger in trainer.loggers
  rank_zero_warn(
Traceback (most recent call last):
  File "run.py", line 26, in <module>
    main()
  File "run.py", line 21, in main
    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
    trainer.fit(model, datamodule=dm)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
    self._call_and_handle_interrupt(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
    results = self._run_stage()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
    return self._run_train()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
    self.fit_loop.run()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
    loss = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 143, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 311, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 168, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 80, in backward
    model.backward(closure_loss, optimizer, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1370, in backward
    loss.backward(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: function HybridFunctionBackward returned an incorrect number of gradients (expected 6, got 3)
Traceback (most recent call last):
  File "run.py", line 26, in <module>
    main()
  File "run.py", line 21, in main
    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
    trainer.fit(model, datamodule=dm)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
    self._call_and_handle_interrupt(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
    results = self._run_stage()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
    return self._run_train()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
    self.fit_loop.run()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
    loss = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 143, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 311, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 168, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 80, in backward
    model.backward(closure_loss, optimizer, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1370, in backward
    loss.backward(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: function HybridFunctionBackward returned an incorrect number of gradients (expected 6, got 3)
Traceback (most recent call last):
  File "run.py", line 26, in <module>
    main()
  File "run.py", line 21, in main
    metrics = runner.run(profiler=cfg.get("profiler", "simple"))
  File "/home/sol/git/QML2/src/runners/runner.py", line 164, in run
    trainer.fit(model, datamodule=dm)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 768, in fit
    self._call_and_handle_interrupt(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 809, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1234, in _run
    results = self._run_stage()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1321, in _run_stage
    return self._run_train()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1351, in _run_train
    self.fit_loop.run()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1593, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/optim/adam.py", line 118, in step
    loss = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 143, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 311, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1763, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 168, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 80, in backward
    model.backward(closure_loss, optimizer, *args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1370, in backward
    loss.backward(*args, **kwargs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/sol/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 0:   0%|                                                                                                                                | 0/14 [00:00<?, ?it/s]tensor([[[-1.2259e-04, -3.5378e-05,  2.0404e-04, -5.3832e-04, -1.3598e-04,
           1.1074e-05, -1.3978e-05, -2.7132e-05,  2.0439e-04, -0.0000e+00,
           0.0000e+00,  1.3097e-05, -1.1092e-03,  5.0345e-05, -3.6265e-05,
          -4.4005e-05],
         [-1.7312e-04, -2.6207e-05,  2.3512e-04, -5.4559e-04, -1.4102e-04,
          -5.7421e-05, -1.4496e-05,  7.7379e-05, -3.5329e-04, -2.2727e-05,
          -2.5686e-05,  3.1693e-05, -6.5452e-04,  9.1372e-05, -3.0088e-04,
          -4.9862e-05],
         [ 1.4464e-04,  2.7478e-05, -2.4652e-04,  7.0509e-04,  2.1122e-05,
          -1.2041e-05, -7.5997e-06, -1.4751e-05, -1.8521e-04,  5.9573e-06,
           1.3466e-05, -1.1868e-05,  1.0814e-03, -8.2117e-05,  3.1548e-04,
           4.6078e-05],
         [ 1.6053e-04,  5.4571e-06, -4.8958e-05,  8.7187e-04,  1.4682e-04,
           1.1957e-05,  0.0000e+00, -3.6619e-05,  3.6782e-05,  5.9154e-06,
          -0.0000e+00, -4.7139e-06,  6.6079e-04, -4.0770e-05,  7.8315e-05,
           4.9274e-05],
         [ 1.4520e-04,  0.0000e+00,  0.0000e+00,  7.0729e-04,  1.2478e-04,
           0.0000e+00,  0.0000e+00, -5.0831e-05,  1.4588e-04,  0.0000e+00,
          -1.3258e-05, -9.3476e-06,  8.5992e-04,  0.0000e+00,  3.8825e-05,
           5.8452e-05],
         [-1.3187e-04, -0.0000e+00, -0.0000e+00, -6.8041e-04, -5.8926e-05,
          -0.0000e+00, -0.0000e+00,  0.0000e+00,  1.0334e-04, -0.0000e+00,
           0.0000e+00,  6.6217e-06, -1.1023e-03, -0.0000e+00, -0.0000e+00,
          -5.3560e-05],
         [-1.0208e-04, -3.0431e-05,  6.8254e-05, -7.3666e-04,  2.1443e-04,
          -1.1113e-05, -0.0000e+00, -8.8489e-05,  3.4186e-04, -0.0000e+00,
          -3.7283e-05, -2.6287e-05, -1.3243e-03,  3.7892e-05, -3.6393e-05,
          -3.5165e-05],
         [ 1.4335e-04,  0.0000e+00,  0.0000e+00,  6.4606e-04, -2.0934e-05,
           0.0000e+00,  0.0000e+00, -4.3859e-05,  1.4685e-04,  0.0000e+00,
          -0.0000e+00, -1.6467e-05,  1.1336e-03,  0.0000e+00,  0.0000e+00,
           6.0596e-05],
         [ 1.7155e-04,  5.4495e-06, -2.4445e-05,  8.3109e-04,  6.2835e-05,
          -1.1940e-05,  7.5359e-06, -2.9255e-05, -1.1019e-04,  0.0000e+00,
          -1.3353e-05, -7.0610e-06,  1.1135e-03,  0.0000e+00,  0.0000e+00,
           5.6235e-05],
         [-1.7902e-04, -1.5533e-05,  2.3225e-05, -6.6428e-04, -1.7910e-04,
          -0.0000e+00, -7.1598e-06,  3.4743e-05, -3.8388e-04, -0.0000e+00,
           1.2687e-05,  2.6834e-05, -6.4654e-04, -0.0000e+00, -1.1146e-04,
          -5.2594e-05],
         [-9.2586e-05, -7.6888e-05,  1.1497e-04, -4.8393e-04, -0.0000e+00,
          -0.0000e+00,  2.8353e-05, -3.4397e-05,  1.3820e-04,  1.1113e-05,
          -2.5120e-05,  8.8556e-06, -9.8922e-04,  2.0424e-04, -4.0459e-04,
          -3.8845e-05],
         [-1.2536e-04, -2.5843e-05,  6.4917e-04, -5.0047e-04, -1.5892e-04,
          -6.7947e-05,  7.1473e-06,  1.3873e-05, -2.7870e-04,  5.6026e-06,
           3.7993e-05,  8.9292e-06, -7.8231e-04,  1.2871e-04, -4.4504e-04,
          -3.4168e-05],
         [ 1.0851e-04,  0.0000e+00,  0.0000e+00,  8.3048e-04, -2.1267e-04,
           0.0000e+00,  0.0000e+00,  6.6836e-05, -6.3404e-04,  0.0000e+00,
          -0.0000e+00,  9.5595e-06,  1.4029e-03, -1.3780e-05,  7.9410e-05,
           4.6394e-05],
         [ 1.4959e-04,  2.7345e-05, -1.4720e-04,  8.0758e-04,  1.2612e-04,
           1.1983e-05, -1.5126e-05,  2.9359e-05, -3.6863e-05,  5.9283e-06,
           1.3401e-05, -1.6535e-05,  8.4849e-04,  0.0000e+00,  7.8486e-05,
           5.0263e-05],
         [ 1.7625e-04,  0.0000e+00, -2.4318e-05,  6.2991e-04,  2.0836e-05,
           0.0000e+00,  0.0000e+00, -1.3096e-04,  6.5771e-04,  0.0000e+00,
          -0.0000e+00, -1.8731e-05,  8.4105e-04,  0.0000e+00,  0.0000e+00,
           6.1186e-05],
         [-1.6705e-04, -1.0276e-05,  2.3048e-05, -6.2189e-04,  1.5798e-04,
          -4.5031e-05, -0.0000e+00,  2.7583e-05, -3.4632e-05,  5.5696e-06,
           0.0000e+00, -1.1096e-05, -1.0693e-03,  5.1182e-05, -3.3181e-04,
          -3.5623e-05],
         [-9.3100e-05, -8.2469e-05,  3.6994e-04, -6.2386e-04,  5.9431e-05,
          -0.0000e+00,  2.8511e-05, -3.4587e-05,  4.5164e-04,  5.5872e-06,
          -1.2630e-05, -1.1131e-05, -9.3620e-04,  1.7971e-04, -7.0271e-04,
          -2.9919e-05],
         [-1.0067e-04, -4.0016e-05,  8.9750e-05, -5.6910e-04, -7.6899e-05,
           2.1919e-05, -6.9169e-06, -9.3982e-05,  2.3600e-04,  1.0844e-05,
          -1.2256e-05, -1.0802e-05, -1.1735e-03,  7.4740e-05, -2.1535e-04,
          -3.8712e-05],
         [-1.0017e-04, -1.9909e-05,  2.2326e-05, -7.7109e-04,  2.1042e-04,
           1.0905e-05,  1.3765e-05, -1.1355e-04,  3.0193e-04, -5.3951e-06,
           0.0000e+00, -1.9347e-05, -9.9818e-04,  4.9579e-05, -1.4285e-04,
          -2.6483e-05],
         [ 1.4191e-04,  2.1998e-05, -7.4008e-05,  8.6534e-04, -2.1137e-05,
          -3.6149e-05,  0.0000e+00,  5.9046e-05, -4.4482e-04, -5.9614e-06,
           2.6951e-05,  1.6627e-05,  1.2070e-03, -1.3696e-05,  1.5785e-04,
           3.8129e-05],
         [-1.0809e-04, -5.1086e-06,  2.2916e-05, -8.0383e-04,  7.8539e-05,
           1.1193e-05, -0.0000e+00, -1.1655e-04,  1.7217e-04, -0.0000e+00,
           0.0000e+00,  0.0000e+00, -1.0825e-03, -0.0000e+00, -3.6657e-05,
          -4.4480e-05],
         [ 1.5178e-04,  0.0000e+00,  0.0000e+00,  7.3833e-04, -1.6747e-04,
           0.0000e+00,  0.0000e+00,  5.8477e-05,  2.2026e-04,  0.0000e+00,
          -0.0000e+00, -2.3523e-06,  1.0717e-03,  0.0000e+00,  0.0000e+00,
           3.9518e-05],
         [ 1.0419e-04,  0.0000e+00, -9.7903e-05,  7.1325e-04, -2.9360e-04,
           0.0000e+00,  1.5091e-05,  8.0551e-05, -1.4711e-04, -2.3658e-05,
           2.6739e-05,  2.8279e-05,  1.5279e-03, -2.7176e-05,  3.9152e-05,
           3.6071e-05],
         [-1.0931e-04, -5.4185e-05,  6.6290e-05, -8.4663e-04,  2.0826e-04,
          -1.0793e-05,  1.3624e-05, -6.6110e-05,  5.9764e-04,  1.0679e-05,
           0.0000e+00, -1.7020e-05, -8.0151e-04,  8.5871e-05, -3.8881e-04,
          -2.6210e-05],
         [ 6.2819e-05,  8.8528e-05, -1.2410e-04,  5.8933e-04, -2.9772e-04,
          -1.2123e-05, -5.3559e-05,  1.3366e-04, -4.8482e-04, -1.1995e-05,
           1.3557e-05,  4.7794e-05,  9.6310e-04, -9.6453e-05,  9.1315e-04,
           1.6950e-05],
         [ 1.0296e-04,  1.1084e-05,  0.0000e+00,  8.7199e-04, -2.1300e-04,
           0.0000e+00,  0.0000e+00,  2.0825e-04, -8.2177e-04, -1.8022e-05,
          -0.0000e+00,  3.1116e-05,  1.5308e-03,  0.0000e+00,  0.0000e+00,
           2.5019e-05],
         [ 1.3939e-04,  2.2049e-05, -2.4726e-05,  9.6074e-04, -3.3898e-04,
          -1.2077e-05, -7.6225e-06,  1.9974e-04, -4.8300e-04, -5.9752e-06,
           1.3507e-05,  2.3807e-05,  1.4184e-03, -2.7455e-05,  2.3732e-04,
           2.4886e-05],
         [-6.9683e-05, -4.5008e-05,  8.9732e-05, -6.1741e-04,  1.5377e-04,
           1.0957e-05,  2.0747e-05, -1.2081e-04,  4.7191e-04,  5.4210e-06,
           0.0000e+00, -4.9678e-05, -1.3057e-03,  7.4725e-05, -1.7942e-04,
          -2.3384e-05],
         [-6.3148e-05, -5.0985e-05,  6.8612e-05, -8.2692e-04,  5.4868e-04,
           1.1171e-05,  3.5252e-05, -8.2111e-05,  6.5294e-04, -0.0000e+00,
          -2.4986e-05, -4.4041e-05, -9.8393e-04,  1.2697e-05, -3.6584e-04,
          -1.8908e-05],
         [ 1.0781e-04,  6.0472e-05, -3.6990e-04,  6.6539e-04, -2.1129e-05,
          -1.2045e-05, -4.5613e-05,  2.9512e-05, -2.9643e-04, -5.9592e-06,
          -1.3470e-05, -1.1872e-05,  9.1531e-04, -1.5060e-04,  5.5226e-04,
           3.5456e-05],
         [-1.4710e-04, -5.0900e-06, -0.0000e+00, -5.5447e-04, -1.9563e-05,
          -0.0000e+00, -0.0000e+00, -5.4649e-05,  2.4015e-04, -0.0000e+00,
           0.0000e+00, -1.5389e-05, -1.5216e-03, -0.0000e+00, -3.6523e-05,
          -4.0214e-05],
         [ 1.4200e-04,  0.0000e+00,  0.0000e+00,  6.2689e-04,  2.0736e-05,
           0.0000e+00,  0.0000e+00, -2.1722e-05,  6.1820e-04,  0.0000e+00,
          -0.0000e+00,  2.3302e-06,  9.5952e-04,  0.0000e+00,  0.0000e+00,
           5.5674e-05]]], dtype=torch.float64) [[0. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 1. 0.]
 [0. 0. 1. 1.]
 [0. 1. 0. 0.]
 [0. 1. 0. 1.]
 [0. 1. 1. 0.]
 [0. 1. 1. 1.]
 [1. 0. 0. 0.]
 [1. 0. 0. 1.]
 [1. 0. 1. 0.]
 [1. 0. 1. 1.]
 [1. 1. 0. 0.]
 [1. 1. 0. 1.]
 [1. 1. 1. 0.]
 [1. 1. 1. 1.]]
tensor([[[-0.0009, -0.0013, -0.0004, -0.0006],
         [-0.0013, -0.0010, -0.0006, -0.0005],
         [ 0.0012,  0.0013,  0.0008,  0.0007],
         [ 0.0008,  0.0009,  0.0009,  0.0009],
         [ 0.0011,  0.0010,  0.0007,  0.0007],
         [-0.0010, -0.0012, -0.0007, -0.0007],
         [-0.0011, -0.0012, -0.0009, -0.0009],
         [ 0.0013,  0.0011,  0.0006,  0.0006],
         [ 0.0010,  0.0012,  0.0008,  0.0008],
         [-0.0012, -0.0010, -0.0007, -0.0007],
         [-0.0011, -0.0012, -0.0008, -0.0004],
         [-0.0014, -0.0013, -0.0003, -0.0005],
         [ 0.0009,  0.0014,  0.0010,  0.0009],
         [ 0.0009,  0.0011,  0.0008,  0.0009],
         [ 0.0015,  0.0008,  0.0005,  0.0005],
         [-0.0014, -0.0012, -0.0009, -0.0006],
         [-0.0011, -0.0014, -0.0010, -0.0006],
         [-0.0011, -0.0015, -0.0009, -0.0006],
         [-0.0008, -0.0010, -0.0010, -0.0009],
         [ 0.0010,  0.0014,  0.0011,  0.0009],
         [-0.0010, -0.0012, -0.0010, -0.0010],
         [ 0.0013,  0.0010,  0.0008,  0.0008],
         [ 0.0015,  0.0014,  0.0008,  0.0008],
         [-0.0005, -0.0010, -0.0013, -0.0009],
         [ 0.0014,  0.0016,  0.0015,  0.0008],
         [ 0.0007,  0.0016,  0.0011,  0.0011],
         [ 0.0012,  0.0015,  0.0014,  0.0012],
         [-0.0010, -0.0014, -0.0009, -0.0008],
         [-0.0008, -0.0008, -0.0013, -0.0010],
         [ 0.0010,  0.0013,  0.0008,  0.0006],
         [-0.0014, -0.0017, -0.0007, -0.0007],
